{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8205fefc",
   "metadata": {},
   "source": [
    "## 基本思想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c1ca5",
   "metadata": {},
   "source": [
    "线性模型形式简单，易于建模，但却蕴含着机器学习中的一些重要的基本思想：\n",
    "\n",
    "$$\n",
    "f(x) = w_{1}x_{1} + w_{2}x_{2} + \\cdots + w_{d}x_{d} + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1dc8c",
   "metadata": {},
   "source": [
    "## 线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86899b7e",
   "metadata": {},
   "source": [
    "线性回归是使用线性模型拟合数据的一种方法。\n",
    "\n",
    "**最小二乘法**：`Least Square Method`最小方差法。原理是：当预测值和实际值距离的平方和最小时，就选定模型中的两个参数($w$和$b$)。\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{b}} \\sum_{i=1}^{n}(y_{m}-y_{i})^{2}\n",
    "$$\n",
    "\n",
    "假设预测值$y_{i}$与观测值$y_{m}$之间存在的差别是由于其本身存在的随机性导致的，并假设这个随机满足一个均值为`0`，方差为$\\beta^{-1}$的分布。那么把$y_{i}$作为高斯分布的目标$x$就可以建立出一个概率模型，对这个概率模型做最大似然，就会等价于均方差最小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e1fab",
   "metadata": {},
   "source": [
    "### 一元线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a7165",
   "metadata": {},
   "source": [
    "单变量线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ba9fa",
   "metadata": {},
   "source": [
    "### 多元线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc174d01",
   "metadata": {},
   "source": [
    "多变量线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c8fd3",
   "metadata": {},
   "source": [
    "### 广义线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745ebe6b",
   "metadata": {},
   "source": [
    "线性回归基础上加了一层嵌套函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823acdd8",
   "metadata": {},
   "source": [
    "## 线性分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa8104",
   "metadata": {},
   "source": [
    "### 对数几率回归(逻辑回归)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b85396",
   "metadata": {},
   "source": [
    "$$\n",
    "y=\\frac{1}{1+e^{-(w^{T}x + b)}}\n",
    "$$\n",
    "\n",
    "表达某种事件发生的可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ff919",
   "metadata": {},
   "source": [
    "最大似然估计，根据最大可能性寻找最优解。每次猜对的可能性的乘积的最大值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78584c03",
   "metadata": {},
   "source": [
    "### 线性判别分析(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7cd40c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;LDA是基于某种准则对该函数进行优化，进而得到分类面。代表性方法有Fisher线性分类函数，也称作LDA。\n",
    "\n",
    "&emsp;&emsp;给定数据集$D=\\{(x_{i},y_{i})\\}_{i=1}^{m}$，$y_{i} \\in \\{0,1\\}$，令$X_{i}$、$\\mu_{i}$、$\\sum_{i}$分别表示第$i \\in \\{0,1\\}$类示例的集合、均值向量、协方差矩阵。\n",
    "\n",
    "&emsp;&emsp;如果将样本投影到直线$w$上，那么样本所对应的均值和方差也将做一个线性变换，也即是投影之后的均值和方差。依据投影的数学关系，我们可以知道，原始样本的**均值**在$w$上的**投影为$w^{T}\\mu_{i}$** ；原始样本的**协方差**在$w$上的**投影为$w^{T}\\sum_{i}w$**；由于直线在一维空间上，所以$w^{T}\\mu_{0}$、$w^{T}\\mu_{1}$、$w^{T}\\sum_{0}w$、$w^{T}\\sum_{1}w$均为实数。\n",
    "\n",
    "1. 让**同类样本的投影点尽可能接近**这句话在数学上就可以表示为，让同类样本的协方差尽可能地小。即$w^{T}\\sum_{0}w$ + $w^{T}\\sum_{1}w$尽可能地小；\n",
    "2. 让**异类样本投影点尽可能地远离**，所表示的意思就是，让两类样本的均值之间的距离尽可能地大。即$||w^{T}\\mu_{0}-w^{T}\\mu_{1}||_{2}^{2}$尽可能大。\n",
    "\n",
    "&emsp;&emsp;综合以上两点，组合一个最大化的目标函数$J$：\n",
    "\n",
    "$$\n",
    "J=\\frac{||w^{T}\\mu_{0}-w^{T}\\mu_{1}||_{2}^{2}}{w^{T}\\sum_{0}w+w^{T}\\sum_{1}w} \\\\\n",
    "=\\frac{w^{T}(\\mu_{0}-\\mu_{1})(\\mu_{0}-\\mu_{1})^{T}w}{w^{T}(\\sum_{0}+\\sum_{1})w}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;这个式子看起来符号有点多，我们将其化简一下，定义两个量：**类内散度矩阵**和**类间散度矩阵**：\n",
    "\n",
    "- **类内散度矩阵**(within-class scatter matrix)：\n",
    "\n",
    "&emsp;&emsp;定义类内散度矩阵$S_{w}=\\sum_{0}+\\sum_{1}$将其展开可得：\n",
    "\n",
    "$$\n",
    "=\\sum_{x\\in X_{0}}(x-\\mu_{0})(x-\\mu_{0})^{T}+\\sum_{x\\in X_{1}}(x-\\mu_{1})(x-\\mu_{1})^{T}\n",
    "$$\n",
    "\n",
    "- **类间散度矩阵**(between-class scatter matrix)：\n",
    "\n",
    "&emsp;&emsp;定义类间散度矩阵$S_{b}=(\\mu_{0}-\\mu_{1})(\\mu_{0}-\\mu_{1})^{T}$。\n",
    "\n",
    "&emsp;&emsp;此时，最大化的目标函数$J$可重写为：\n",
    "\n",
    "$$\n",
    "J = \\frac{w^{T}S_{b}w}{w^{T}S_{w}w}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;把上式称为$S_{b}$与$S_{w}$的**广义瑞利商**(generalized rayleigh quotient)。\n",
    "\n",
    "\n",
    "- **数学模型求解**\n",
    "\n",
    "&emsp;&emsp;现在的问题就变成了，我们怎么来求这个投影方向$w$，使得目标函数最大。\n",
    "\n",
    "&emsp;&emsp;优化目标函数$J$的分子和分母都是关于$w$的二次项，因此求解最大化$J$与$w$的长度无关，只与其方向有关。那么我们将分母约束为1，将原问题转换为带有约束的最优化问题，再利用拉格朗日乘子法对其求解即可，原问题等价为：\n",
    "\n",
    "$$\n",
    "min_{w} \\ \\ -w^{T}S_{b}w\n",
    "$$\n",
    "\n",
    "$$\n",
    "s.t. \\ \\ w^{T}S_{w}w =1 \n",
    "$$\n",
    "\n",
    "&emsp;&emsp;由拉格朗日乘子法可知，上式等价于：\n",
    "\n",
    "$$\n",
    "S_{b}w=\\lambda S_{w}w\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中$\\lambda$是拉格朗日乘子。由于$(\\mu_{0}-\\mu_{1})^{T}w$是标量，所以$S_{b}w$的方向恒为$\\mu_{0}-\\mu_{1}$，不妨令：\n",
    "\n",
    "$$\n",
    "S_{b}w=\\lambda(\\mu_{0}-\\mu_{1})\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;这里之所以可以令参数为$\\lambda$，是因为整个问题我们都在求解方向，且$S_{b}w$的方向恒为$\\mu_{0}-\\mu_{1}$，所以长度设置怎么好算怎么来。将$S_{b}w=\\lambda(\\mu_{0}-\\mu_{1})$带入$S_{b}w=\\lambda S_{w}w$可得：\n",
    "\n",
    "$$\n",
    "w=S_{w}^{-1}(\\mu_{0}-\\mu_{1})\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;到这里投影方向$w$的求解就完事了。但上述解涉及到求逆矩阵，考虑数值解的稳定性，实践过程中通常将$S_{w}$进行奇异值分解。$S_{w}=U\\sum V$，这里$\\sum$是一个实对角矩阵，其对角线上的元素是$S_{w}$的奇异值，再求解，得出$S_{w}^{-1}=V \\sum^{-1} U^{-1}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10b8360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.75722753 17.98292034]\n",
      " [17.23461661 19.78218463]\n",
      " [18.94493753 19.13312794]\n",
      " [16.97789375 17.89275847]\n",
      " [18.10395583 15.2510376 ]\n",
      " [16.09521116 16.39999876]\n",
      " [19.2352536  15.73404229]\n",
      " [19.96280919 18.02596221]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.22334079, 0.20104045])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def createDataSet():\n",
    "    #类别1\n",
    "    X1 = np.mat(np.random.random((8, 2)) * 5 + 15)\n",
    "    #类别2\n",
    "    X2 = np.mat(np.random.random((8, 2)) * 5 + 2)\n",
    "    return X1, X2\n",
    "\n",
    "def average(dataset):\n",
    "    ave = []\n",
    "    a, b = np.shape(dataset)\n",
    "    for i in range(b):\n",
    "        n = np.sum(dataset[:,i]) / a\n",
    "        ave.append(n)\n",
    "    return np.array(ave)\n",
    "\n",
    "def compute_sw(dataset, ave):\n",
    "    sw = 0\n",
    "    a, b = np.shape(dataset)\n",
    "    for i in range(a - 1):\n",
    "        sw += np.dot(dataset[i,:] - ave, (dataset[i,:] - ave).T)\n",
    "    return np.array(sw)\n",
    "\n",
    "x1_x, x2_x = createDataSet()\n",
    "print(x1_x)\n",
    "x1_x_ave = average(x1_x)\n",
    "x2_x_ave = average(x2_x)\n",
    "\n",
    "x1_sw = compute_sw(x1_x, x1_x_ave)\n",
    "x2_sw = compute_sw(x2_x, x2_x_ave)\n",
    "Sw = x1_sw + x2_sw\n",
    " \n",
    "#求广义逆\n",
    "pinv = np.linalg.pinv(Sw)\n",
    "w = np.multiply(x1_x_ave - x2_x_ave, pinv)[0,:]\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e7850",
   "metadata": {},
   "source": [
    "### 多分类学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45572d14",
   "metadata": {},
   "source": [
    "## 其他"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7993ef",
   "metadata": {},
   "source": [
    "### 类别不平衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fa4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
