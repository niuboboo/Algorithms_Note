{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305edf5a",
   "metadata": {},
   "source": [
    "## 基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2181e9f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;概念：基于树模型做决策；**每个节点对应某个属性**；每个分支对应可能的结果；叶子结点对应预测结果。\n",
    "\n",
    "&emsp;&emsp;每个节点对应的判断属性只是一个性质，就是我们依据什么分叉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47cdc0",
   "metadata": {},
   "source": [
    "## 划分选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c690ac",
   "metadata": {},
   "source": [
    "### 信息增益(ID3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d3af1",
   "metadata": {},
   "source": [
    "\n",
    "#### 信息熵的概念与度量\n",
    "\n",
    "##### 概念\n",
    "\n",
    "1. **熵**\n",
    "\n",
    "&emsp;&emsp;一种事物的不确定性叫做熵。\n",
    "\n",
    "2. **信息**\n",
    "\n",
    "&emsp;&emsp;信息说的是一种能够消除不确定性的事物，能够用来调整概率、排除干扰。\n",
    "\n",
    "&emsp;&emsp;**确定情况**是：比如卖瓜的人说了一句，保熟保甜，开了不甜算我的。\n",
    "\n",
    "3. **噪音**\n",
    "\n",
    "&emsp;&emsp;不能消除某人对某件事物的不确定性。\n",
    "\n",
    "4. **数据**\n",
    "\n",
    "&emsp;&emsp;我们所能拿到的数据就是噪音加上信息。\n",
    "\n",
    "##### 熵如何量化\n",
    "\n",
    "&emsp;&emsp;以抛硬币为例，假设不确定性有`8`种，那么抛硬币就需要抛$log_{2}^{8} = 3$种。取完对数之后就能够去衡量这个熵了。\n",
    "\n",
    "&emsp;&emsp;如果是每种情况概率不想等的话，一般分布可以描述为:\n",
    "\n",
    "$$\n",
    "\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k} \\log _{2} p_{k}\n",
    "$$\n",
    "\n",
    "#### 信息如何量化\n",
    "\n",
    "&emsp;&emsp;当知道了熵如何量化之后，我们就能够去知道说知道了多少信息。\n",
    "\n",
    "&emsp;&emsp;在得知信息的前后，我的不确定性的变化。也就是**对这个信息不确定性的差额就是信息提供的信息量**。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e50318",
   "metadata": {},
   "source": [
    "&emsp;&emsp;拿到一个数据集之后，根节点的信息增益，就是依据标签计算出来的。\n",
    "\n",
    "&emsp;&emsp;之后再计算每个特征划分完之后，里面有多少正例、里面有多少反例。比如依据色泽划分完之后，里面有**青绿**、**乌黑**和**浅白**。\n",
    "\n",
    "1. 青绿的有六分之三的正样本，六分之三的负样本；\n",
    "\n",
    "2. 乌黑的有六分之四的正样本，六分之二的负样本；\n",
    "\n",
    "3. 浅白的里面有五分之一的正样本，五分之四的负样本。\n",
    "\n",
    "&emsp;&emsp;然后计算熵减了多少即可，减少最多的就说明信息越多。\n",
    "\n",
    "&emsp;&emsp;**熵减最大**：\n",
    "\n",
    "$$\n",
    "\\Delta_{Entropy Reduction} = Entropy(parent) - \\sum_{j=1}^{k} \\frac{N(j)}{N}Entropy(j)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$k$是划分属性取值的个数。$N$是父亲结点上样本的总数，$N(j)$是第$j$个儿子结点上样本的数目。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2082266",
   "metadata": {},
   "source": [
    "### 增益率(C4.5)\n",
    "\n",
    "&emsp;&emsp;如果以信息增益率为划分依据，存在偏向选择**特征取值较多的特征**，信息增益比是对这一问题进行矫正。比如划分用户的时候，依据用户`ID`划分的话，只需要一层决策树就够了。\n",
    "\n",
    "- 实际上，信息增益准则对可取数目较多的属性有所偏好，`C4.5`决策树算法不直接使用信息增益，而是使用**增益率**(`gain ratio`)来选择最优划分属性。增益率定义为:\n",
    "\n",
    "$$\n",
    "\\operatorname{Gain} \\operatorname{ratio}(D, a)=\\frac{\\operatorname{Gain}(D, a)}{\\operatorname{IV}(a)}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中:\n",
    "\n",
    "$$\n",
    "\\operatorname{IV}(a)=-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\log _{2} \\frac{\\left|D^{v}\\right|}{|D|}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;属性$a$的可能取值数目越多(即V越大)，则$IV(a)$的取值通常会越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368436d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在具体使用的时候，先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36f897",
   "metadata": {},
   "source": [
    "### 基尼指数(CART)\n",
    "\n",
    "&emsp;&emsp;我们也可以将纯度的计算换成基尼指数：\n",
    "\n",
    "$$\n",
    "Gini(t)=1-\\sum_{i=1}^{c}p(i)^{2}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;假设我们依据`Chest Pain`特征划分，之后得到是否患心脏病的统计数字为: `yes-105`，`no-39`；`yes-34`，`no-125`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2699ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39\n",
      "0.36\n"
     ]
    }
   ],
   "source": [
    "def gini_index_single(a, b):\n",
    "    single_gini = 1 - ((a / (a + b))**2) - ((b / (a + b))**2)\n",
    "    return round(single_gini, 2)\n",
    "\n",
    "def gini_index(a, b, c, d):\n",
    "    left = gini_index_single(a, b)\n",
    "    right = gini_index_single(c, d)\n",
    "    \n",
    "    # 算出左边和右边之后再进行加权平均。\n",
    "    gini_index = left * ((a + b) / (a + b + c + d)) + right * ((c + d) / (a + b + c + d))\n",
    "    return round(gini_index, 2)\n",
    "\n",
    "print(gini_index_single(105, 39))\n",
    "print(gini_index(105, 39, 34, 125))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620be62",
   "metadata": {},
   "source": [
    "#### 分类树\n",
    "\n",
    "&emsp;&emsp;基尼指数最小准则。与熵减最小类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b38240",
   "metadata": {},
   "source": [
    "#### 回归树\n",
    "\n",
    "&emsp;&emsp;平方误差最小准则。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdde82f",
   "metadata": {},
   "source": [
    "## 剪枝处理\n",
    "\n",
    "&emsp;&emsp;剪枝的目的就是为了对付过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5745790",
   "metadata": {},
   "source": [
    "### 预剪枝\n",
    "\n",
    "&emsp;&emsp;选择一个特征，在不分的时候，查看一下验证集上的精度(依据训练集上的正负样本的比例来判断)。如果划分之后，精度有提升的话，就划分，不然就不划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33323983",
   "metadata": {},
   "source": [
    "### 后剪枝\n",
    "\n",
    "&emsp;&emsp;就是全部分完，之后再看能不剪枝。如果剪枝前后没有变化，就不剪了。预剪枝是能剪的全部剪掉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8a151",
   "metadata": {},
   "source": [
    "## 连续与缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3fe88",
   "metadata": {},
   "source": [
    "### 连续值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e0271",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用二分法，找到能够使得信息增益减最大的阈值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4468458",
   "metadata": {},
   "source": [
    "### 缺失值处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c8ac1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;属性里面有一些缺失值如何处理？\n",
    "\n",
    "1. **C4.5例子**: 去掉有缺失值的样本，计算一遍信息增益。然后再乘上权重(去掉有缺失值的样本 / 全部样本)。\n",
    "\n",
    "2. **离散值**：\n",
    "    - 众数填充；\n",
    "    - 相关性最高的列填充；\n",
    "\n",
    "3. **连续值**：\n",
    "    - 中位数；\n",
    "    - 相关性最高的列做线性回归进行估计；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4697c",
   "metadata": {},
   "source": [
    "## 多变量决策树\n",
    "\n",
    "&emsp;&emsp;多变量决策树里面判断节点分叉的地方可能是多个属性的组合。里面存在属性的线性组合，分割曲线会比较平滑。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0b59d",
   "metadata": {},
   "source": [
    "## 参考资料"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050138e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
