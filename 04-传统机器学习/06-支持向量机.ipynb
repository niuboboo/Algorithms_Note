{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbd4bf6",
   "metadata": {},
   "source": [
    "## 简单支持向量机\n",
    "\n",
    "&emsp;&emsp;感知机中采用的是函数间隔，因为感知机是一个误分类驱动的系统。间隔最大化思想是`SVM`中独有的: 试图要去找到一个超平面，这个超平面可以使得与它最近的样本点的距离必须大于其他所有超平面划分时，与最近的样本点的距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75565c15",
   "metadata": {},
   "source": [
    "## 带约束条件的最大间隔\n",
    "\n",
    "&emsp;&emsp;开始之前，简单回顾一下**函数间隔**和**几何间隔**：\n",
    "\n",
    "1. **函数间隔**:\n",
    "\n",
    "$$\n",
    "\\tilde{\\gamma}_{i} = y_{i}(wx_{i} + b)\n",
    "$$\n",
    "\n",
    "2. **几何间隔**：\n",
    "\n",
    "$$\n",
    "\\hat{\\gamma} = y_{i}(\\frac{w}{||w||} x_{i} + \\frac{b}{||w||}) = \\frac{\\tilde{\\gamma}_{i}}{||w||}\n",
    "$$\n",
    "\n",
    "3. **优化目标**：\n",
    "\n",
    "&emsp;&emsp;最大间隔分离超平面：\n",
    "\n",
    "$$\n",
    "\\max_{w, b} \\hat{\\gamma} \\\\\n",
    "s.t. \\ \\ \\ y_{i}(\\frac{w}{||w||}x_{i} + \\frac{b}{||w||}) \\geq \\gamma \\ \\ \\ i=1,2,\\cdots, N\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;上式中$\\hat{\\gamma}$是几何间隔。如果我们想要用函数间隔$\\tilde{\\gamma}$来表示的话，可以描述为：\n",
    "\n",
    "$$\n",
    "\\max_{w, b} \\frac{\\tilde{\\gamma}}{||w||} \\\\\n",
    "s.t. \\ \\ \\ y_{i}(w x_{i} + b) \\geq \\tilde{\\gamma} \\ \\ \\ i=1,2,\\cdots, N\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;为了之后方便计算，我们可以等比例放大和缩小函数间隔，使得$\\tilde{\\gamma}=1$。可以得到优化目标：\n",
    "\n",
    "$$\n",
    "\\min_{w,b} = \\frac{1}{2} ||w||^{2} \\\\\n",
    "s.t. \\ \\ y_{i}(wx_{i} + b) - 1 \\geq 0 \\ \\ \\ i=1,2,\\cdots, N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f04831a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;基于拉格朗日乘子法构建目标函数：\n",
    "\n",
    "$$\n",
    "L(w, b, \\alpha) = \\frac{1}{2} ||w||^{2} - \\sum_{i=1}^{N}\\alpha_{i}y_{i}(w x_{i} + b) + \\sum_{i=1}^{N} \\alpha_{i}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;此时目标函数可以写为$\\min_{w,b}\\max_{\\alpha} L(w, b, \\alpha)$表示在最大化$\\alpha$的情况下，来最小化几何间隔。转换成$\\max_{\\alpha}\\min_{w, b} L(w, b, \\alpha)$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12928f",
   "metadata": {},
   "source": [
    "4. **求解**：\n",
    "\n",
    "&emsp;&emsp;将拉格朗日函数$L(w, b, \\alpha)$分别对$w,b$求偏导，并令其等于0:\n",
    "\n",
    "$$\n",
    "\\nabla_{w}L(w, b, \\alpha) = w - \\sum_{i=1}^{N}\\alpha_{i}y_{i}x_{i}=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{b}L(w, b, \\alpha) = \\sum_{i=1}^{N}\\alpha_{i}y_{i}=0\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;得出：\n",
    "\n",
    "$$\n",
    "w = \\sum_{i=1}^{N} \\alpha_{i} y_{i} x_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} \\alpha_{i}y_{i}=0\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;将结果代入拉格朗日函数$L(w, b, \\alpha) = \\frac{1}{2} ||w||^{2} - \\sum_{i=1}^{N}\\alpha_{i}y_{i}(w x_{i} + b) + \\sum_{i=1}^{N} \\alpha_{i}$中，整理后可得：\n",
    "\n",
    "$$\\min _{w, b} L(w, b, \\alpha)=-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3f219",
   "metadata": {},
   "source": [
    "&emsp;&emsp;求$\\min_{w,b} L(w,b,\\alpha)$对$\\alpha$的极大，即是对偶问题：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\max _{\\alpha} \\quad-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i}\\\\\n",
    "&\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0\\\\\n",
    "&\\alpha_{i} \\geqslant 0, \\quad i=1,2, \\ldots, N\n",
    "\\end{aligned}$$\n",
    "\n",
    "&emsp;&emsp;将求$\\max$转换为求$\\min$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\min _{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i}\\\\\n",
    "&\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0\\\\\n",
    "&\\alpha_{i} \\geqslant 0, \\quad i=1,2, \\ldots, N\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe99cb6",
   "metadata": {},
   "source": [
    "## 软间隔最大化\n",
    "\n",
    "&emsp;&emsp;为了解决某些样本不满足约束的情况，对每个样本点引入一个松弛变量$\\xi_{i} \\geq 0$，使函数间隔加上松弛变量大于等于1，这样约束条件就变为：\n",
    "\n",
    "$$y_{i}(w \\cdot x+b) \\geq 1-\\xi_{i}$$\n",
    "\n",
    "&emsp;&emsp;为了避免$\\xi_{i}$取太大的值，需要在目标函数中对他们进行惩罚，其中$C > 0$称为惩罚系数，$C$值大时，对误分类的惩罚增加，$C$值小时对误分类的惩罚减小。\n",
    "\n",
    "$$\\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N} \\xi_{i}$$\n",
    "\n",
    "1. **优化目标**：\n",
    "\n",
    "&emsp;&emsp;线性不可分的线性支持向量机的学习问题就变为如下凸二次规划问题：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\min _{w, b, \\xi} \\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N} \\xi_{i}\\\\\n",
    "&\\text {s.t. } \\quad y_{i}\\left(w \\cdot x_{i}+b\\right) \\geq 1-\\xi_{i}, i=1, \\cdots, N\\\\\n",
    "&\\xi_{i} \\geq 0, i=1, \\cdots, N\n",
    "\\end{aligned}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbdd5fe",
   "metadata": {},
   "source": [
    "2. **求解**：\n",
    "\n",
    "&emsp;&emsp;构建拉格朗日对偶函数：\n",
    "\n",
    "$$L(w, b, \\xi, \\alpha, \\mu)=\\frac{1}{2}\\|w\\|^{2}+C \\sum_{i=1}^{N} \\xi_{i}-\\sum_{i=1}^{N} \\alpha_{i}\\left[y_{i}\\left(w \\cdot x_{i}+b\\right)-1+\\xi_{i}\\right]-\\sum_{i=1}^{N} \\mu_{i} \\xi_{i}$$\n",
    "\n",
    "&emsp;&emsp;$\\alpha_{i} \\geq 0$，$\\mu_{i} \\geq 0$，需要先求$L(w,b,\\xi, \\alpha,\\mu)$对$w,b,\\xi$的极小，再求对$\\alpha$的极大。\n",
    "\n",
    "&emsp;&emsp;(1) 分别对$w,b,\\xi$求偏导数，令$\\partial L / \\partial w, \\partial L / \\partial b, \\partial L / \\partial \\xi$等于零：\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "\\nabla_{w} L(w, b, \\xi, \\alpha, \\mu)=w-\\sum_{i=1}^{N} \\alpha_{i} y_{i} x_{i}=0 \\\\\n",
    "\\nabla_{b} L(w, b, \\xi, \\alpha, \\mu)=-\\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n",
    "\\nabla_{\\xi_{i}} L(w, b, \\xi, \\alpha, \\mu)=C-\\alpha_{i}-\\mu_{i}=0\n",
    "\\end{array}$$\n",
    "\n",
    "&emsp;&emsp;可以推出：\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "w=\\sum_{i=1}^{N} \\alpha_{i} y_{i} x_{i} \\\\\n",
    "\\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n",
    "C-\\alpha_{i}-\\mu_{i}=0\n",
    "\\end{array}$$\n",
    "\n",
    "&emsp;&emsp;(2) 将结果代入拉格朗日函数中，整理后可得：\n",
    "\n",
    "$$\\min _{w, b, \\xi} L(w, b, \\xi, \\alpha, \\mu)=-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i}$$\n",
    "\n",
    "&emsp;&emsp;$\\alpha_{i} \\geq 0$，$\\mu_{i} \\geq 0$，需要先求$L(w,b,\\xi,\\alpha,\\mu)$对$w,b,\\xi$的极小，再求对$\\alpha$的极大。\n",
    "\n",
    "&emsp;&emsp;(3) 求$min_{w,b,\\xi}L(w,b,\\xi,\\alpha,\\mu)$对$\\alpha$的极大，即得对偶问题：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\max _{\\alpha} &-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i} \\\\\n",
    "& \\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n",
    "& C-\\alpha_{i}-\\mu_{i}=0 \\\\\n",
    "& \\alpha_{i} \\geqslant 0 \\\\\n",
    "\\mu_{i} \\geqslant 0, & i=1,2, \\ldots, N\n",
    "\\end{aligned}$$\n",
    "\n",
    "&emsp;&emsp;对偶问题，凸二次规划问题，有唯一的最优解：\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "\\min _{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} \\\\\n",
    "\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n",
    "0 \\leqslant \\alpha_{i} \\leqslant C, \\quad i=1,2, \\ldots, N\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1c0d6",
   "metadata": {},
   "source": [
    "## 核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811d920",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于对偶问题：\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "\\min _{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} \\\\\n",
    "\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n",
    "0 \\leqslant \\alpha_{i} \\leqslant C, \\quad i=1,2, \\ldots, N\n",
    "\\end{array}$$\n",
    "\n",
    "&emsp;&emsp;来看，式子中间有$x_{i}$和$x_{j}$的点积，这个会有点难搞。例如在手写数字数据集Mnist中，训练集有6万个样本，6万乘6万勉强能接受，但是每个样本为784维时，6万个样本两两做点积，是非常慢的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5bc89",
   "metadata": {},
   "source": [
    "- **梳理一下**\n",
    "\n",
    "1. 由于公示的需要，我们需要计算$x_{i}$和$x_{j}$的点积。\n",
    "\n",
    "2. 此外，我们需要将样本映射到高维去，加入映射函数为$\\Phi(x)$, 那么$\\Phi(x_{i})$和$\\Phi(x_{j})$的维度数目进一步扩大，它们的点积会让运算变得及其复杂(因为维度太高了)。\n",
    "\n",
    "3. 我们希望存在一个函数$K(x, y)=\\Phi(x_{i}) \\Phi(x_{j})$, 但函数$K$的计算方式更简单。也就是说，我将样本通过函数升维得到$\\Phi(x_{i})$和$\\Phi(x_{j})$，接下来要计算它们的点积，能不能有个很简单的计算公式，计算出来的结果和$\\Phi(x_{i})$, $\\Phi(x_{j})$一样？那样我就不用再去算$\\Phi(x_{i}) \\Phi(x_{j})$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f589b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在`SVM`中，我们通常使用高斯核函数：\n",
    "\n",
    "$$\n",
    "K(x, z) = exp(-\\frac{||x-z||^{2}}{2\\sigma^{2}})\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;问题就变成了：\n",
    "\n",
    "$$\\begin{array}{c}\n",
    "\\min _{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}K\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i} \\\\\n",
    "\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0 \\\\\n",
    "0 \\leqslant \\alpha_{i} \\leqslant C, \\quad i=1,2, \\ldots, N\n",
    "\\end{array}$$\n",
    "\n",
    "&emsp;&emsp;其中剩下的$\\alpha$需要去求解，用`SMO`算法求解即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc9c7b",
   "metadata": {},
   "source": [
    "## 序列最小最优算法(SMO)\n",
    "\n",
    "&emsp;&emsp;求$\\min_{w,b} L(w,b,\\alpha)$对$\\alpha$的极大，即是对偶问题：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\max _{\\alpha} \\quad-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)+\\sum_{i=1}^{N} \\alpha_{i}\\\\\n",
    "&\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0\\\\\n",
    "&\\alpha_{i} \\geqslant 0, \\quad i=1,2, \\ldots, N\n",
    "\\end{aligned}$$\n",
    "\n",
    "&emsp;&emsp;将求$\\max$转换为求$\\min$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\min _{\\alpha} \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(x_{i} \\cdot x_{j}\\right)-\\sum_{i=1}^{N} \\alpha_{i}\\\\\n",
    "&\\text { s.t. } \\sum_{i=1}^{N} \\alpha_{i} y_{i}=0\\\\\n",
    "&\\alpha_{i} \\geqslant 0, \\quad i=1,2, \\ldots, N\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "&emsp;&emsp;我们最后求解出来的$\\alpha$,一定是让整个结果满足`KKT`条件的。如果不满足，那一定不是最优解。所以我们可以每次不断地调整$\\alpha$的值，直到所有$\\alpha$都满足`KKT`条件，这时候我一定得到了最优解。\n",
    "\n",
    "&emsp;&emsp;假设整个式子中有$N$个$\\alpha$ ($\\alpha_{1}, \\alpha_{2}, \\alpha_{3}, \\cdots, \\alpha_{N}$), 先固定其他$\\alpha$, 找$\\alpha_{1}$。先让$\\alpha_{1}$满足`KKT`条件，但如果固定除$\\alpha_{1}$以外的所有$\\alpha$, 等于也固定了$\\alpha_{1}$。\n",
    "\n",
    "$$\n",
    "\\alpha_{1} y_{1} = - \\sum_{i=2}^{N} \\alpha_{1} y_{i}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;所以，我们每次选择优化两个$\\alpha$\n",
    "\n",
    "$$\n",
    "\\alpha_{1}y_{1} + \\alpha_{2}y_{2} = -\\sum_{i=3}^{N} \\alpha_{i} y_{i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357ba27",
   "metadata": {},
   "source": [
    "## 核方法\n",
    "\n",
    "\n",
    "### 线性核\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\kappa\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)=\\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{j} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 多项式核函数\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\kappa\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)=\\left(\\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{j}\\right)^{d}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中$d \\geq 1$为多项式的次数。\n",
    "\n",
    "### 高斯核函数\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\kappa\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)=\\exp \\left(-\\frac{\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}\\right\\|^{2}}{2 \\sigma^{2}}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$\\sigma > 0$为高斯核的带宽(`width`)。\n",
    "\n",
    "### 拉普拉斯核\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\kappa\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)=\\exp \\left(-\\frac{\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}\\right\\|}{\\sigma}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$\\sigma > 0$。\n",
    "\n",
    "### Sigmoid核\n",
    "\n",
    "$$\n",
    "\\kappa\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)=\\tanh \\left(\\beta \\boldsymbol{x}_{i}^{\\mathrm{T}} \\boldsymbol{x}_{j}+\\theta\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;`tanh`为双曲正切函数, $\\beta > 0$, $\\theta < 0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db7981",
   "metadata": {},
   "source": [
    "## 其他\n",
    "\n",
    "### 支持向量回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d772808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
