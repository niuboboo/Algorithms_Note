{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101fbfb4",
   "metadata": {},
   "source": [
    "## 虚拟遗憾最小化\n",
    "\n",
    "虚拟遗憾最小化算法原论文:\n",
    "\n",
    "- [Regret Minimization in Games with Incomplete Information](https://proceedings.neurips.cc/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf)\n",
    "\n",
    "之后还有一篇文章是在No Limit Poker上打败了人类选手。\n",
    "\n",
    "- [Superhuman AI for heads-up no-limit poker: Libratus beats top professionals](https://www.science.org/doi/10.1126/science.aao1733)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75018010",
   "metadata": {},
   "source": [
    "虚拟遗憾最小化的英文全称为`Counterfactual Regret Minimization`，将其拆开理解:\n",
    "\n",
    "1. 虚拟(`Counterfactual`): 定义为实际没有发生，但是在某些条件成立的情况下，是会发生的事情。比如你和我玩石头剪刀布的游戏，我这把出的石头，但是我也是有可能会出剪刀和布的，所以剪刀和布就可以看作是这把的虚拟动作。\n",
    "\n",
    "2. 遗憾(`Regret`): 定义为你已经做了，但是你希望没有做，比如我出了石头，但是你出了布，所以我希望我没有出石头，不然我就输掉了，所以这个称为遗憾。\n",
    "\n",
    "3. 最小化(`Minimization`): 是算法的目标，我们想要遗憾变得最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1aa60c",
   "metadata": {},
   "source": [
    "## 举例详解\n",
    "\n",
    "石头剪刀布的游戏，\n",
    "\n",
    "1. 第一把：我出的石头，对手也出的石头，此时我们两收到的奖励都是0。\n",
    "\n",
    "对于遗憾值的计算，如果我出剪刀，那么将会得到的奖励是-1，对手得到的奖励就是+1。如果我出布，那么得到的奖励就是1，对手得到的奖励就是-1。\n",
    "\n",
    "对于我来说，遗憾奖励(`Counterfactual rewards`)就是:\n",
    "\n",
    "| 布     | 剪刀   |\n",
    "| ------ | ------ |\n",
    "| 1      | -1    |\n",
    "\n",
    "遗憾值就是遗憾奖励减去真实奖励(`Counterfactual rewards - real rewards`)\n",
    "\n",
    "| 石头    | 布     |   剪刀   | \n",
    "| ------ | ------ |  ------ |\n",
    "| 0      |   1    |    -1   |\n",
    "\n",
    "\n",
    "这是一种情况。\n",
    "\n",
    "2. 第二把：如果我出石头，对方出布的话，我得到的奖励就是-1。\n",
    "\n",
    "此时对于我来说，遗憾奖励(`Counterfactual rewards`)就是:\n",
    "\n",
    "| 布     | 剪刀    |\n",
    "| ------ | ------ |\n",
    "| 0      |    1   |\n",
    "\n",
    "遗憾值就是遗憾奖励减去真实奖励(`Counterfactual rewards - real rewards`)\n",
    "\n",
    "| 石头    |    布  |    剪刀  | \n",
    "| ------ | ------ |  ------ |\n",
    "| 0     |   1    |     2   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9a517",
   "metadata": {},
   "source": [
    "## 纳什均衡\n",
    "\n",
    "`Nash Equilibrium`是在非合作博弈中确定最优解的一种理论。石头剪刀布需要10000次迭代收敛到纳什均衡。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50181430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1204470",
   "metadata": {},
   "source": [
    "http://modelai.gettysburg.edu/2013/cfr/cfr.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc556a",
   "metadata": {},
   "source": [
    "## 代码实现CFR玩石头剪刀布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3cc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce746bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPSTrainer(object):\n",
    "    def __init__(self):\n",
    "        self.numActions = 3 # 动作个数3个\n",
    "        self.possibleActions = np.arange(self.numActions) # 可能的动作\n",
    "        self.actionUtility = np.array([\n",
    "            [0, -1, 1],\n",
    "            [1, 0, -1],\n",
    "            [-1, 1, 0]\n",
    "        ])  # 横坐标是我方，纵坐标是对方，对应的是石头，布，剪刀的收益。\n",
    "        \n",
    "        self.regretSum = np.zeros(self.numActions)  # 遗憾值的和，对应的是各个动作。\n",
    "        self.strategySum = np.zeros(self.numActions)  # 策略和\n",
    "        \n",
    "        self.oppregretSum = np.zeros(self.numActions)  # 对方遗憾值的和\n",
    "        self.oppstrategySum = np.zeros(self.numActions) # 对方策略和\n",
    "    \n",
    "    def getStrategy(self, regret_sum):\n",
    "        \"\"\"输入遗憾值获取策略\"\"\"\n",
    "        regret_sum[regret_sum < 0] = 0\n",
    "        normalizing_sum = sum(regret_sum)\n",
    "        strategy = regret_sum\n",
    "        for a in range(self.numActions):\n",
    "            if normalizing_sum > 0:\n",
    "                strategy[a] /= normalizing_sum\n",
    "            else:\n",
    "                strategy[a] = 1.0 / self.numActions\n",
    "        return strategy\n",
    "    \n",
    "    def getAverageStrategy(self, strategySum):\n",
    "        average_strategy = [0, 0, 0]\n",
    "        normalizing_sum = sum(strategySum)\n",
    "        for a in range(self.numActions):\n",
    "            if normalizing_sum > 0:\n",
    "                average_strategy[a] = strategySum[a] / normalizing_sum\n",
    "            else:\n",
    "                average_strategy[a] = 1.0 / self.numActions\n",
    "        return average_strategy\n",
    "    \n",
    "    def getAction(self, strategy):\n",
    "        \"\"\"依据策略概率选择动作\"\"\"\n",
    "        return np.random.choice(self.possibleActions, p=strategy)\n",
    "    \n",
    "    def getReward(self, myAction, opponentAction):\n",
    "        \"\"\"依据我方选择的动作和对方的动作获取奖励值\"\"\"\n",
    "        return self.actionUtility[myAction, opponentAction]\n",
    "    \n",
    "    def train(self, iterations):\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            strategy = self.getStrategy(self.regretSum)  # 拿到己方策略\n",
    "            oppStrategy = self.getStrategy(self.oppregretSum) # 拿到对方策略\n",
    "            \n",
    "            self.strategySum += strategy  # 己方策略和, 方便之后获取最终策略\n",
    "            self.oppstrategySum += oppStrategy # 对方策略和\n",
    "            \n",
    "            oppenentAction = self.getAction(oppStrategy)\n",
    "            myAction = self.getAction(strategy)\n",
    "            \n",
    "            myReward = self.getReward(myAction, oppenentAction)\n",
    "            oppReward = self.getReward(oppenentAction, myAction)\n",
    "            \n",
    "            for a in range(self.numActions):\n",
    "                myRegret = self.getReward(a, oppenentAction) - myReward\n",
    "                oppRegret = self.getReward(a, myAction) - oppReward\n",
    "                \n",
    "                self.regretSum[a] += myRegret\n",
    "                self.oppregretSum[a] += oppRegret  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c6668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    trainer = RPSTrainer()\n",
    "    trainer.train(1000)\n",
    "    targetPolicy = trainer.getAverageStrategy(trainer.strategySum)\n",
    "    opp_target_policy = trainer.getAverageStrategy(trainer.oppstrategySum)\n",
    "    print(\"player 1 policy {}\".format(targetPolicy))\n",
    "    print(\"player 2 policy {}\".format(opp_target_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4314d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 policy [0.3235306458519336, 0.3438130831862383, 0.3326562709618281]\n",
      "player 2 policy [0.33752920158710675, 0.3330774095362037, 0.32939338887668956]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f21ff",
   "metadata": {},
   "source": [
    "## 虚拟遗憾最小化应用于序贯博弈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17595d",
   "metadata": {},
   "source": [
    "### Kuhn Poker\n",
    "\n",
    "- [Vanilla CFR](https://justinsermeno.com/posts/cfr/)\n",
    "\n",
    "\n",
    "\n",
    "库恩扑克是一种两个玩家的零和非完美信息游戏，非完美说的是玩家并不知道对手手中的牌是什么。\n",
    "\n",
    "牌值只有1，2，3这三种情况，每轮每位玩家各持一张手牌，根据各自判断来决定加多少赌注。\n",
    "\n",
    "\n",
    "<img src=\"../images/11-kuhnPoker.png\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126d674",
   "metadata": {},
   "source": [
    "库恩扑克中有两个玩家，玩家1选择过牌pass，玩家2也选择过牌pass的话，就看看谁的牌值大谁就获胜。整个决策过程的决策树如上图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9e036",
   "metadata": {},
   "source": [
    "### 虚拟遗憾最小化算法回顾\n",
    "\n",
    "虚拟遗憾最小化算法可以分为以下几步:\n",
    "\n",
    "1. 选择你想要采取的动作\n",
    "\n",
    "2. 计算获得的奖励\n",
    "\n",
    "3. 计算遗憾奖励，就是选择其它的动作能够获得的奖励\n",
    "\n",
    "4. 真实奖励减去遗憾奖励得到遗憾值\n",
    "\n",
    "5. 将所有的遗憾值存储起来\n",
    "\n",
    "5. 对遗憾值求和，并将其归一化得到策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647e31a",
   "metadata": {},
   "source": [
    "### 程序思路\n",
    "\n",
    "1. 递归遍历博弈树\n",
    "\n",
    "2. 当遇见终止节点时，计算所能获得的奖励\n",
    "\n",
    "3. 在每个决策节点计算总的遗憾值\n",
    "\n",
    "4. 计算每个节点的决策概率\n",
    "\n",
    "5. 更新策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7a8bd",
   "metadata": {},
   "source": [
    "### 程序实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "face775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, key, actionDict, nActions=2):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.key = key\n",
    "        self.nActions = nActions # 可选动作个数，默认为2个\n",
    "        self.regretSum = np.zeros(self.nActions) # 遗憾值记录列表\n",
    "        self.strategySum = np.zeros(self.nActions)  # 策略记录列表\n",
    "        self.actionDict = actionDict\n",
    "        self.strategy = np.repeat(1 / self.nActions, self.nActions)  # 初始化策略\n",
    "        \n",
    "        self.reachPr = 0  # 到达概率\n",
    "        self.reachPrSum = 0 \n",
    "        \n",
    "    def updateStrategy(self):\n",
    "        self.strategySum += self.reachPr * self.strategy\n",
    "        self.reachPrSum += self.reachPr\n",
    "        \n",
    "        self.strategy = self.getStrategy()  # 更新策略\n",
    "        self.reachPr = 0\n",
    "    \n",
    "    def getStrategy(self):\n",
    "        regrets = self.regretSum\n",
    "        regrets[regrets < 0] = 0\n",
    "        normalizingSum = sum(regrets)\n",
    "        if normalizingSum > 0:\n",
    "            return regrets / normalizingSum\n",
    "        else:\n",
    "            return np.repeat(1 / self.nAction, self.nAction)\n",
    "    \n",
    "    def getAverageStrategy(self):\n",
    "        strategy = self.strategySum / self.reachPrSum\n",
    "        # Re-normalize\n",
    "        total = sum(strategy)\n",
    "        strategy /= total\n",
    "        return strategy\n",
    "    \n",
    "    def __str__(self):\n",
    "        strategies = ['{:03.2f}'.format(x)\n",
    "                      for x in self.get_average_strategy()]\n",
    "        return '{} {}'.format(self.key.ljust(6), strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "790fa3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "class Kunh(object):\n",
    "    def __init__(self):\n",
    "        self.nodeMap = {} # 创建节点哈希表\n",
    "        self.deck = np.array([0, 1, 2])\n",
    "        self.nAction = 2\n",
    "    \n",
    "    def train(self, nIterations=50000):\n",
    "        expectedGameValue = 0\n",
    "        for _ in range(nIterations):\n",
    "            shuffle(self.deck)  # 打乱牌序\n",
    "            expected_game_value += self.cfr('', 1, 1)\n",
    "            for _, v in self.nodeMap.items():\n",
    "                v.updateStra\n",
    "    \n",
    "    def cfr(self, history, pr_1, pr_2):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        n = len(history)\n",
    "        isPlayer1 = n % 2 == 0 # 判断是否是玩家1\n",
    "        \n",
    "        playerCard = self.deck[0] if isPlayer1 else self.deck[1]\n",
    "        \n",
    "        if self.isTerminal(history):\n",
    "            cardPlayer = self.deck[0] if isPlayer1 else self.deck[1]\n",
    "            cardOpponent = self.deck[1] if isPlayer1 else self.deck[0]\n",
    "            reward = self.getReward(history, cardPlayer, cardOpponent)\n",
    "            return reward\n",
    "        \n",
    "        node = self.getNode(self, playerCard, history)\n",
    "        strategy = node.strategy\n",
    "        \n",
    "        # 对于每个动作的遗憾收益\n",
    "        actionUtils = np.zeros(self.nAction)\n",
    "        \n",
    "        for act in range(self.nAction):\n",
    "            nextHistory = history + node.actionDict[act]  # 添加历史动作\n",
    "            if isPlayer1:\n",
    "                actionUtils[act] = -1 * self.cfr(nextHistory, pr_1 * strategy[act], pr_2) # 收益等于对手收益 * -1\n",
    "            else:\n",
    "                actionUtils[act] = -1 * self.cfr(nextHistory, pr_1, pr_2 * strategy[act])\n",
    "        \n",
    "        # Utility of information set\n",
    "        util = sum(actionUtils * strategy)\n",
    "        regrets = actionUtils - util\n",
    "        \n",
    "        if isPlayer1:\n",
    "            node.reachPr += pr_1\n",
    "            node.regretSum += pr_2 * regrets\n",
    "        else:\n",
    "            node.reachPr += pr_2\n",
    "            node.reachSum += pr_1 * regrets\n",
    "        \n",
    "        return util\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def isTerminal(history):\n",
    "        \"\"\"p表示pass，b表示bet\"\"\"\n",
    "        if history[-2:] == 'pp' or history[-2:] == 'bb' or history[-2:] == 'bp':\n",
    "            return true\n",
    "    \n",
    "    @staticmethod\n",
    "    def getReward(history, playerCard, opponentCard):\n",
    "        \"\"\"\"\"\"\n",
    "        terminalPass = history[-2] == 'p'\n",
    "        doubleBet = history[-2:] == 'bb'\n",
    "        if terminalPass: # 如果最后一个状态是pass。\n",
    "            if history[-2:] == 'pp': # 都pass则比较大小。\n",
    "                return 1 if playerCard > opponentCard else -1\n",
    "            else: # bet , pass, 则我方赢\n",
    "                return 1\n",
    "        elif doubleBet:\n",
    "            return 2 if playerCard > opponentCard else -2\n",
    "    \n",
    "    def getNode(self, card, history):\n",
    "        key = str(card) + \" \" + history\n",
    "        if key not in self.nodeMap:\n",
    "            actionDict = {0: 'p', 1: 'b'}\n",
    "            infoSet = Node(key, actionDict)\n",
    "            self.nodeMap[key] = infoSet\n",
    "            return infoSet\n",
    "        return self.nodeMap[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec375acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
