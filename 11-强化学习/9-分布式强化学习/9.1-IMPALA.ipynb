{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4331caca",
   "metadata": {},
   "source": [
    "## IMAPLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e4807",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd7bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8448ab",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2bcc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int):\n",
    "        super(MlpPolicy, self).__init__()\n",
    "        self.model = (\n",
    "            nn.Sequential(\n",
    "                nn.Linear(obs_dim, hidden_dim),\n",
    "                nn.Dropout(p=0.8),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, action_dim),\n",
    "            )\n",
    "            .to(device)\n",
    "            .to(dtype)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.model(x)\n",
    "        return logits\n",
    "\n",
    "    def select_action(self, obs: torch.Tensor, deterministic: bool = False):\n",
    "        logits = self.forward(obs)\n",
    "        if deterministic:\n",
    "            action = torch.argmax(logits)\n",
    "        else:\n",
    "            action = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "\n",
    "        return action, logits\n",
    "\n",
    "\n",
    "class MlpValueFn(nn.Module):\n",
    "    def __init__(self, obs_dim: int, hidden_dim: int):\n",
    "        super(MlpValueFn, self).__init__()\n",
    "        self.model = (\n",
    "            nn.Sequential(\n",
    "                nn.Linear(obs_dim, hidden_dim),\n",
    "                nn.Dropout(p=0.8),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1),\n",
    "            )\n",
    "            .to(device)\n",
    "            .to(dtype)\n",
    "        )\n",
    "\n",
    "    def forward(self, observation: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(observation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ebc20",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e33b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import gym\n",
    "# import pybullet_envs  # noqa: F401\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e6d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperparameters = namedtuple(\n",
    "    \"Hyperparameters\",\n",
    "    [\n",
    "        \"max_updates\",\n",
    "        \"policy_hidden_dims\",\n",
    "        \"value_fn_hidden_dims\",\n",
    "        \"batch_size\",\n",
    "        \"gamma\",\n",
    "        \"rho_bar\",\n",
    "        \"c_bar\",\n",
    "        # \"policy_lr\",\n",
    "        # \"value_fn_lr\",\n",
    "        \"lr\",\n",
    "        \"policy_loss_c\",\n",
    "        \"v_loss_c\",\n",
    "        \"entropy_c\",\n",
    "        \"max_timesteps\",\n",
    "        \"queue_lim\",\n",
    "        \"max_norm\",\n",
    "        \"n_actors\",\n",
    "        \"env_name\",\n",
    "        \"log_path\",\n",
    "        \"save_every\",\n",
    "        \"eval_every\",\n",
    "        \"eval_eps\",\n",
    "        \"verbose\",\n",
    "        \"render\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "class Trajectory:\n",
    "    def __init__(\n",
    "        self,\n",
    "        id: int,\n",
    "        observations: List[torch.Tensor] = [],\n",
    "        actions: List[torch.Tensor] = [],\n",
    "        rewards: List[torch.Tensor] = [],\n",
    "        dones: List[torch.Tensor] = [],\n",
    "        logits: List[torch.Tensor] = [],\n",
    "    ):\n",
    "        self.id = id\n",
    "        self.obs = observations\n",
    "        self.a = actions\n",
    "        self.r = rewards\n",
    "        self.d = dones\n",
    "        self.logits = logits\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        a: torch.Tensor,\n",
    "        r: torch.Tensor,\n",
    "        d: torch.Tensor,\n",
    "        logits: torch.Tensor,\n",
    "    ):\n",
    "        self.obs.append(obs)\n",
    "        self.a.append(a)\n",
    "        self.r.append(r)\n",
    "        self.d.append(d)\n",
    "        self.logits.append(logits)\n",
    "\n",
    "\n",
    "class Counter:\n",
    "    def __init__(self, init_val: int = 0):\n",
    "        self._val = mp.RawValue(\"i\", init_val)\n",
    "        self._lock = mp.Lock()\n",
    "\n",
    "    def increment(self):\n",
    "        with self._lock:\n",
    "            self._val.value += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        with self._lock:\n",
    "            return self._val.value\n",
    "\n",
    "\n",
    "def make_env(env_name: str):\n",
    "    if \"Bullet\" in env_name:\n",
    "        try:\n",
    "            env = gym.make(env_name, isDiscrete=True)\n",
    "        except TypeError:\n",
    "            env = gym.make(env_name)\n",
    "    else:\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "    if env.action_space.__class__.__name__ != \"Discrete\":\n",
    "        raise NotImplementedError(\"Continuous environments not supported yet\")\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def test_policy(\n",
    "    policy: MlpPolicy,\n",
    "    env: Union[gym.Env, str],\n",
    "    episodes: int,\n",
    "    deterministic: bool,\n",
    "    max_episode_len: int,\n",
    "    log_dir: Union[str, None] = None,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    start_time = datetime.datetime.now()\n",
    "    start_text = f\"Started testing at {start_time:%d-%m-%Y %H:%M:%S}\\n\"\n",
    "\n",
    "    if type(env) == str:\n",
    "        env = make_env(env)\n",
    "\n",
    "    if log_dir is not None:\n",
    "        Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "        fpath = Path(log_dir).joinpath(f\"test_log_{start_time:%d%m%Y%H%M%S}.txt\")\n",
    "        fpath.write_text(start_text)\n",
    "    if verbose:\n",
    "        print(start_text)\n",
    "    policy.eval()\n",
    "    rewards = []\n",
    "    for e in range(episodes):\n",
    "        obs = env.reset()\n",
    "        obs = torch.tensor(obs, device=device, dtype=dtype)\n",
    "        d = False\n",
    "        ep_rewards = []\n",
    "        for t in range(max_episode_len):\n",
    "            action, _ = policy.select_action(obs, deterministic)\n",
    "            obs, r, d, _ = env.step(action.item())\n",
    "            obs = torch.tensor(obs, device=device, dtype=dtype)\n",
    "            ep_rewards.append(r)\n",
    "            if d:\n",
    "                break\n",
    "        rewards.append(sum(ep_rewards))\n",
    "        ep_text = f\"Episode {e+1}: Reward = {rewards[-1]:.2f}\\n\"\n",
    "        if log_dir is not None:\n",
    "            with open(fpath, mode=\"a\") as f:\n",
    "                f.write(ep_text)\n",
    "        if verbose:\n",
    "            print(ep_text)\n",
    "    avg_reward = np.mean(rewards)\n",
    "    std_dev = np.std(rewards)\n",
    "    complete_text = (\n",
    "        f\"-----\\n\"\n",
    "        f\"Testing completed in \"\n",
    "        f\"{(datetime.datetime.now() - start_time).seconds} seconds\\n\"\n",
    "        f\"Average Reward per episode: {avg_reward}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(complete_text)\n",
    "    if log_dir is not None:\n",
    "        with open(fpath, mode=\"a\") as f:\n",
    "            f.write(complete_text)\n",
    "\n",
    "    return avg_reward, std_dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d88adb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "960d51bd",
   "metadata": {},
   "source": [
    "## learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc852b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "\n",
    "\n",
    "class Learner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        id: int,\n",
    "        hparams: Hyperparameters,\n",
    "        policy: MlpPolicy,\n",
    "        value_fn: MlpValueFn,\n",
    "        q: mp.Queue,\n",
    "        update_counter: Counter,\n",
    "        log_path: Union[str, Path, None] = None,\n",
    "        timeout=200,\n",
    "    ):\n",
    "        self.id = id\n",
    "        self.hp = hparams\n",
    "        self.policy = policy\n",
    "        self.value_fn = value_fn\n",
    "        # self.policy_optimizer = torch.optim.Adam(\n",
    "        #     self.policy.parameters(), lr=self.hp.policy_lr\n",
    "        # )\n",
    "        # self.value_fn_optimizer = torch.optim.Adam(\n",
    "        #     self.value_fn.parameters(), lr=self.hp.value_fn_lr\n",
    "        # )\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            [*self.policy.parameters(), *self.value_fn.parameters()], lr=self.hp.lr\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda epoch: 0.95)\n",
    "        self.timeout = timeout\n",
    "        self.q = q\n",
    "        self.update_counter = update_counter\n",
    "        self.log_path = log_path\n",
    "        if self.log_path is not None:\n",
    "            self.log_path = Path(log_path) / Path(f\"l{self.id}\")\n",
    "            self.log_path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "        self.completion = mp.Event()\n",
    "        self.p = mp.Process(target=self._learn, name=f\"learner_{self.id}\")\n",
    "        print(f\"[main] learner_{self.id} Initialized\")\n",
    "\n",
    "    def start(self):\n",
    "        self.completion.clear()\n",
    "        self.p.start()\n",
    "        print(f\"[main] Started learner_{self.id} with pid {self.p.pid}\")\n",
    "\n",
    "    def terminate(self):\n",
    "        self.p.terminate()\n",
    "        print(f\"[main] Terminated learner_{self.id}\")\n",
    "\n",
    "    def join(self):\n",
    "        self.p.join()\n",
    "\n",
    "    def _learn(self):\n",
    "        try:\n",
    "            update_count = 0\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                writer = SummaryWriter(self.log_path)\n",
    "                writer.add_text(\"hyperparameters\", f\"{self.hp}\")\n",
    "\n",
    "            while update_count < self.hp.max_updates:\n",
    "\n",
    "                if self.hp.verbose >= 2:\n",
    "                    print(f\"[learner_{self.id}] Beginning Update_{update_count + 1}\")\n",
    "\n",
    "                # set up tracking variables\n",
    "                traj_count = 0\n",
    "                value_fn_loss = 0.0\n",
    "                policy_loss = 0.0\n",
    "                policy_entropy = 0.0\n",
    "                loss = torch.zeros(1, device=device, dtype=dtype, requires_grad=True)\n",
    "                reward = 0.0\n",
    "\n",
    "                # process batch of trajectories\n",
    "                while traj_count < self.hp.batch_size:\n",
    "                    try:\n",
    "                        traj = self.q.get(timeout=self.timeout)\n",
    "                    except queue.Empty as e:\n",
    "                        print(\n",
    "                            f\"[learner_{self.id}] No trajectory recieved for {self.timeout}\"\n",
    "                            f\" seconds. Exiting!\"\n",
    "                        )\n",
    "                        if self.log_path is not None:\n",
    "                            writer.close()\n",
    "                        self.completion.set()\n",
    "                        raise e\n",
    "\n",
    "                    if self.hp.verbose >= 2:\n",
    "                        print(f\"[learner_{self.id}] Processing traj_{traj.id}\")\n",
    "                    traj_len = len(traj.r)\n",
    "                    obs = torch.stack(traj.obs)\n",
    "                    actions = torch.stack(traj.a)\n",
    "                    r = torch.stack(traj.r)\n",
    "                    reward += torch.sum(r).item() / self.hp.batch_size\n",
    "                    disc = self.hp.gamma * (~torch.stack(traj.d))\n",
    "\n",
    "                    # compute value estimates and logits for observed states\n",
    "                    v = self.value_fn(obs).squeeze(1)\n",
    "                    curr_logits = self.policy(obs[:-1])\n",
    "\n",
    "                    # compute log probs for current and old policies\n",
    "                    curr_log_probs = action_log_probs(curr_logits, actions)\n",
    "                    traj_log_probs = action_log_probs(torch.stack(traj.logits), actions)\n",
    "\n",
    "                    # computing v trace targets recursively\n",
    "                    with torch.no_grad():\n",
    "                        imp_sampling = torch.exp(\n",
    "                            curr_log_probs - traj_log_probs\n",
    "                        ).squeeze(1)\n",
    "                        rho = torch.clamp(imp_sampling, max=self.hp.rho_bar)\n",
    "                        c = torch.clamp(imp_sampling, max=self.hp.c_bar)\n",
    "                        delta = rho * (r + self.hp.gamma * v[1:] - v[:1])\n",
    "                        vt = torch.zeros(traj_len + 1, device=device, dtype=dtype)\n",
    "\n",
    "                        for i in range(traj_len - 1, -1, -1):\n",
    "                            vt[i] = delta[i] + disc[i] * c[i] * (vt[i + 1] - v[i + 1])\n",
    "                        vt = torch.add(vt, v)\n",
    "\n",
    "                        # vt = (vt - torch.mean(vt)) / torch.std(vt)\n",
    "\n",
    "                        pg_adv = rho * (r + disc * vt[1:] - v[:-1])\n",
    "\n",
    "                    # print(f\"v: {v}\")\n",
    "                    # print(f\"vt: {vt}\")\n",
    "                    # print(f\"pg_adv: {pg_adv}\")\n",
    "                    # print(f\"rho: {rho}\")\n",
    "\n",
    "                    # compute loss as sum of value loss, policy loss and entropy\n",
    "                    # traj_value_fn_loss = 0.5 * torch.sum(torch.pow(v - vt, 2))\n",
    "                    # traj_policy_loss = torch.sum(curr_log_probs * pg_adv.detach())\n",
    "                    # traj_policy_entropy = -1 * torch.sum(\n",
    "                    #     F.softmax(curr_logits, dim=-1)\n",
    "                    #     * F.log_softmax(curr_logits, dim=-1)\n",
    "                    # )\n",
    "                    traj_value_fn_loss = compute_baseline_loss(v - vt)\n",
    "                    traj_policy_loss = compute_policy_gradient_loss(\n",
    "                        curr_logits, actions, pg_adv\n",
    "                    )\n",
    "                    traj_policy_entropy = -1 * compute_entropy_loss(curr_logits)\n",
    "                    traj_loss = (\n",
    "                        self.hp.v_loss_c * traj_value_fn_loss\n",
    "                        + self.hp.policy_loss_c * traj_policy_loss\n",
    "                        - self.hp.entropy_c * traj_policy_entropy\n",
    "                    )\n",
    "                    loss = torch.add(loss, traj_loss / self.hp.batch_size)\n",
    "                    value_fn_loss += traj_value_fn_loss.item() / self.hp.batch_size\n",
    "                    policy_loss += traj_policy_loss.item() / self.hp.batch_size\n",
    "                    policy_entropy += traj_policy_entropy.item() / self.hp.batch_size\n",
    "                    traj_count += 1\n",
    "\n",
    "                if self.hp.verbose >= 2:\n",
    "                    print(\n",
    "                        f\"[learner_{self.id}] Updating model weights \"\n",
    "                        f\" for Update {update_count + 1}\"\n",
    "                    )\n",
    "\n",
    "                # backpropogating loss and updating weights\n",
    "                # self.policy_optimizer.zero_grad()\n",
    "                # self.value_fn_optimizer.zero_grad()\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.policy.parameters(), self.hp.max_norm\n",
    "                )\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.value_fn.parameters(), self.hp.max_norm\n",
    "                )\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "                # self.policy_optimizer.step()\n",
    "                # self.value_fn_optimizer.step()\n",
    "\n",
    "                # log to console\n",
    "                if self.hp.verbose >= 1:\n",
    "                    print(\n",
    "                        f\"[learner_{self.id}] Update {update_count + 1} | \"\n",
    "                        f\"Batch Mean Reward: {reward:.2f} | Loss: {loss.item():.2f}\"\n",
    "                    )\n",
    "\n",
    "                # evaluate current policy\n",
    "                if self.hp.eval_every is not None:\n",
    "                    if (update_count + 1) % self.hp.eval_every == 0:\n",
    "                        eval_r, eval_std = test_policy(\n",
    "                            self.policy,\n",
    "                            self.hp.env_name,\n",
    "                            self.hp.eval_eps,\n",
    "                            True,\n",
    "                            self.hp.max_timesteps,\n",
    "                        )\n",
    "                        if self.hp.verbose >= 1:\n",
    "                            print(\n",
    "                                f\"[learner_{self.id}] Update {update_count + 1} | \"\n",
    "                                f\"Evaluation Reward: {eval_r:.2f}, Std Dev: {eval_std:.2f}\"\n",
    "                            )\n",
    "                        if self.log_path is not None:\n",
    "                            writer.add_scalar(\n",
    "                                f\"learner_{self.id}/rewards/evaluation_reward\",\n",
    "                                eval_r,\n",
    "                                update_count + 1,\n",
    "                            )\n",
    "\n",
    "                # log to tensorboard\n",
    "                if self.log_path is not None:\n",
    "                    writer.add_scalar(\n",
    "                        f\"learner_{self.id}/rewards/batch_mean_reward\",\n",
    "                        reward,\n",
    "                        update_count + 1,\n",
    "                    )\n",
    "                    writer.add_scalar(\n",
    "                        f\"learner_{self.id}/loss/policy_loss\",\n",
    "                        policy_loss,\n",
    "                        update_count + 1,\n",
    "                    )\n",
    "                    writer.add_scalar(\n",
    "                        f\"learner_{self.id}/loss/value_fn_loss\",\n",
    "                        value_fn_loss,\n",
    "                        update_count + 1,\n",
    "                    )\n",
    "                    writer.add_scalar(\n",
    "                        f\"learner_{self.id}/loss/policy_entropy\",\n",
    "                        policy_entropy,\n",
    "                        update_count + 1,\n",
    "                    )\n",
    "                    writer.add_scalar(\n",
    "                        f\"learner_{self.id}/loss/total_loss\", loss, update_count + 1\n",
    "                    )\n",
    "\n",
    "                # save model weights every given interval\n",
    "                if (update_count + 1) % self.hp.save_every == 0:\n",
    "                    path = self.log_path / Path(\n",
    "                        f\"IMPALA_{self.hp.env_name}_l{self.id}_{update_count+1}.pt\"\n",
    "                    )\n",
    "                    self.save(path)\n",
    "                    print(\n",
    "                        f\"[learner_{self.id}] Saved model weights at \"\n",
    "                        f\"update {update_count+1} to {path}\"\n",
    "                    )\n",
    "\n",
    "                # increment update counter\n",
    "                self.update_counter.increment()\n",
    "                update_count = self.update_counter.value\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                writer.close()\n",
    "\n",
    "            print(f\"[learner_{self.id}] Finished learning\")\n",
    "            self.completion.set()\n",
    "            return\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"[learner_{self.id}] Interrupted\")\n",
    "            if self.log_path is not None:\n",
    "                writer.close()\n",
    "            self.completion.set()\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.log_path is not None:\n",
    "                writer.close()\n",
    "            print(f\"[learner_{self.id}] Encoutered exception\")\n",
    "            raise e\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\" Save model parameters \"\"\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"policy_state_dict\": self.policy.state_dict(),\n",
    "                \"value_fn_state_dict\": self.value_fn.state_dict(),\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\" Load model parameters \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "        self.value_fn.load_state_dict(checkpoint[\"value_fn_state_dict\"])\n",
    "\n",
    "    @property\n",
    "    def policy_weights(self) -> torch.Tensor:\n",
    "        return self.policy.state_dict()\n",
    "\n",
    "\n",
    "def action_log_probs(policy_logits, actions):\n",
    "    return -F.nll_loss(\n",
    "        F.log_softmax(policy_logits, dim=-1),\n",
    "        target=torch.flatten(actions),\n",
    "        reduction=\"none\",\n",
    "    ).view_as(actions)\n",
    "\n",
    "\n",
    "def compute_baseline_loss(advantages):\n",
    "    return 0.5 * torch.sum(advantages ** 2)\n",
    "\n",
    "\n",
    "def compute_entropy_loss(logits):\n",
    "    \"\"\"Return the entropy loss, i.e., the negative entropy of the policy.\"\"\"\n",
    "    policy = F.softmax(logits, dim=-1)\n",
    "    log_policy = F.log_softmax(logits, dim=-1)\n",
    "    return torch.sum(policy * log_policy)\n",
    "\n",
    "\n",
    "def compute_policy_gradient_loss(logits, actions, advantages):\n",
    "    cross_entropy = F.nll_loss(\n",
    "        F.log_softmax(logits, dim=-1), target=torch.flatten(actions), reduction=\"none\",\n",
    "    ).view_as(advantages)\n",
    "    return torch.sum(cross_entropy * advantages.detach())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66937429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22a53f64",
   "metadata": {},
   "source": [
    "## Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6ff9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577d1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Actor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        id: int,\n",
    "        hparams: Hyperparameters,\n",
    "        policy: MlpPolicy,\n",
    "        learner: Learner,\n",
    "        q: mp.Queue,\n",
    "        update_counter: Counter,\n",
    "        log_path: Union[Path, str, None] = None,\n",
    "        timeout=10,\n",
    "    ):\n",
    "        self.id = id\n",
    "        self.hp = hparams\n",
    "        self.policy = policy\n",
    "        for p in self.policy.parameters():\n",
    "            p.requires_grad = False\n",
    "        self.learner = learner\n",
    "        self.timeout = timeout\n",
    "        self.q = q\n",
    "        self.update_counter = update_counter\n",
    "        self.log_path = log_path\n",
    "        if self.log_path is not None:\n",
    "            self.log_path = Path(self.log_path) / Path(f\"a{self.id}\")\n",
    "            self.log_path.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "        self.completion = mp.Event()\n",
    "        self.p = mp.Process(target=self._act, name=f\"actor_{self.id}\")\n",
    "        print(f\"[main] actor_{self.id} Initialized\")\n",
    "\n",
    "    def start(self):\n",
    "        self.p.start()\n",
    "        print(f\"[main] Started actor_{self.id} with pid {self.p.pid}\")\n",
    "\n",
    "    def terminate(self):\n",
    "        self.p.terminate()\n",
    "        print(f\"[main] Terminated actor_{self.id}\")\n",
    "\n",
    "    def join(self):\n",
    "        self.p.join()\n",
    "\n",
    "    def _act(self):\n",
    "        try:\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                writer = SummaryWriter(self.log_path)\n",
    "                writer.add_text(\"hyperparameters\", f\"{self.hp}\")\n",
    "\n",
    "            env = make_env(self.hp.env_name)\n",
    "            traj_no = 0\n",
    "\n",
    "            while not self.learner.completion.is_set():\n",
    "                traj_no += 1\n",
    "                self.policy.load_state_dict(self.learner.policy_weights)\n",
    "                traj_id = (self.id, traj_no)\n",
    "                traj = Trajectory(traj_id, [], [], [], [], [])\n",
    "                obs = env.reset()\n",
    "                obs = torch.tensor(obs, device=device, dtype=dtype)\n",
    "                traj.obs.append(obs)\n",
    "                c = 0\n",
    "\n",
    "                if self.hp.verbose >= 2:\n",
    "                    print(f\"[actor_{self.id}] Starting traj_{traj.id}\")\n",
    "\n",
    "                # record trajectory\n",
    "                while c < self.hp.max_timesteps:\n",
    "                    if self.hp.render:\n",
    "                        env.render()\n",
    "                    c += 1\n",
    "                    a, logits = self.policy.select_action(obs)\n",
    "                    # print(f\"[actor_{self.id}] a_probs: {a_probs}\")\n",
    "                    obs, r, d, _ = env.step(a.item())\n",
    "                    obs = torch.tensor(obs, device=device, dtype=dtype)\n",
    "                    r = torch.tensor(r, device=device, dtype=dtype)\n",
    "                    d = torch.tensor(d, device=device)\n",
    "                    traj.add(obs, a, r, d, logits)\n",
    "\n",
    "                    if d:\n",
    "                        break\n",
    "\n",
    "                if self.hp.verbose >= 2:\n",
    "                    print(\n",
    "                        f\"[actor_{self.id}] traj_{traj.id} completed Reward = {sum(traj.r)}\"\n",
    "                    )\n",
    "                if self.log_path is not None:\n",
    "                    # action_one_hot = torch.zeros(env.action_space.n)\n",
    "                    # action_one_hot[a] += 1\n",
    "                    writer.add_histogram(\n",
    "                        f\"actor_{self.id}/actions/action_taken\", a, traj_no\n",
    "                    )\n",
    "                    writer.add_histogram(\n",
    "                        f\"actor_{self.id}/actions/logits\", logits.detach(), traj_no\n",
    "                    )\n",
    "                    writer.add_scalar(\n",
    "                        f\"actor_{self.id}/rewards/trajectory_reward\",\n",
    "                        sum(traj.r),\n",
    "                        traj_no,\n",
    "                    )\n",
    "\n",
    "                while True:\n",
    "                    try:\n",
    "                        self.q.put(traj, timeout=self.timeout)\n",
    "                        break\n",
    "                    except queue.Full:\n",
    "                        if self.learner.completion.is_set():\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                writer.close()\n",
    "            env.close()\n",
    "            print(f\"[actor_{self.id}] Finished acting\")\n",
    "            self.completion.set()\n",
    "            return\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"[actor_{self.id}] interrupted\")\n",
    "            if self.log_path is not None:\n",
    "                writer.close()\n",
    "            env.close()\n",
    "            self.completion.set()\n",
    "            return\n",
    "\n",
    "        except Exception as e:\n",
    "            if self.log_path is not None:\n",
    "                writer.close()\n",
    "            env.close()\n",
    "            print(f\"[actor_{self.id}] encoutered exception\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab75dd8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05142c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d188f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f99e32b7",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "680fe4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] Start time: 07-08-2021 15:33:37\n",
      "[main] Hyperparameters(max_updates=50, policy_hidden_dims=128, value_fn_hidden_dims=128, batch_size=32, gamma=0.99, rho_bar=1.0, c_bar=1.0, lr=0.001, policy_loss_c=1, v_loss_c=0.5, entropy_c=0.0006, max_timesteps=1000, queue_lim=8, max_norm=10, n_actors=1, env_name='CartPole-v1', log_path='./logs/', save_every=50, eval_every=2, eval_eps=20, verbose=1, render=False)\n",
      "\n",
      "[main] learner_1 Initialized\n",
      "[main] actor_1 Initialized\n",
      "[main] Initialized\n",
      "[main] Started actor_1 with pid 6932\n",
      "[main] Started learner_1 with pid 6933\n",
      "[learner_1] Update 1 | Batch Mean Reward: 17.75 | Loss: 94.44\n",
      "[learner_1] Update 2 | Batch Mean Reward: 18.31 | Loss: 115.99\n",
      "[learner_1] Update 2 | Evaluation Reward: 9.25, Std Dev: 0.89\n",
      "[learner_1] Update 3 | Batch Mean Reward: 18.97 | Loss: 142.60\n",
      "[learner_1] Update 4 | Batch Mean Reward: 22.44 | Loss: 210.46\n",
      "[learner_1] Update 4 | Evaluation Reward: 9.30, Std Dev: 0.64\n",
      "[learner_1] Update 5 | Batch Mean Reward: 18.47 | Loss: 152.07\n",
      "[learner_1] Update 6 | Batch Mean Reward: 17.78 | Loss: 115.85\n",
      "[learner_1] Update 6 | Evaluation Reward: 9.40, Std Dev: 0.66\n",
      "[learner_1] Update 7 | Batch Mean Reward: 20.66 | Loss: 160.95\n",
      "[learner_1] Update 8 | Batch Mean Reward: 22.62 | Loss: 228.79\n",
      "[learner_1] Update 8 | Evaluation Reward: 9.55, Std Dev: 0.59\n",
      "[learner_1] Update 9 | Batch Mean Reward: 24.03 | Loss: 213.08\n",
      "[learner_1] Update 10 | Batch Mean Reward: 23.69 | Loss: 145.22\n",
      "[learner_1] Update 10 | Evaluation Reward: 9.50, Std Dev: 0.67\n",
      "[learner_1] Update 11 | Batch Mean Reward: 19.84 | Loss: 120.37\n",
      "[learner_1] Update 12 | Batch Mean Reward: 21.97 | Loss: 115.64\n",
      "[learner_1] Update 12 | Evaluation Reward: 9.45, Std Dev: 0.59\n",
      "[learner_1] Update 13 | Batch Mean Reward: 21.28 | Loss: 124.30\n",
      "[learner_1] Update 14 | Batch Mean Reward: 24.94 | Loss: 203.42\n",
      "[learner_1] Update 14 | Evaluation Reward: 9.45, Std Dev: 0.67\n",
      "[learner_1] Update 15 | Batch Mean Reward: 23.09 | Loss: 116.32\n",
      "[learner_1] Update 16 | Batch Mean Reward: 23.19 | Loss: 142.68\n",
      "[learner_1] Update 16 | Evaluation Reward: 9.20, Std Dev: 0.68\n",
      "[learner_1] Update 17 | Batch Mean Reward: 20.88 | Loss: 113.70\n",
      "[learner_1] Update 18 | Batch Mean Reward: 24.69 | Loss: 147.62\n",
      "[learner_1] Update 18 | Evaluation Reward: 9.10, Std Dev: 0.83\n",
      "[learner_1] Update 19 | Batch Mean Reward: 21.88 | Loss: 93.02\n",
      "[learner_1] Update 20 | Batch Mean Reward: 24.78 | Loss: 130.19\n",
      "[learner_1] Update 20 | Evaluation Reward: 9.30, Std Dev: 0.64\n",
      "[learner_1] Update 21 | Batch Mean Reward: 21.91 | Loss: 87.03\n",
      "[learner_1] Update 22 | Batch Mean Reward: 26.91 | Loss: 149.14\n",
      "[learner_1] Update 22 | Evaluation Reward: 9.35, Std Dev: 0.65\n",
      "[learner_1] Update 23 | Batch Mean Reward: 24.94 | Loss: 77.72\n",
      "[learner_1] Update 24 | Batch Mean Reward: 24.78 | Loss: 121.65\n",
      "[learner_1] Update 24 | Evaluation Reward: 9.40, Std Dev: 0.92\n",
      "[learner_1] Update 25 | Batch Mean Reward: 23.62 | Loss: 119.82\n",
      "[learner_1] Update 26 | Batch Mean Reward: 27.59 | Loss: 113.05\n",
      "[learner_1] Update 26 | Evaluation Reward: 9.20, Std Dev: 0.75\n",
      "[learner_1] Update 27 | Batch Mean Reward: 20.41 | Loss: 67.63\n",
      "[learner_1] Update 28 | Batch Mean Reward: 24.22 | Loss: 78.16\n",
      "[learner_1] Update 28 | Evaluation Reward: 9.20, Std Dev: 0.68\n",
      "[learner_1] Update 29 | Batch Mean Reward: 21.75 | Loss: 91.78\n",
      "[learner_1] Update 30 | Batch Mean Reward: 20.38 | Loss: 69.51\n",
      "[learner_1] Update 30 | Evaluation Reward: 9.20, Std Dev: 0.87\n",
      "[learner_1] Update 31 | Batch Mean Reward: 21.00 | Loss: 75.44\n",
      "[learner_1] Update 32 | Batch Mean Reward: 20.31 | Loss: 69.15\n",
      "[learner_1] Update 32 | Evaluation Reward: 9.30, Std Dev: 0.56\n",
      "[learner_1] Update 33 | Batch Mean Reward: 22.12 | Loss: 40.55\n",
      "[learner_1] Update 34 | Batch Mean Reward: 23.00 | Loss: 69.56\n",
      "[learner_1] Update 34 | Evaluation Reward: 9.45, Std Dev: 0.74\n",
      "[learner_1] Update 35 | Batch Mean Reward: 18.88 | Loss: 52.87\n",
      "[learner_1] Update 36 | Batch Mean Reward: 21.47 | Loss: 46.89\n",
      "[learner_1] Update 36 | Evaluation Reward: 9.50, Std Dev: 0.74\n",
      "[learner_1] Update 37 | Batch Mean Reward: 22.22 | Loss: 65.02\n",
      "[learner_1] Update 38 | Batch Mean Reward: 19.06 | Loss: 59.69\n",
      "[learner_1] Update 38 | Evaluation Reward: 8.95, Std Dev: 0.74\n",
      "[learner_1] Update 39 | Batch Mean Reward: 19.19 | Loss: 48.26\n",
      "[learner_1] Update 40 | Batch Mean Reward: 20.72 | Loss: 61.74\n",
      "[learner_1] Update 40 | Evaluation Reward: 9.05, Std Dev: 0.92\n",
      "[learner_1] Update 41 | Batch Mean Reward: 17.66 | Loss: 61.13\n",
      "[learner_1] Update 42 | Batch Mean Reward: 18.16 | Loss: 51.30\n",
      "[learner_1] Update 42 | Evaluation Reward: 9.15, Std Dev: 0.85\n",
      "[learner_1] Update 43 | Batch Mean Reward: 16.50 | Loss: 55.40\n",
      "[learner_1] Update 44 | Batch Mean Reward: 18.62 | Loss: 63.58\n",
      "[learner_1] Update 44 | Evaluation Reward: 9.60, Std Dev: 0.66\n",
      "[learner_1] Update 45 | Batch Mean Reward: 17.91 | Loss: 40.07\n",
      "[learner_1] Update 46 | Batch Mean Reward: 18.66 | Loss: 48.65\n",
      "[learner_1] Update 46 | Evaluation Reward: 9.35, Std Dev: 0.65\n",
      "[learner_1] Update 47 | Batch Mean Reward: 18.47 | Loss: 69.41\n",
      "[learner_1] Update 48 | Batch Mean Reward: 17.47 | Loss: 42.01\n",
      "[learner_1] Update 48 | Evaluation Reward: 9.60, Std Dev: 0.66\n",
      "[learner_1] Update 49 | Batch Mean Reward: 16.66 | Loss: 50.53\n",
      "[learner_1] Update 50 | Batch Mean Reward: 15.66 | Loss: 56.58\n",
      "[learner_1] Update 50 | Evaluation Reward: 9.35, Std Dev: 0.79\n",
      "[learner_1] Saved model weights at update 50 to logs/07082021153337/l1/IMPALA_CartPole-v1_l1_50.pt\n",
      "[learner_1] Finished learning\n",
      "[actor_1] Finished acting\n",
      "[main] Terminated learner_1\n",
      "[main] Terminated actor_1\n",
      "[main] Completed in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "hparams = Hyperparameters(\n",
    "    max_updates=50,\n",
    "    policy_hidden_dims=128,\n",
    "    value_fn_hidden_dims=128,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    rho_bar=1.0,\n",
    "    c_bar=1.0,\n",
    "    # policy_lr=1e-3,\n",
    "    # value_fn_lr=1e-3,\n",
    "    lr=1e-3,\n",
    "    policy_loss_c=1,\n",
    "    v_loss_c=0.5,\n",
    "    entropy_c=0.0006,\n",
    "    max_timesteps=1000,\n",
    "    queue_lim=8,\n",
    "    max_norm=10,\n",
    "    n_actors=1,\n",
    "    env_name=\"CartPole-v1\",          # \"RacecarBulletEnv-v0\",\n",
    "    log_path=\"./logs/\",\n",
    "    save_every=50,\n",
    "    eval_every=2,\n",
    "    eval_eps=20,\n",
    "    verbose=1,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "    print(f\"[main] Start time: {start_time:%d-%m-%Y %H:%M:%S}\")\n",
    "    print(f\"[main] {hparams}\\n\")\n",
    "\n",
    "    if hparams.log_path is not None:\n",
    "        log_path = Path(Path(hparams.log_path) / f\"{start_time:%d%m%Y%H%M%S}\")\n",
    "        log_path.mkdir(parents=True, exist_ok=True)\n",
    "        with open(Path(log_path / \"hyperparameters.txt\"), \"w+\") as f:\n",
    "            f.write(f\"{hparams}\")\n",
    "        if not hparams.save_every > 0:\n",
    "            raise ValueError(\n",
    "                f\"save_every hyperparameter should be greater than 0, \"\n",
    "                f\"got {hparams.save_every}\"\n",
    "            )\n",
    "    else:\n",
    "        log_path = None\n",
    "\n",
    "    q = mp.Queue(maxsize=hparams.queue_lim)\n",
    "    update_counter = Counter(init_val=0)\n",
    "    env = make_env(hparams.env_name)\n",
    "    observation_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    env.close()\n",
    "    policy = MlpPolicy(observation_size, action_size, hparams.policy_hidden_dims)\n",
    "    policy.share_memory()\n",
    "    value_fn = MlpValueFn(observation_size, hparams.value_fn_hidden_dims)\n",
    "    learner = Learner(1, hparams, policy, value_fn, q, update_counter, log_path)\n",
    "\n",
    "    actors = []\n",
    "    for i in range(hparams.n_actors):\n",
    "        policy = MlpPolicy(observation_size, action_size, hparams.policy_hidden_dims)\n",
    "        actors.append(\n",
    "            Actor(i + 1, hparams, policy, learner, q, update_counter, log_path)\n",
    "        )\n",
    "\n",
    "    print(\"[main] Initialized\")\n",
    "\n",
    "    for a in actors:\n",
    "        a.start()\n",
    "    learner.start()\n",
    "\n",
    "    learner.completion.wait()\n",
    "    for a in actors:\n",
    "        a.completion.wait()\n",
    "\n",
    "    learner.terminate()\n",
    "    for a in actors:\n",
    "        a.terminate()\n",
    "\n",
    "    learner.join()\n",
    "    for a in actors:\n",
    "        a.join()\n",
    "\n",
    "    print(\n",
    "        f\"[main] Completed in {(datetime.datetime.now() - start_time).seconds} seconds\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9610262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86632da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c1870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41e59baf",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec23d825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -pp POLICY_PATH -hd POLICY_HIDDEN_DIM -en\n",
      "                             ENV_NAME [-ne NUM_EPISODES] [-d]\n",
      "                             [-el MAX_EPISODE_LEN] [-ld LOG_DIR] [-v]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -pp/--policy_path, -hd/--policy_hidden_dim, -en/--env_name\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinyzqh/miniforge3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3449: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.double\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"-pp\", \"--policy_path\", type=str, required=True, help=\"path to policy weights\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-hd\",\n",
    "        \"--policy_hidden_dim\",\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help=\"dimension of hidden layer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-en\",\n",
    "        \"--env_name\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"name of gym envrionment to test in\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ne\",\n",
    "        \"--num_episodes\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"number of episodes to test for\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-d\", \"--deterministic\", help=\"use deterministic policy\", action=\"store_true\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-el\",\n",
    "        \"--max_episode_len\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"maximum number of steps per episode\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ld\",\n",
    "        \"--log_dir\",\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=\"directory to store log file in\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-v\", \"--verbose\", help=\"increase output verbosity\", action=\"store_true\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    env = make_env(args.env_name)\n",
    "    observation_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    policy = MlpPolicy(observation_size, action_size, args.policy_hidden_dim)\n",
    "    checkpoint = torch.load(args.policy_path)\n",
    "    policy.load_state_dict(checkpoint[\"policy_state_dict\"])\n",
    "\n",
    "    test_policy(\n",
    "        policy,\n",
    "        env,\n",
    "        args.num_episodes,\n",
    "        args.deterministic,\n",
    "        args.max_episode_len,\n",
    "        args.log_dir,\n",
    "        args.verbose,\n",
    "    )\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3216081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
