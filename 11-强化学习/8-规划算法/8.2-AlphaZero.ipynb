{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3105a385",
   "metadata": {},
   "source": [
    "## Alpha Zero详解\n",
    "\n",
    "- http://joshvarty.github.io/AlphaZero/\n",
    "\n",
    "考虑一个简单的游戏环境，`Connect2`。游戏规则是在一个$1 \\times 4$的棋盘上，目标是在两个相邻位置方式相同的元素。整个博弈树展开为如下形式:\n",
    "\n",
    "<img src=\"../images/11-connect2.png\" width=\"100%\">\n",
    "\n",
    "很明显，这时一个先手必赢的游戏。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e274c",
   "metadata": {},
   "source": [
    "## Connect 2游戏环境代码实现\n",
    "\n",
    "用1表示自己这方落子的表示，-1表示对方落子的表示，空的棋盘表示为0，因此初始状态是[0, 0, 0, 0]。赢得游戏的一方获得的奖励是+1，输掉游戏的一方获得奖励是-1。平局获得的奖励是0\n",
    "\n",
    "Note: There is a very important detail here: The player who is playing always sees their own pieces as 1. In our implementation, we simply multiply the board by -1 every time we transition between players. Below, observe that state “toggles” as players take turns placing pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4ea49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class Connect2Game(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化函数\"\"\"\n",
    "        self.columns = 4 # 四列\n",
    "        self.win = 2 # 当两个连子即为赢\n",
    "    \n",
    "    def get_init_board(self):\n",
    "        \"\"\"获取初始化的棋盘\"\"\"\n",
    "        return np.zeros((self.columns), dtype=np.int)\n",
    "    \n",
    "    def get_board_size(self):\n",
    "        \"\"\"获取棋盘大小\"\"\"\n",
    "        return self.columns\n",
    "    \n",
    "    def get_action_size(self):\n",
    "        \"\"\"获取动作空间大小\"\"\"\n",
    "        return self.columns\n",
    "    \n",
    "    def get_next_state(self, board, player, action):\n",
    "        b = np.copy(board)\n",
    "        b[action] = player\n",
    "        \n",
    "        # 返回新的game，但是需要转换玩家视角，player需要乘一个负号\n",
    "        return (b, -player)\n",
    "    \n",
    "    def has_legal_moves(self, board):\n",
    "        \"\"\"判断是否还有合法的动作空间\"\"\"\n",
    "        for index in range(self.columns):\n",
    "            if board[index] == 0:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_valid_moves(self, board):\n",
    "        \"\"\"获取有效的动作\"\"\"\n",
    "        # 初始化时，所有的动作都是有效的\n",
    "        valid_moves = [0] * self.get_action_size()\n",
    "        b = np.copy(board)\n",
    "        \n",
    "        for index in range(self.columns):\n",
    "            if board[index] == 0:\n",
    "                valid_moves[index] = 1\n",
    "                \n",
    "        return valid_moves\n",
    "    \n",
    "    def is_win(self, board, player):\n",
    "        \"\"\"判断是否赢\"\"\"\n",
    "        count = 0\n",
    "        for index in range(self.columns):\n",
    "            if board[index] == player:\n",
    "                count += 1\n",
    "            else:\n",
    "                count += 0\n",
    "            \n",
    "            if count == self.win:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "    def get_reward_for_player(self, board, player):\n",
    "        \"\"\"如果没有结束返回None，玩家1胜返回1，玩家2胜返回-1\"\"\"\n",
    "        if self.is_win(board, player):\n",
    "            return 1\n",
    "        if self.is_win(board, player):\n",
    "            return -1\n",
    "        if self.has_legal_moves(board):\n",
    "            return None\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def get_canonical_board(self, board, player):\n",
    "        return player * board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04298625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c819a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e037cb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "528773ba",
   "metadata": {},
   "source": [
    "## Value Network\n",
    "\n",
    "值网络的输入就是当前棋盘的状态表示，需要预测一个标量的输出，比如我方胜利的状态输入给`value network`的话，期望`value network`的输出是`1`，反之为`-1`，平局为`0`。\n",
    "\n",
    "### 训练\n",
    "\n",
    "为了训练`value network`，我们需要记录在`self-play`过程中的每一步，例如:\n",
    "\n",
    "1. 初始化状态`[0, 0, 0, 0]`, 玩家`1`在`1`位置落子。\n",
    "2. 状态进行转移，玩家身份互换，对于玩家`2`来说，状态为`[-1, 0, 0, 0]`, 玩家`2`选择在`3`位置落子。\n",
    "3. 状态再次进行转移，玩家身份互换，对于玩家`1`来说，状态为`[1, 0, -1, 0]`, 玩家`1`选择在`2`位置落子。\n",
    "4. 游戏结束。\n",
    "\n",
    "游戏结束之后，回溯整个状态，将玩家`1`奖励设置为`1`，玩家`2`奖励设置为`-1`，记录表格如下所示:\n",
    "\n",
    "```python\n",
    "([ 0  0  0  0],  1)   # Player 1 plays in the first position\n",
    "([-1  0  0  0], -1)   # Player 2 plays in the third position\n",
    "([ 1  0 -1  0],  1)   # Player 1 plays in the second position\n",
    "```\n",
    "\n",
    "之后采集大量的样本，训练value network。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf4205a",
   "metadata": {},
   "source": [
    "## Policy Network\n",
    "\n",
    "policy network与value network一样，接收棋盘状态的输入，但是需要给出每个动作的选择概率。策略网络对于每个动作的选择概率之后要勇于MCTS的指导搜索，并将policy network的这个指导称之为priors。\n",
    "\n",
    "但是网络对于非法的动作，通常不会输出概率为0，因此我们需要将非法的动作mask掉，之后再将其re-normalizing一下，来保障最终的概率和为1。\n",
    "\n",
    "### 训练\n",
    "\n",
    "这里并不直接训练一个网络来决策出如何走，而是让`policy`来学习`MCTS`的输出，因此在记录的时候，我们需要记录状态，和每个状态下`MCTS`的输出(`probabilities produced by the MCTS`):\n",
    "\n",
    "```python\n",
    "([ 0  0  0  0], [0.1, 0.4, 0.4, 0.1])\n",
    "([-1  0  0  0], [0.0, 0.3, 0.3, 0.3])\n",
    "([ 1  0 -1  0], [0.0, 0.8, 0.0, 0.2])\n",
    "```\n",
    "\n",
    "`policy network`和`MCTS`是迭代进行的，`MCTS`之后需要用到`policy network`来指导搜索。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec07cb",
   "metadata": {},
   "source": [
    "## Value Network和Policy Network代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a02697",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch_size': 64,\n",
    "    'num_simulations': 100,                         # Number of Monte Carlo simulations for each move\n",
    "    'numIters': 500,                                # Total number of training iterations\n",
    "    'numEps': 100,                                  # Number of full games (episodes) to run during each iteration\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "    'epochs': 2,                                    # Number of epochs of training per iteration\n",
    "    'checkpoint_path': 'latest.pth'                 # location to save latest set of weights\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0575f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Connect2Model(nn.Module):\n",
    "\n",
    "    def __init__(self, board_size, action_size, device):\n",
    "\n",
    "        super(Connect2Model, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.size = board_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=self.size, out_features=16)\n",
    "        self.fc2 = nn.Linear(in_features=16, out_features=16)\n",
    "\n",
    "        # Two heads on our network\n",
    "        self.action_head = nn.Linear(in_features=16, out_features=self.action_size)\n",
    "        self.value_head = nn.Linear(in_features=16, out_features=1)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        action_logits = self.action_head(x)\n",
    "        value_logit = self.value_head(x)\n",
    "\n",
    "        return F.softmax(action_logits, dim=1), torch.tanh(value_logit)\n",
    "\n",
    "    def predict(self, board):\n",
    "        board = torch.FloatTensor(board.astype(np.float32)).to(self.device)\n",
    "        board = board.view(1, self.size)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            pi, v = self.forward(board)\n",
    "\n",
    "        return pi.data.cpu().numpy()[0], v.data.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a3b040",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3c9b0",
   "metadata": {},
   "source": [
    "### Node\n",
    "\n",
    "每个节点代表的就是一个可到达的棋盘状态，节点上的边就代表玩家在当前节点状态采取的动作，节点存储的信息如下所示:\n",
    "\n",
    "```python\n",
    "class Node:\n",
    "  def __init__(self, prior, to_play):\n",
    "      self.prior = prior      # The prior probability of selecting this state from its parent\n",
    "      self.to_play = to_play  # The player whose turn it is. (1 or -1)\n",
    "\n",
    "      self.children = {}      # A lookup of legal child positions\n",
    "      self.visit_count = 0    # Number of times this state was visited during MCTS. \"Good\" are visited more then \"bad\" states.\n",
    "      self.value_sum = 0      # The total value of this state from all visits\n",
    "      self.state = None       # The board state as this node\n",
    "\n",
    "  def value(self):\n",
    "       # Average value for a node\n",
    "      return self.value_sum / self.visit_count  \n",
    "```\n",
    "\n",
    "节点中的`prior`就是父亲节点选择到这个当前节点的概率，比如父亲节点输出的动作概率为`[0.8, 0, 0, 0.2]`, 如果选择第一个节点`0.8`概率的这个动作，就会转移到当前节点的话，那么当前节点的`prior`概率就是`0.8`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982af49",
   "metadata": {},
   "source": [
    "### MCTS\n",
    "\n",
    "定义好了节点之后，就可以通过mcts来构建博弈树了，整个博弈树的展开是多个simulations构成的，每个simulation会添加一个节点到博弈树中，由以下三部分组成:\n",
    "\n",
    "1. Select\n",
    "\n",
    "2. Expand\n",
    "\n",
    "3. Backup\n",
    "\n",
    "simulation次数越多，得到的模型就会越好。核心思路如下:\n",
    "\n",
    "```python\n",
    "def run(self, model, state, to_play):\n",
    "\n",
    "    root = Node(0, to_play)\n",
    "\n",
    "    # EXPAND root\n",
    "    action_probs, value = model.predict(state)\n",
    "    valid_moves = self.game.get_valid_moves(state)\n",
    "    action_probs = action_probs * valid_moves  # mask invalid moves\n",
    "    action_probs /= np.sum(action_probs)\n",
    "    root.expand(state, to_play, action_probs)\n",
    "\n",
    "    for _ in range(self.args['num_simulations']):\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "\n",
    "        # SELECT\n",
    "        while node.expanded():\n",
    "            action, node = node.select_child()\n",
    "            search_path.append(node)\n",
    "\n",
    "        parent = search_path[-2]\n",
    "        state = parent.state\n",
    "        # Now we're at a leaf node and we would like to expand\n",
    "        # Players always play from their own perspective\n",
    "        next_state, _ = self.game.get_next_state(state, player=1, action=action)\n",
    "        # Get the board from the perspective of the other player\n",
    "        next_state = self.game.get_canonical_board(next_state, player=-1)\n",
    "\n",
    "        # The value of the new state from the perspective of the other player\n",
    "        value = self.game.get_reward_for_player(next_state, player=1)\n",
    "        if value is None:\n",
    "            # If the game has not ended:\n",
    "            # EXPAND\n",
    "            action_probs, value = model.predict(next_state)\n",
    "            valid_moves = self.game.get_valid_moves(next_state)\n",
    "            action_probs = action_probs * valid_moves  # mask invalid moves\n",
    "            action_probs /= np.sum(action_probs)\n",
    "            node.expand(next_state, parent.to_play * -1, action_probs)\n",
    "\n",
    "        self.backpropagate(search_path, value, parent.to_play * -1)\n",
    "\n",
    "    return root\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005e376",
   "metadata": {},
   "source": [
    "接收当前的棋盘状态`state`，当前的玩家是哪一方，`mcts`的`simulation`次数是多少。之后就是创建跟节点，然后扩展根节点，当然在这里，扩展根节点并没有用到`policy network`的`prior`的知识。\n",
    "\n",
    "扩展完之后，我们就有孩子节点，然后去选择孩子节点，选择孩子节点通过`UCB`的算法来选择。选择到了孩子节点之后，`node`就会被更新，并且返回了这个被选择的动作，之后，退出`while`循环，因为这个孩子节点并没有被`expand`，所以没有孩子节点，并且这个孩子节点被记录到`search_path`中。\n",
    "\n",
    "之后将这个选择的动作给到`get_next_state`中，进行了状态转移，之后玩家身份也需要转换，状态也需要转换。\n",
    "\n",
    "然后获取奖励，如果游戏没有结束，那么获取到的奖励将会是`None`，然后再次`expand`，获取到三个子节点，但是他们的`value`都是`None`，之后再回溯叠加奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa401d",
   "metadata": {},
   "source": [
    "### UCB\n",
    "\n",
    "UCB需要三个东西:\n",
    "\n",
    "1. 孩子节点的先验概率。\n",
    "\n",
    "2. 孩子节点的value，其实也就是对手的value。\n",
    "\n",
    "3. 访问次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884e44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ucb_score(parent, child):\n",
    "    \"\"\"\n",
    "    The score for an action that would transition between the parent and child.\n",
    "    \"\"\"\n",
    "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if child.visit_count > 0:\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = -child.value()\n",
    "    else:\n",
    "        value_score = 0\n",
    "\n",
    "    return value_score + prior_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562c410",
   "metadata": {},
   "source": [
    "## MCTS代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3daea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, prior, to_play):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = to_play\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.state = None\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def select_action(self, temperature):\n",
    "        \"\"\"\n",
    "        Select action according to the visit count distribution and the temperature.\n",
    "        \"\"\"\n",
    "        visit_counts = np.array([child.visit_count for child in self.children.values()])\n",
    "        actions = [action for action in self.children.keys()]\n",
    "        if temperature == 0:\n",
    "            action = actions[np.argmax(visit_counts)]\n",
    "        elif temperature == float(\"inf\"):\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            # See paper appendix Data Generation\n",
    "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "            action = np.random.choice(actions, p=visit_count_distribution)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def select_child(self):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        best_score = -np.inf\n",
    "        best_action = -1\n",
    "        best_child = None\n",
    "\n",
    "        for action, child in self.children.items():\n",
    "            score = ucb_score(self, child)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "                best_child = child\n",
    "\n",
    "        return best_action, best_child\n",
    "\n",
    "    def expand(self, state, to_play, action_probs):\n",
    "        \"\"\"\n",
    "        We expand a node and keep track of the prior policy probability given by neural network\n",
    "        \"\"\"\n",
    "        self.to_play = to_play\n",
    "        self.state = state\n",
    "        for a, prob in enumerate(action_probs):\n",
    "            if prob != 0:\n",
    "                self.children[a] = Node(prior=prob, to_play=self.to_play * -1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Debugger pretty print node info\n",
    "        \"\"\"\n",
    "        prior = \"{0:.2f}\".format(self.prior)\n",
    "        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0bddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "\n",
    "    def __init__(self, game, model, args):\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    def run(self, model, state, to_play):\n",
    "\n",
    "        root = Node(0, to_play)\n",
    "\n",
    "        # EXPAND root\n",
    "        action_probs, value = model.predict(state)\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        action_probs = action_probs * valid_moves  # mask invalid moves\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        root.expand(state, to_play, action_probs)\n",
    "\n",
    "        for _ in range(self.args['num_simulations']):\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "\n",
    "            # SELECT\n",
    "            while node.expanded():\n",
    "                action, node = node.select_child()\n",
    "                search_path.append(node)\n",
    "\n",
    "            parent = search_path[-2]\n",
    "            state = parent.state\n",
    "            # Now we're at a leaf node and we would like to expand\n",
    "            # Players always play from their own perspective\n",
    "            next_state, _ = self.game.get_next_state(state, player=1, action=action)\n",
    "            # Get the board from the perspective of the other player\n",
    "            next_state = self.game.get_canonical_board(next_state, player=-1)\n",
    "\n",
    "            # The value of the new state from the perspective of the other player\n",
    "            value = self.game.get_reward_for_player(next_state, player=1)\n",
    "            if value is None:\n",
    "                # If the game has not ended:\n",
    "                # EXPAND\n",
    "                action_probs, value = model.predict(next_state)\n",
    "                valid_moves = self.game.get_valid_moves(next_state)\n",
    "                action_probs = action_probs * valid_moves  # mask invalid moves\n",
    "                action_probs /= np.sum(action_probs)\n",
    "                node.expand(next_state, parent.to_play * -1, action_probs)\n",
    "\n",
    "            self.backpropagate(search_path, value, parent.to_play * -1)\n",
    "\n",
    "        return root\n",
    "\n",
    "    def backpropagate(self, search_path, value, to_play):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        for node in reversed(search_path):\n",
    "            node.value_sum += value if node.to_play == to_play else -value\n",
    "            node.visit_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a7b92",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70d3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, game, model, args):\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.model, self.args)\n",
    "\n",
    "    def exceute_episode(self):\n",
    "\n",
    "        train_examples = []\n",
    "        current_player = 1\n",
    "        episode_step = 0\n",
    "        state = self.game.get_init_board()\n",
    "\n",
    "        while True:\n",
    "            episode_step += 1\n",
    "\n",
    "            canonical_board = self.game.get_canonical_board(state, current_player)\n",
    "\n",
    "            self.mcts = MCTS(self.game, self.model, self.args)\n",
    "            root = self.mcts.run(self.model, canonical_board, to_play=1)\n",
    "\n",
    "            action_probs = [0 for _ in range(self.game.get_action_size())]\n",
    "            for k, v in root.children.items():\n",
    "                action_probs[k] = v.visit_count\n",
    "\n",
    "            action_probs = action_probs / np.sum(action_probs)\n",
    "            train_examples.append((canonical_board, current_player, action_probs))\n",
    "\n",
    "            action = root.select_action(temperature=0)\n",
    "            state, current_player = self.game.get_next_state(state, current_player, action)\n",
    "            reward = self.game.get_reward_for_player(state, current_player)\n",
    "\n",
    "            if reward is not None:\n",
    "                ret = []\n",
    "                for hist_state, hist_current_player, hist_action_probs in train_examples:\n",
    "                    # [Board, currentPlayer, actionProbabilities, Reward]\n",
    "                    ret.append((hist_state, hist_action_probs, reward * ((-1) ** (hist_current_player != current_player))))\n",
    "\n",
    "                return ret\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(1, self.args['numIters'] + 1):\n",
    "\n",
    "            print(\"{}/{}\".format(i, self.args['numIters']))\n",
    "\n",
    "            train_examples = []\n",
    "\n",
    "            for eps in range(self.args['numEps']):\n",
    "                iteration_train_examples = self.exceute_episode()\n",
    "                train_examples.extend(iteration_train_examples)\n",
    "\n",
    "            shuffle(train_examples)\n",
    "            self.train(train_examples)\n",
    "            filename = self.args['checkpoint_path']\n",
    "            self.save_checkpoint(folder=\".\", filename=filename)\n",
    "\n",
    "    def train(self, examples):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=5e-4)\n",
    "        pi_losses = []\n",
    "        v_losses = []\n",
    "\n",
    "        for epoch in range(self.args['epochs']):\n",
    "            self.model.train()\n",
    "\n",
    "            batch_idx = 0\n",
    "\n",
    "            while batch_idx < int(len(examples) / self.args['batch_size']):\n",
    "                sample_ids = np.random.randint(len(examples), size=self.args['batch_size'])\n",
    "                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
    "                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "                target_pis = torch.FloatTensor(np.array(pis))\n",
    "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "                # predict\n",
    "                boards = boards.contiguous()#.cuda()\n",
    "                target_pis = target_pis.contiguous()#.cuda()\n",
    "                target_vs = target_vs.contiguous()#.cuda()\n",
    "\n",
    "                # compute output\n",
    "                out_pi, out_v = self.model(boards)\n",
    "                l_pi = self.loss_pi(target_pis, out_pi)\n",
    "                l_v = self.loss_v(target_vs, out_v)\n",
    "                total_loss = l_pi + l_v\n",
    "\n",
    "                pi_losses.append(float(l_pi))\n",
    "                v_losses.append(float(l_v))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_idx += 1\n",
    "\n",
    "            print()\n",
    "            print(\"Policy Loss\", np.mean(pi_losses))\n",
    "            print(\"Value Loss\", np.mean(v_losses))\n",
    "            print(\"Examples:\")\n",
    "            print(out_pi[0].detach())\n",
    "            print(target_pis[0])\n",
    "\n",
    "    def loss_pi(self, targets, outputs):\n",
    "        loss = -(targets * torch.log(outputs)).sum(dim=1)\n",
    "        return loss.mean()\n",
    "\n",
    "    def loss_v(self, targets, outputs):\n",
    "        loss = torch.sum((targets-outputs.view(-1))**2)/targets.size()[0]\n",
    "        return loss\n",
    "\n",
    "    def save_checkpoint(self, folder, filename):\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        torch.save({\n",
    "            'state_dict': self.model.state_dict(),\n",
    "        }, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4e1174a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/xgj8n5xd2bb0cv55w7c72bn40000gn/T/ipykernel_36933/3774257678.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.zeros((self.columns), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy Loss 1.4615188439687092\n",
      "Value Loss 1.0158655941486359\n",
      "Examples:\n",
      "tensor([0.2474, 0.1966, 0.2601, 0.2958])\n",
      "tensor([0.0300, 0.0400, 0.8800, 0.0500])\n",
      "\n",
      "Policy Loss 1.4592657387256622\n",
      "Value Loss 0.9948037962118784\n",
      "Examples:\n",
      "tensor([0.2299, 0.2020, 0.2514, 0.3167])\n",
      "tensor([0.5500, 0.4500, 0.0000, 0.0000])\n",
      "2/500\n",
      "\n",
      "Policy Loss 1.443178693453471\n",
      "Value Loss 0.9317807555198669\n",
      "Examples:\n",
      "tensor([0.2467, 0.2011, 0.2628, 0.2894])\n",
      "tensor([0.0300, 0.0300, 0.8900, 0.0500])\n",
      "\n",
      "Policy Loss 1.4429944852987926\n",
      "Value Loss 0.9395932952562968\n",
      "Examples:\n",
      "tensor([0.2458, 0.2198, 0.2627, 0.2717])\n",
      "tensor([0., 1., 0., 0.])\n",
      "3/500\n",
      "\n",
      "Policy Loss 1.4357839822769165\n",
      "Value Loss 0.8991268078486124\n",
      "Examples:\n",
      "tensor([0.2457, 0.2055, 0.2658, 0.2830])\n",
      "tensor([0.0300, 0.0300, 0.9100, 0.0300])\n",
      "\n",
      "Policy Loss 1.4346220592657726\n",
      "Value Loss 0.9015381038188934\n",
      "Examples:\n",
      "tensor([0.2447, 0.2263, 0.2625, 0.2665])\n",
      "tensor([0., 1., 0., 0.])\n",
      "4/500\n",
      "\n",
      "Policy Loss 1.4544566075007122\n",
      "Value Loss 0.9462345341841379\n",
      "Examples:\n",
      "tensor([0.2327, 0.2161, 0.2905, 0.2607])\n",
      "tensor([0.0000, 0.4400, 0.0000, 0.5600])\n",
      "\n",
      "Policy Loss 1.4459502398967743\n",
      "Value Loss 0.9336509903271993\n",
      "Examples:\n",
      "tensor([0.2593, 0.1997, 0.2358, 0.3053])\n",
      "tensor([0., 1., 0., 0.])\n",
      "5/500\n",
      "\n",
      "Policy Loss 1.4134589036305745\n",
      "Value Loss 0.8520236710707346\n",
      "Examples:\n",
      "tensor([0.2257, 0.2245, 0.2613, 0.2884])\n",
      "tensor([0.5000, 0.5000, 0.0000, 0.0000])\n",
      "\n",
      "Policy Loss 1.409546583890915\n",
      "Value Loss 0.8455655624469122\n",
      "Examples:\n",
      "tensor([0.2568, 0.2147, 0.2539, 0.2746])\n",
      "tensor([0.3400, 0.3100, 0.0000, 0.3500])\n",
      "6/500\n",
      "\n",
      "Policy Loss 1.393516480922699\n",
      "Value Loss 0.83328777551651\n",
      "Examples:\n",
      "tensor([0.2263, 0.2297, 0.2660, 0.2780])\n",
      "tensor([0.5000, 0.5000, 0.0000, 0.0000])\n",
      "\n",
      "Policy Loss 1.3956348200639088\n",
      "Value Loss 0.8116416931152344\n",
      "Examples:\n",
      "tensor([0.2368, 0.2460, 0.2628, 0.2544])\n",
      "tensor([0., 1., 0., 0.])\n",
      "7/500\n",
      "\n",
      "Policy Loss 1.421597162882487\n",
      "Value Loss 0.8620552519957224\n",
      "Examples:\n",
      "tensor([0.2356, 0.2246, 0.2771, 0.2627])\n",
      "tensor([0.0300, 0.0200, 0.9300, 0.0200])\n",
      "\n",
      "Policy Loss 1.4195933938026428\n",
      "Value Loss 0.8593589464823405\n",
      "Examples:\n",
      "tensor([0.2601, 0.2134, 0.2374, 0.2890])\n",
      "tensor([0., 1., 0., 0.])\n",
      "8/500\n",
      "\n",
      "Policy Loss 1.4106772144635518\n",
      "Value Loss 0.8413176437218984\n",
      "Examples:\n",
      "tensor([0.2297, 0.2310, 0.2785, 0.2608])\n",
      "tensor([0.0200, 0.0200, 0.9400, 0.0200])\n",
      "\n",
      "Policy Loss 1.4042658011118572\n",
      "Value Loss 0.8275997191667557\n",
      "Examples:\n",
      "tensor([0.2497, 0.2296, 0.2536, 0.2672])\n",
      "tensor([0.3400, 0.3200, 0.0000, 0.3400])\n",
      "9/500\n",
      "\n",
      "Policy Loss 1.3315823475519817\n",
      "Value Loss 0.785865306854248\n",
      "Examples:\n",
      "tensor([0.2249, 0.2344, 0.2793, 0.2614])\n",
      "tensor([0.0200, 0.0200, 0.9400, 0.0200])\n",
      "\n",
      "Policy Loss 1.3272375365098317\n",
      "Value Loss 0.7815997898578644\n",
      "Examples:\n",
      "tensor([0.2486, 0.2288, 0.2503, 0.2724])\n",
      "tensor([0.3400, 0.3200, 0.0000, 0.3400])\n",
      "10/500\n",
      "\n",
      "Policy Loss 1.38993904987971\n",
      "Value Loss 0.7319584488868713\n",
      "Examples:\n",
      "tensor([0.2516, 0.2284, 0.2485, 0.2715])\n",
      "tensor([0.3300, 0.3200, 0.0000, 0.3500])\n",
      "\n",
      "Policy Loss 1.3787346382935841\n",
      "Value Loss 0.7354530642429987\n",
      "Examples:\n",
      "tensor([0.2124, 0.2552, 0.2721, 0.2603])\n",
      "tensor([0.4400, 0.5600, 0.0000, 0.0000])\n",
      "11/500\n",
      "\n",
      "Policy Loss 1.3211124340693157\n",
      "Value Loss 0.7322046160697937\n",
      "Examples:\n",
      "tensor([0.2553, 0.2270, 0.2457, 0.2720])\n",
      "tensor([0.3400, 0.3200, 0.0000, 0.3400])\n",
      "\n",
      "Policy Loss 1.32157767812411\n",
      "Value Loss 0.7173030078411102\n",
      "Examples:\n",
      "tensor([0.2239, 0.2338, 0.2792, 0.2632])\n",
      "tensor([0.0200, 0.0200, 0.9400, 0.0200])\n",
      "12/500\n",
      "\n",
      "Policy Loss 1.3141576647758484\n",
      "Value Loss 0.6980780363082886\n",
      "Examples:\n",
      "tensor([0.2539, 0.2255, 0.2432, 0.2774])\n",
      "tensor([0.3400, 0.3200, 0.0000, 0.3400])\n",
      "\n",
      "Policy Loss 1.3083275059858959\n",
      "Value Loss 0.679344986875852\n",
      "Examples:\n",
      "tensor([0.2533, 0.2248, 0.2416, 0.2803])\n",
      "tensor([0.3400, 0.3200, 0.0000, 0.3400])\n",
      "13/500\n",
      "\n",
      "Policy Loss 1.3704281449317932\n",
      "Value Loss 0.6416531999905905\n",
      "Examples:\n",
      "tensor([0.2213, 0.2333, 0.2799, 0.2655])\n",
      "tensor([0.0200, 0.0300, 0.9300, 0.0200])\n",
      "\n",
      "Policy Loss 1.366008033355077\n",
      "Value Loss 0.6301346868276596\n",
      "Examples:\n",
      "tensor([0.2233, 0.2338, 0.2799, 0.2631])\n",
      "tensor([0.0200, 0.0300, 0.9300, 0.0200])\n",
      "14/500\n",
      "\n",
      "Policy Loss 1.3077838222185771\n",
      "Value Loss 0.6248712738355001\n",
      "Examples:\n",
      "tensor([0.2619, 0.2206, 0.2362, 0.2813])\n",
      "tensor([0.3400, 0.3200, 0.0000, 0.3400])\n",
      "\n",
      "Policy Loss 1.3042141298453014\n",
      "Value Loss 0.6127886126438776\n",
      "Examples:\n",
      "tensor([0.1970, 0.2505, 0.3066, 0.2460])\n",
      "tensor([0.0000, 0.5100, 0.0000, 0.4900])\n",
      "15/500\n",
      "\n",
      "Policy Loss 1.3586797912915547\n",
      "Value Loss 0.546517570813497\n",
      "Examples:\n",
      "tensor([0.2695, 0.2169, 0.2223, 0.2914])\n",
      "tensor([1., 0., 0., 0.])\n",
      "\n",
      "Policy Loss 1.352776676416397\n",
      "Value Loss 0.5409787744283676\n",
      "Examples:\n",
      "tensor([0.2237, 0.2362, 0.2772, 0.2629])\n",
      "tensor([0.0200, 0.0300, 0.9300, 0.0200])\n",
      "16/500\n",
      "\n",
      "Policy Loss 1.3034138083457947\n",
      "Value Loss 0.5614470938841502\n",
      "Examples:\n",
      "tensor([0.1988, 0.2550, 0.3045, 0.2417])\n",
      "tensor([0.0000, 0.5200, 0.0000, 0.4800])\n",
      "\n",
      "Policy Loss 1.300204982360204\n",
      "Value Loss 0.539043240249157\n",
      "Examples:\n",
      "tensor([0.2700, 0.2155, 0.2248, 0.2896])\n",
      "tensor([0.3400, 0.3200, 0.0000, 0.3400])\n",
      "17/500\n",
      "\n",
      "Policy Loss 1.2897978623708088\n",
      "Value Loss 0.5150701353947321\n",
      "Examples:\n",
      "tensor([0.1924, 0.2591, 0.3035, 0.2450])\n",
      "tensor([0.0000, 0.5300, 0.0000, 0.4700])\n",
      "\n",
      "Policy Loss 1.2847032745679219\n",
      "Value Loss 0.5014520535866419\n",
      "Examples:\n",
      "tensor([0.2135, 0.2385, 0.2791, 0.2689])\n",
      "tensor([0.0200, 0.0300, 0.9300, 0.0200])\n",
      "18/500\n",
      "\n",
      "Policy Loss 1.3406193653742473\n",
      "Value Loss 0.45448484520117444\n",
      "Examples:\n",
      "tensor([0.2794, 0.2078, 0.2066, 0.3062])\n",
      "tensor([1., 0., 0., 0.])\n",
      "\n",
      "Policy Loss 1.335003674030304\n",
      "Value Loss 0.4457059179743131\n",
      "Examples:\n",
      "tensor([0.2871, 0.2053, 0.2029, 0.3047])\n",
      "tensor([1., 0., 0., 0.])\n",
      "19/500\n",
      "\n",
      "Policy Loss 1.2773800293604534\n",
      "Value Loss 0.4457533707221349\n",
      "Examples:\n",
      "tensor([0.2148, 0.2410, 0.2796, 0.2646])\n",
      "tensor([0.0100, 0.0300, 0.9400, 0.0200])\n",
      "\n",
      "Policy Loss 1.2753919263680775\n",
      "Value Loss 0.43114664653937024\n",
      "Examples:\n",
      "tensor([0.2117, 0.2419, 0.2805, 0.2658])\n",
      "tensor([0.0100, 0.0300, 0.9400, 0.0200])\n",
      "20/500\n",
      "\n",
      "Policy Loss 1.3251246611277263\n",
      "Value Loss 0.3813849339882533\n",
      "Examples:\n",
      "tensor([0.1894, 0.2823, 0.2837, 0.2445])\n",
      "tensor([0.3800, 0.6200, 0.0000, 0.0000])\n",
      "\n",
      "Policy Loss 1.3177393873532612\n",
      "Value Loss 0.36964184542497\n",
      "Examples:\n",
      "tensor([0.1920, 0.2868, 0.2825, 0.2388])\n",
      "tensor([0.3800, 0.6200, 0.0000, 0.0000])\n",
      "21/500\n",
      "\n",
      "Policy Loss 1.2736679514249165\n",
      "Value Loss 0.3600849161545436\n",
      "Examples:\n",
      "tensor([0.2125, 0.2467, 0.2797, 0.2611])\n",
      "tensor([0.0100, 0.0300, 0.9500, 0.0100])\n",
      "\n",
      "Policy Loss 1.267398516337077\n",
      "Value Loss 0.3502119779586792\n",
      "Examples:\n",
      "tensor([0.1817, 0.2794, 0.3026, 0.2363])\n",
      "tensor([0.0000, 0.5600, 0.0000, 0.4400])\n",
      "22/500\n",
      "\n",
      "Policy Loss 1.2988623976707458\n",
      "Value Loss 0.3171529173851013\n",
      "Examples:\n",
      "tensor([0.3052, 0.1938, 0.1840, 0.3169])\n",
      "tensor([1., 0., 0., 0.])\n",
      "\n",
      "Policy Loss 1.298123021920522\n",
      "Value Loss 0.3014703144629796\n",
      "Examples:\n",
      "tensor([0.2943, 0.2026, 0.2001, 0.3030])\n",
      "tensor([0.3300, 0.3200, 0.0000, 0.3500])\n",
      "23/500\n",
      "\n",
      "Policy Loss 1.2606068849563599\n",
      "Value Loss 0.28706232210000354\n",
      "Examples:\n",
      "tensor([0.1804, 0.2888, 0.3035, 0.2273])\n",
      "tensor([0.0000, 0.5800, 0.0000, 0.4200])\n",
      "\n",
      "Policy Loss 1.2589332560698192\n",
      "Value Loss 0.28267395744721097\n",
      "Examples:\n",
      "tensor([0.3278, 0.1715, 0.1521, 0.3486])\n",
      "tensor([0., 0., 0., 1.])\n",
      "24/500\n",
      "\n",
      "Policy Loss 1.2378746271133423\n",
      "Value Loss 0.2509621058901151\n",
      "Examples:\n",
      "tensor([0.2035, 0.2576, 0.2806, 0.2583])\n",
      "tensor([0.0100, 0.0300, 0.9500, 0.0100])\n",
      "\n",
      "Policy Loss 1.2360510726769764\n",
      "Value Loss 0.2457282952964306\n",
      "Examples:\n",
      "tensor([0.1691, 0.3002, 0.3035, 0.2273])\n",
      "tensor([0.0000, 0.5900, 0.0000, 0.4100])\n",
      "25/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m Connect2Model(board_size, action_size, device)\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(game, model, args)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mTrainer.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m train_examples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumEps\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 56\u001b[0m     iteration_train_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexceute_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     train_examples\u001b[38;5;241m.\u001b[39mextend(iteration_train_examples)\n\u001b[1;32m     59\u001b[0m shuffle(train_examples)\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mTrainer.exceute_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m canonical_board \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_canonical_board(state, current_player)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmcts \u001b[38;5;241m=\u001b[39m MCTS(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m---> 27\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanonical_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_play\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_action_size())]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m root\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems():\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mMCTS.run\u001b[0;34m(self, model, state, to_play)\u001b[0m\n\u001b[1;32m     34\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_canonical_board(next_state, player\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# The value of the new state from the perspective of the other player\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reward_for_player\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# If the game has not ended:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# EXPAND\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     action_probs, value \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(next_state)\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mConnect2Game.get_reward_for_player\u001b[0;34m(self, board, player)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_reward_for_player\u001b[39m(\u001b[38;5;28mself\u001b[39m, board, player):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124;03m\"\"\"如果没有结束返回None，玩家1胜返回1，玩家2胜返回-1\"\"\"\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_win\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_win(board, player):\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mConnect2Game.is_win\u001b[0;34m(self, board, player)\u001b[0m\n\u001b[1;32m     49\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mboard\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m player:\n\u001b[1;32m     52\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "game = Connect2Game()\n",
    "board_size = game.get_board_size()\n",
    "action_size = game.get_action_size()\n",
    "\n",
    "model = Connect2Model(board_size, action_size, device)\n",
    "\n",
    "trainer = Trainer(game, model, args)\n",
    "trainer.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0ad80",
   "metadata": {},
   "source": [
    "## 测试\n",
    "\n",
    "测试代码在:\n",
    "\n",
    "1. https://github.com/JoshVarty/AlphaZeroSimple/blob/ecf72a468aba26b8b155ec6fb1b91697a2fbb7a9/tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc89b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
