{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212c9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3567bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Auxilary files for those who wanted to solve breakout with CEM or policy gradient\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from skimage.transform import resize\n",
    "from gym.core import Wrapper\n",
    "from gym.spaces.box import Box\n",
    "\n",
    "class PreprocessAtari(Wrapper):\n",
    "    def __init__(self, env, height=42, width=42, color=False,\n",
    "                 crop=lambda img: img, n_frames=4, dim_order='theano'):\n",
    "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
    "        super(PreprocessAtari, self).__init__(env)\n",
    "        assert dim_order in ('theano', 'tensorflow')\n",
    "        self.img_size = (height,width)\n",
    "        self.crop=crop\n",
    "        self.color=color\n",
    "        self.dim_order = dim_order\n",
    "        \n",
    "        n_channels = (3 * n_frames) if color else n_frames\n",
    "        obs_shape = [n_channels,height,width] if dim_order == 'theano' else [height,width,n_channels]\n",
    "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "        self.framebuffer = np.zeros(obs_shape, 'float32')\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
    "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
    "        self.update_buffer(self.env.reset())\n",
    "        return self.framebuffer\n",
    "    \n",
    "    def step(self,action):\n",
    "        \"\"\"plays breakout for 1 step, returns frame buffer\"\"\"\n",
    "        new_img,r,done,info = self.env.step(action)\n",
    "        self.update_buffer(new_img)\n",
    "        return self.framebuffer,r,done,info\n",
    "    \n",
    "    ### image processing ###\n",
    "    \n",
    "    def update_buffer(self,img):\n",
    "        img = self.preproc_image(img)\n",
    "        offset = 3 if self.color else 1\n",
    "        if self.dim_order == 'theano':\n",
    "            axis = 0\n",
    "            cropped_framebuffer = self.framebuffer[:-offset]\n",
    "        else:\n",
    "            axis = -1\n",
    "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
    "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n",
    "\n",
    "    def preproc_image(self, img):\n",
    "        \"\"\"what happens to the observation\"\"\"\n",
    "        img = self.crop(img)\n",
    "        img = resize(img, self.img_size)\n",
    "        if not self.color:\n",
    "            img = img.mean(-1, keepdims=True)\n",
    "        if self.dim_order == 'theano':\n",
    "            img = img.transpose([2,0,1]) # [h, w, c] to [c, h, w]\n",
    "        img = img.astype('float32')/255.\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a752b8de",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "We're Unable to find the game \"KungFuMaster\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"KungFuMaster\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"KungFuMaster\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     env \u001b[38;5;241m=\u001b[39m PreprocessAtari(env, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      6\u001b[0m                           crop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m img: img[\u001b[38;5;241m60\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m15\u001b[39m:],\n\u001b[1;32m      7\u001b[0m                           color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, n_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[0;32m---> 10\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m obs_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     13\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mmake_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_env\u001b[39m():\n\u001b[0;32m----> 4\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKungFuMasterDeterministic-v4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     env \u001b[38;5;241m=\u001b[39m PreprocessAtari(env, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m      6\u001b[0m                           crop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m img: img[\u001b[38;5;241m60\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m15\u001b[39m:],\n\u001b[1;32m      7\u001b[0m                           color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, n_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:676\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:520\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec(path)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Construct the environment\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:140\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[0;32m--> 140\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m spec \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/envs/atari/environment.py:123\u001b[0m, in \u001b[0;36mAtariEnv.__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, render_mode)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetBool(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Seed + Load\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetLegalActionSet()\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m full_action_space\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetMinimalActionSet()\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/envs/atari/environment.py:171\u001b[0m, in \u001b[0;36mAtariEnv.seed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed2)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game):\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWe\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mre Unable to find the game \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Note: Gym no longer distributes ROMs. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you own a license to use the necessary ROMs for research purposes you can download them \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvia `pip install gym[accept-rom-license]`. Otherwise, you should try importing \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvia the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis unsupported. To check if this is the case try providing the environment variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m     )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mloadROM(\u001b[38;5;28mgetattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game))\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mError\u001b[0m: We're Unable to find the game \"KungFuMaster\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"KungFuMaster\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"KungFuMaster\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v4\")\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop = lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af13e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation')\n",
    "plt.imshow(s.reshape([42,42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06211710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcdb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous step and observation, \n",
    "        returns next state and whatever it needs to learn (tf tensors)\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE: apply the whole neural net for one step here.\n",
    "        # See docs on self.rnn(...)\n",
    "        # the recurrent cell should take the last feedforward dense layer as input\n",
    "        <YOUR CODE>\n",
    "        \n",
    "        new_state = <YOUR CODE>\n",
    "        logits = <YOUR CODE>\n",
    "        state_value = <YOUR CODE>\n",
    "        \n",
    "        return new_state, (logits, state_value)\n",
    "    \n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start. Each state is a np array of shape [batch_size, ...]\"\"\"\n",
    "        return (Variable(torch.zeros((batch_size, 128))),\n",
    "                Variable(torch.zeros((batch_size, 128))))\n",
    "    \n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = F.softmax(logits)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "    \n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\" like forward, but obs_t is not Variable \"\"\"\n",
    "        obs_t = Variable(torch.FloatTensor(np.array(obs_t)))\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an entire game start to end, returns session rewards.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done: break\n",
    "                \n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b55222",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print (rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c43fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb37870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A thin wrapper for openAI gym environments that maintains a set of parallel games and has a method to generate\n",
    "interaction sessions given agent one-step applier function.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# A whole lot of space invaders\n",
    "class EnvPool(object):\n",
    "    def __init__(self, agent, make_env, n_parallel_games=1):\n",
    "        \"\"\"\n",
    "        A special class that handles training on multiple parallel sessions\n",
    "        and is capable of some auxilary actions like evaluating agent on one game session (See .evaluate()).\n",
    "\n",
    "        :param agent: Agent which interacts with the environment.\n",
    "        :param make_env: Factory that produces environments OR a name of the gym environment.\n",
    "        :param n_games: Number of parallel games. One game by default.\n",
    "        :param max_size: Max pool size by default (if appending sessions). By default, pool is not constrained in size.\n",
    "        \"\"\"\n",
    "        # Create atari games.\n",
    "        self.agent = agent\n",
    "        self.make_env = make_env\n",
    "        self.envs = [self.make_env() for _ in range(n_parallel_games)]\n",
    "\n",
    "        # Initial observations.\n",
    "        self.prev_observations = [env.reset() for env in self.envs]\n",
    "\n",
    "        # Agent memory variables (if you use recurrent networks).\n",
    "        self.prev_memory_states = agent.get_initial_state(n_parallel_games)\n",
    "\n",
    "        # Whether particular session has just been terminated and needs restarting.\n",
    "        self.just_ended = [False] * len(self.envs)\n",
    "\n",
    "    def interact(self, n_steps=100, verbose=False):\n",
    "        \"\"\"Generate interaction sessions with ataries (openAI gym atari environments)\n",
    "        Sessions will have length n_steps. Each time one of games is finished, it is immediately getting reset\n",
    "        and this time is recorded in is_alive_log (See returned values).\n",
    "\n",
    "        :param n_steps: Length of an interaction.\n",
    "        :returns: observation_seq, action_seq, reward_seq, is_alive_seq\n",
    "        :rtype: a bunch of tensors [batch, tick, ...]\n",
    "        \"\"\"\n",
    "\n",
    "        def env_step(i, action):\n",
    "            if not self.just_ended[i]:\n",
    "                new_observation, cur_reward, is_done, info = self.envs[i].step(action)\n",
    "                if is_done:\n",
    "                    # Game ends now, will finalize on next tick.\n",
    "                    self.just_ended[i] = True\n",
    "\n",
    "                # note: is_alive=True in any case because environment is still alive (last tick alive) in our notation.\n",
    "                return new_observation, cur_reward, True, info\n",
    "            else:\n",
    "                # Reset environment, get new observation to be used on next tick.\n",
    "                new_observation = self.envs[i].reset()\n",
    "\n",
    "                # Reset memory for new episode.\n",
    "                initial_memory_state = self.agent.get_initial_state(batch_size=1)\n",
    "                for m_i in range(len(new_memory_states)):\n",
    "                    new_memory_states[m_i][i] = initial_memory_state[m_i][0]\n",
    "\n",
    "                if verbose:\n",
    "                    print(\"env %i reloaded\" % i)\n",
    "\n",
    "                self.just_ended[i] = False\n",
    "\n",
    "                return new_observation, 0, False, {'end': True}\n",
    "\n",
    "        history_log = []\n",
    "\n",
    "        for i in range(n_steps - 1):\n",
    "            new_memory_states, readout = self.agent.step(self.prev_memory_states, self.prev_observations)\n",
    "            actions = self.agent.sample_actions(readout)\n",
    "\n",
    "            new_observations, cur_rewards, is_alive, infos = zip(*map(env_step, range(len(self.envs)), actions))\n",
    "\n",
    "            # Append data tuple for this tick.\n",
    "            history_log.append((self.prev_observations, actions, cur_rewards, is_alive))\n",
    "\n",
    "            self.prev_observations = new_observations\n",
    "            self.prev_memory_states = new_memory_states\n",
    "        \n",
    "        #add last observation\n",
    "        dummy_actions = [0] * len(self.envs)\n",
    "        dummy_rewards = [0] * len(self.envs)\n",
    "        dummy_mask = [1] * len(self.envs)\n",
    "        history_log.append((self.prev_observations, dummy_actions, dummy_rewards, dummy_mask))\n",
    "\n",
    "        # cast to numpy arrays, transpose from [time, batch, ...] to [batch, time, ...]\n",
    "        history_log = [np.array(tensor).swapaxes(0, 1) for tensor in zip(*history_log)]\n",
    "        observation_seq, action_seq, reward_seq, is_alive_seq = history_log\n",
    "\n",
    "        return observation_seq, action_seq, reward_seq, is_alive_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc42d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d444d248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa26a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \",rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01773026",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return Variable(y_one_hot) if isinstance(y, Variable) else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e501e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # cast everything into a variable\n",
    "    states = Variable(torch.FloatTensor(np.array(states)))   # shape: [batch_size, time, c, h, w]\n",
    "    actions = Variable(torch.IntTensor(np.array(actions)))   # shape: [batch_size, time]\n",
    "    rewards = Variable(torch.FloatTensor(np.array(rewards))) # shape: [batch_size, time]\n",
    "    is_not_done = Variable(torch.FloatTensor(is_not_done.astype('float32')))  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent. \n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "    \n",
    "    logits = [] # append logit sequence here\n",
    "    state_values = [] #append state values here\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "        \n",
    "        # use agent to comute logits_t and state values_t.\n",
    "        # append them to logits and state_values array\n",
    "        \n",
    "        memory, (logits_t, values_t) = <YOUR CODE>\n",
    "        \n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "        \n",
    "        \n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = F.softmax(logits, dim=2)\n",
    "    logprobas = F.log_softmax(logits, dim=2)\n",
    "        \n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim = -1)\n",
    "    \n",
    "    # Now let's compute two loss components:\n",
    "    # 1) Policy gradient objective. \n",
    "    # Notes: Please don't forget to call .detach() on advantage term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    J_hat = 0 # policy objective as in the formula for J_hat\n",
    "    \n",
    "    # 2) Temporal difference MSE for state values\n",
    "    # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "    # it's okay to use loops if you want\n",
    "    value_loss = 0\n",
    "    \n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "    \n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                                # current rewards\n",
    "        V_t = state_values[:, t]                           # current state values\n",
    "        V_next = state_values[:, t + 1].detach()           # next state values\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]          # log-probability of a_t in s_t\n",
    "        \n",
    "        # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "        \n",
    "        # Compute temporal difference error (MSE for V(s))\n",
    "        value_loss += <YOUR CODE>\n",
    "        \n",
    "        # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "        advantage = <YOUR CODE>\n",
    "        advantage = advantage.detach()\n",
    "        \n",
    "        # compute policy pseudo-loss aka -J_hat.\n",
    "        J_hat += <YOUR CODE>\n",
    "        \n",
    "    #regularize with entropy\n",
    "    entropy_reg = <compute entropy regularizer>\n",
    "    \n",
    "    # add-up three loss components and average over time\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "           value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "    \n",
    "    \n",
    "    # Gradient descent step\n",
    "    < your code >\n",
    "    \n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a295879",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import ewma\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(15000):  \n",
    "    \n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)    \n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(ewma(np.array(rewards_history),span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29fb89",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just [clip gradients](https://stackoverflow.com/a/43486487) and maybe use a smaller network\n",
    "* Us. Or TF developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e2b6d",
   "metadata": {},
   "source": [
    "## 最后评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddc1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1104cba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
