{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f4143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf6a707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526960fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinyzqh/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13d2db5e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb097af",
   "metadata": {},
   "source": [
    "## 定义一个智能体网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18fbef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "agent = nn.Sequential(\n",
    "    nn.Linear(state_dim[0], 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, n_actions),\n",
    "    )\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056e0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    states = torch.FloatTensor(states)\n",
    "    policy = softmax(agent(states))\n",
    "    return policy.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6647517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_proba(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], n_actions), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis = 1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bcdc3",
   "metadata": {},
   "source": [
    "## 与环境交互\n",
    "\n",
    "有了策略，我们就可以用智能体策略与环境进行交互了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd2649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = predict_proba(np.array([s]))[0] \n",
    "        \n",
    "        a = np.random.choice(range(n_actions), p=action_probas)\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18f0d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9fd41",
   "metadata": {},
   "source": [
    "## 计算累计奖励"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a58ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    cumulative_rewards = np.empty_like(rewards)\n",
    "    cumulative_rewards = cumulative_rewards.astype(float)\n",
    "    cumulative_rewards[-1] = rewards[-1]\n",
    "    \n",
    "    for index in range(len(rewards)-2, -1, -1):\n",
    "        discount = cumulative_rewards[index+1]*gamma\n",
    "        reward = rewards[index]\n",
    "        cumulative_rewards[index] = discount + reward\n",
    "    \n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9644aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc0851",
   "metadata": {},
   "source": [
    "### 损失函数和更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc2ce6b",
   "metadata": {},
   "source": [
    "我们现在需要去定义一个目标函数来更新策略\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "基于REINFORCE算法定义, 对其求偏导数，得到: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7aeb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, torch.Tensor) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return torch.Tensor(y_one_hot) if isinstance(y, torch.Tensor) else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e134ce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(agent.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "898ed28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # cast everything into a variable\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.IntTensor(actions)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.FloatTensor(cumulative_returns)\n",
    "    \n",
    "    # predict logits, probas and log-probas using an agent. \n",
    "    logits = agent(states)\n",
    "    probas = F.softmax(logits)\n",
    "    logprobas = F.log_softmax(logits)\n",
    "    \n",
    "#     logits = torch.FloatTensor(agent(states))\n",
    "#     probas = softmax(logits)\n",
    "#     log_probas = F.log_softmax(probas, dim=0)\n",
    "    \n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probas, logprobas]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_proba function\"\n",
    "    \n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions), dim = 1)\n",
    "    \n",
    "    # REINFORCE objective function\n",
    "    # J_hat = <policy objective as in the formula for J_hat. Please use mean, not sum.>\n",
    "    J_hat = torch.mean(logprobas_for_actions * cumulative_returns)\n",
    "    #regularize with entropy\n",
    "    # entropy_reg = - torch.mean(probas * logprobas) # <compute mean entropy of probas. Don't forget the sign!>\n",
    "    entropy_reg = - (probas * logprobas).sum(-1).mean()\n",
    "    loss = - J_hat - 0.1 * entropy_reg\n",
    "    \n",
    "    # Gradient descent step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # < your code >\n",
    "    \n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5722b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/xgj8n5xd2bb0cv55w7c72bn40000gn/T/ipykernel_33719/4116162574.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(logits)\n",
      "/var/folders/x9/xgj8n5xd2bb0cv55w7c72bn40000gn/T/ipykernel_33719/4116162574.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logprobas = F.log_softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:24.020\n",
      "mean reward:45.450\n",
      "mean reward:95.380\n",
      "mean reward:128.640\n",
      "mean reward:156.100\n",
      "mean reward:114.410\n",
      "mean reward:157.420\n",
      "mean reward:111.960\n",
      "mean reward:131.550\n",
      "mean reward:132.740\n",
      "mean reward:105.100\n",
      "mean reward:156.480\n",
      "mean reward:127.520\n",
      "mean reward:96.920\n",
      "mean reward:131.760\n",
      "mean reward:152.400\n",
      "mean reward:105.790\n",
      "mean reward:173.010\n",
      "mean reward:179.020\n",
      "mean reward:161.460\n",
      "mean reward:100.900\n",
      "mean reward:138.760\n",
      "mean reward:129.790\n",
      "mean reward:131.210\n",
      "mean reward:179.800\n",
      "mean reward:186.510\n",
      "mean reward:139.140\n",
      "mean reward:118.320\n",
      "mean reward:119.130\n",
      "mean reward:133.760\n",
      "mean reward:84.550\n",
      "mean reward:109.710\n",
      "mean reward:186.970\n",
      "mean reward:194.180\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    \n",
    "    rewards = [train_on_session(*generate_session()) for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 190:\n",
    "        print (\"You Win!\") # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080cd17a",
   "metadata": {},
   "source": [
    "## Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34382cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinyzqh/miniforge3/lib/python3.9/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c03684d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.33719.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c302fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
