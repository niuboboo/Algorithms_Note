{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc71b151",
   "metadata": {},
   "source": [
    "## Markov decision process\n",
    "\n",
    "<img src=\"../../images/11-Markov_Decision_Process_example.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaef32d",
   "metadata": {},
   "source": [
    "对上图的转移概率做一个定义如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc78134",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}},\n",
    "  's1':{'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}},\n",
    "  's2':{'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}}\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334ff5d",
   "metadata": {},
   "source": [
    "之后我们可以对转移概率和奖励做一个检测:\n",
    "\n",
    "1. 转移概率需要是一个字典;\n",
    "2. 状态和对应的动作下的状态转移也是一个字典，并且转移到下一个状态的概率不是0。\n",
    "3. 转移到下一个状态的所有概率之和为1。\n",
    "4. 奖励也是一个字典，状态和动作对应之后的奖励也是一个字典，因为需要确定转移到哪个下一个状态才有奖励。\n",
    "\n",
    "检测代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aff570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_param_consistency(transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state], dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                             \"a dictionary but is instead %s\" % (state, action,type(transition_probs[state, action]))\n",
    "                \n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state], dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action], dict), \"rewards for %s, %s should be a \" \\\n",
    "                        \"a dictionary but is instead %s\" % (state, action, type(transition_probs[state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
    "\n",
    "_check_param_consistency(transition_probs, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dc5eed",
   "metadata": {},
   "source": [
    "整个MDP可以定义为如下形式:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8346b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def weighted_choice(v, p):\n",
    "    total = sum(p)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in zip(v, p):\n",
    "        if upto + w >= r:\n",
    "            return c\n",
    "        upto += w\n",
    "    assert False, \"Shouldn't get here\"\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, \\\n",
    "        but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(sorted(self._transition_probs))\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if self._initial_state is None:\n",
    "            self._current_state = random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(\n",
    "            *self.get_next_states(self._current_state, action).items())\n",
    "        next_state = weighted_choice(possible_states, p=probs)\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state],\n",
    "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],\n",
    "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action,\n",
    "                                             type(transition_probs[\n",
    "                                                      state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, \\\n",
    "                                                                                                        action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state], dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action], dict), \"rewards for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action, type(transition_probs[state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048a0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2a15f",
   "metadata": {},
   "source": [
    "之后就可以像gym一样操作这个mdp了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce56e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1f868",
   "metadata": {},
   "source": [
    "对于值迭代的话，还需要其它的一些方法:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77ef9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.get_all_states = ('s0', 's1', 's2')\n",
      "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7e983",
   "metadata": {},
   "source": [
    "## 值迭代\n",
    "\n",
    "想要求解这个mdp最简单的方法就是值迭代(__V__alue __I__teration)\n",
    "\n",
    "\n",
    "其伪代码如下:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` 初始化 $V^{(0)}(s)=0$, 对所有的 $s$\n",
    "\n",
    "`2.` 遍历 $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, 对所有的 $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89d636",
   "metadata": {},
   "source": [
    "### 计算Q值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebd9dd",
   "metadata": {},
   "source": [
    "定义一个状态-动作值函数$Q^{\\pi}$:\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9299a6",
   "metadata": {},
   "source": [
    "之后要计算某个状态-动作的值函数，这里先定义一个函数来计算状态动作值函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "988238a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    Q = 0.0  # 初始化状态-动作值函数为0。\n",
    "    next_states = mdp.get_next_states(state, action)\n",
    "    for next_state in next_states:\n",
    "        prob = next_states[next_state]\n",
    "        reward = mdp.get_reward(state, action, next_state)\n",
    "        Q += (prob * (reward + gamma * state_values[next_state]))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9db9a8",
   "metadata": {},
   "source": [
    "之后测试一下计算Q值的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eca79df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_Vs = {s : i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af58b80",
   "metadata": {},
   "source": [
    "## 值迭代更新\n",
    "\n",
    "基于`Q`值，我们就可以更新状态值函数:\n",
    "\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5574110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    if mdp.is_terminal(state): return 0   # 如果是终局状态，直接返回0\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    action_values = [get_action_value(mdp, state_values, state, action, gamma) for action in actions]\n",
    "    \n",
    "    return max(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdebe5",
   "metadata": {},
   "source": [
    "测试值迭代更新代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1ea4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 0.69)\n",
    "assert test_Vs == test_Vs_copy, \"please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b77ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
