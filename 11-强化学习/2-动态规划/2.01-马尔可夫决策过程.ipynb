{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc71b151",
   "metadata": {},
   "source": [
    "## Markov decision process\n",
    "\n",
    "<img src=\"../../images/11-Markov_Decision_Process_example.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a449dd94",
   "metadata": {},
   "source": [
    "对上图的转移概率做一个定义如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc78134",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {\n",
    "  's0':{'a0': {'s0': 0.5, 's2': 0.5},\n",
    "        'a1': {'s2': 1}},\n",
    "  's1':{'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "        'a1': {'s1': 0.95, 's2': 0.05}},\n",
    "  's2':{'a0': {'s0': 0.4, 's1': 0.6},\n",
    "        'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}}\n",
    "}\n",
    "rewards = {\n",
    "  's1': {'a0': {'s0': +5}},\n",
    "  's2': {'a1': {'s0': -1}}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ba889",
   "metadata": {},
   "source": [
    "之后我们可以对转移概率和奖励做一个检测:\n",
    "\n",
    "1. 转移概率需要是一个字典;\n",
    "2. 状态和对应的动作下的状态转移也是一个字典，并且转移到下一个状态的概率不是0。\n",
    "3. 转移到下一个状态的所有概率之和为1。\n",
    "4. 奖励也是一个字典，状态和动作对应之后的奖励也是一个字典，因为需要确定转移到哪个下一个状态才有奖励。\n",
    "\n",
    "检测代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd2172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_param_consistency(transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state], dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                             \"a dictionary but is instead %s\" % (state, action,type(transition_probs[state, action]))\n",
    "                \n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state], dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action], dict), \"rewards for %s, %s should be a \" \\\n",
    "                        \"a dictionary but is instead %s\" % (state, action, type(transition_probs[state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg\n",
    "\n",
    "_check_param_consistency(transition_probs, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39101bc7",
   "metadata": {},
   "source": [
    "整个MDP可以定义为如下形式:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e799232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def weighted_choice(v, p):\n",
    "    total = sum(p)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in zip(v, p):\n",
    "        if upto + w >= r:\n",
    "            return c\n",
    "        upto += w\n",
    "    assert False, \"Shouldn't get here\"\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, \\\n",
    "        but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
    "        return tuple(sorted(self._transition_probs))\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if self._initial_state is None:\n",
    "            self._current_state = random.choice(\n",
    "                tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(\n",
    "            *self.get_next_states(self._current_state, action).items())\n",
    "        next_state = weighted_choice(possible_states, p=probs)\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state],\n",
    "                              dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (\n",
    "                                         state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action],\n",
    "                                  dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action,\n",
    "                                             type(transition_probs[\n",
    "                                                      state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, \\\n",
    "                                                                                                        action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                             \"add up to %f (should be 1)\" % (\n",
    "                                                 state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state], dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action], dict), \"rewards for %s, %s should be a \" \\\n",
    "                                         \"a dictionary but is instead %s\" % (\n",
    "                                             state, action, type(transition_probs[state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "048a0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(transition_probs, rewards, initial_state='s0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0264932",
   "metadata": {},
   "source": [
    "之后就可以像gym一样操作这个mdp了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb82de68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state = s0\n",
      "next_state = s2, reward = 0.0, done = False\n"
     ]
    }
   ],
   "source": [
    "print('initial state =', mdp.reset())\n",
    "next_state, reward, done, info = mdp.step('a1')\n",
    "print('next_state = %s, reward = %s, done = %s' % (next_state, reward, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cade78b",
   "metadata": {},
   "source": [
    "对于值迭代的话，还需要其它的一些方法:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2571e642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.get_all_states = ('s0', 's1', 's2')\n",
      "mdp.get_possible_actions('s1') =  ('a0', 'a1')\n",
      "mdp.get_next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
      "mdp.get_reward('s1', 'a0', 's0') =  5\n",
      "mdp.get_transition_prob('s1', 'a0', 's0') =  0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"mdp.get_all_states =\", mdp.get_all_states())\n",
    "print(\"mdp.get_possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
    "print(\"mdp.get_next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
    "print(\"mdp.get_reward('s1', 'a0', 's0') = \", mdp.get_reward('s1', 'a0', 's0'))\n",
    "print(\"mdp.get_transition_prob('s1', 'a0', 's0') = \", mdp.get_transition_prob('s1', 'a0', 's0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa7d77",
   "metadata": {},
   "source": [
    "## 值迭代\n",
    "\n",
    "想要求解这个mdp最简单的方法就是值迭代(__V__alue __I__teration)\n",
    "\n",
    "\n",
    "其伪代码如下:\n",
    "\n",
    "---\n",
    "\n",
    "`1.` 初始化 $V^{(0)}(s)=0$, 对所有的 $s$\n",
    "\n",
    "`2.` 遍历 $i=0, 1, 2, \\dots$\n",
    " \n",
    "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, 对所有的 $s$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab32612",
   "metadata": {},
   "source": [
    "### 计算Q值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aa4ac9",
   "metadata": {},
   "source": [
    "定义一个状态-动作值函数$Q^{\\pi}$:\n",
    "\n",
    "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581064be",
   "metadata": {},
   "source": [
    "之后要计算某个状态-动作的值函数，这里先定义一个函数来计算状态动作值函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "160a8ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_value(mdp, state_values, state, action, gamma):\n",
    "    Q = 0.0  # 初始化状态-动作值函数为0。\n",
    "    next_states = mdp.get_next_states(state, action)\n",
    "    for next_state in next_states:\n",
    "        prob = next_states[next_state]\n",
    "        reward = mdp.get_reward(state, action, next_state)\n",
    "        Q += (prob * (reward + gamma * state_values[next_state]))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21f039",
   "metadata": {},
   "source": [
    "之后测试一下计算Q值的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "561a75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_Vs = {s : i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
    "assert np.allclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb9538",
   "metadata": {},
   "source": [
    "## 值迭代更新\n",
    "\n",
    "基于`Q`值，我们就可以更新状态值函数:\n",
    "\n",
    " $$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf5aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_state_value(mdp, state_values, state, gamma):\n",
    "    if mdp.is_terminal(state): return 0   # 如果是终局状态，直接返回0\n",
    "    \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    action_values = [get_action_value(mdp, state_values, state, action, gamma) for action in actions]\n",
    "    \n",
    "    return max(action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915ce8f",
   "metadata": {},
   "source": [
    "测试值迭代更新代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033fb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Vs_copy = dict(test_Vs)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
    "assert np.allclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 0.69)\n",
    "assert test_Vs == test_Vs_copy, \"please do not change state_values in get_new_state_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b2e6e",
   "metadata": {},
   "source": [
    "之后添加循环迭代代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc93b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0   |   diff: 3.50000   |   V(s0) = 0.000   V(s1) = 3.500   V(s2) = 0.000\n",
      "\n",
      "iter    1   |   diff: 1.89000   |   V(s0) = 0.000   V(s1) = 3.815   V(s2) = 1.890\n",
      "\n",
      "iter    2   |   diff: 1.70100   |   V(s0) = 1.701   V(s1) = 4.184   V(s2) = 2.060\n",
      "\n",
      "iter    3   |   diff: 1.13542   |   V(s0) = 1.854   V(s1) = 5.319   V(s2) = 2.871\n",
      "\n",
      "iter    4   |   diff: 0.73024   |   V(s0) = 2.584   V(s1) = 5.664   V(s2) = 3.540\n",
      "\n",
      "iter    5   |   diff: 0.61135   |   V(s0) = 3.186   V(s1) = 6.275   V(s2) = 3.989\n",
      "\n",
      "iter    6   |   diff: 0.54664   |   V(s0) = 3.590   V(s1) = 6.790   V(s2) = 4.535\n",
      "\n",
      "iter    7   |   diff: 0.49198   |   V(s0) = 4.082   V(s1) = 7.189   V(s2) = 4.959\n",
      "\n",
      "iter    8   |   diff: 0.42210   |   V(s0) = 4.463   V(s1) = 7.611   V(s2) = 5.352\n",
      "\n",
      "iter    9   |   diff: 0.36513   |   V(s0) = 4.816   V(s1) = 7.960   V(s2) = 5.717\n",
      "\n",
      "iter   10   |   diff: 0.32862   |   V(s0) = 5.145   V(s1) = 8.280   V(s2) = 6.032\n",
      "\n",
      "iter   11   |   diff: 0.29262   |   V(s0) = 5.429   V(s1) = 8.572   V(s2) = 6.323\n",
      "\n",
      "iter   12   |   diff: 0.26189   |   V(s0) = 5.691   V(s1) = 8.830   V(s2) = 6.584\n",
      "\n",
      "iter   13   |   diff: 0.23503   |   V(s0) = 5.925   V(s1) = 9.065   V(s2) = 6.817\n",
      "\n",
      "iter   14   |   diff: 0.21124   |   V(s0) = 6.135   V(s1) = 9.276   V(s2) = 7.028\n",
      "\n",
      "iter   15   |   diff: 0.19012   |   V(s0) = 6.325   V(s1) = 9.465   V(s2) = 7.218\n",
      "\n",
      "iter   16   |   diff: 0.17091   |   V(s0) = 6.496   V(s1) = 9.636   V(s2) = 7.388\n",
      "\n",
      "iter   17   |   diff: 0.15366   |   V(s0) = 6.649   V(s1) = 9.790   V(s2) = 7.542\n",
      "\n",
      "iter   18   |   diff: 0.13830   |   V(s0) = 6.788   V(s1) = 9.928   V(s2) = 7.680\n",
      "\n",
      "iter   19   |   diff: 0.12445   |   V(s0) = 6.912   V(s1) = 10.052   V(s2) = 7.805\n",
      "\n",
      "iter   20   |   diff: 0.11200   |   V(s0) = 7.024   V(s1) = 10.164   V(s2) = 7.917\n",
      "\n",
      "iter   21   |   diff: 0.10079   |   V(s0) = 7.125   V(s1) = 10.265   V(s2) = 8.017\n",
      "\n",
      "iter   22   |   diff: 0.09071   |   V(s0) = 7.216   V(s1) = 10.356   V(s2) = 8.108\n",
      "\n",
      "iter   23   |   diff: 0.08164   |   V(s0) = 7.297   V(s1) = 10.437   V(s2) = 8.190\n",
      "\n",
      "iter   24   |   diff: 0.07347   |   V(s0) = 7.371   V(s1) = 10.511   V(s2) = 8.263\n",
      "\n",
      "iter   25   |   diff: 0.06612   |   V(s0) = 7.437   V(s1) = 10.577   V(s2) = 8.329\n",
      "\n",
      "iter   26   |   diff: 0.05951   |   V(s0) = 7.496   V(s1) = 10.636   V(s2) = 8.389\n",
      "\n",
      "iter   27   |   diff: 0.05356   |   V(s0) = 7.550   V(s1) = 10.690   V(s2) = 8.442\n",
      "\n",
      "iter   28   |   diff: 0.04820   |   V(s0) = 7.598   V(s1) = 10.738   V(s2) = 8.491\n",
      "\n",
      "iter   29   |   diff: 0.04338   |   V(s0) = 7.641   V(s1) = 10.782   V(s2) = 8.534\n",
      "\n",
      "iter   30   |   diff: 0.03904   |   V(s0) = 7.681   V(s1) = 10.821   V(s2) = 8.573\n",
      "\n",
      "iter   31   |   diff: 0.03514   |   V(s0) = 7.716   V(s1) = 10.856   V(s2) = 8.608\n",
      "\n",
      "iter   32   |   diff: 0.03163   |   V(s0) = 7.747   V(s1) = 10.887   V(s2) = 8.640\n",
      "\n",
      "iter   33   |   diff: 0.02846   |   V(s0) = 7.776   V(s1) = 10.916   V(s2) = 8.668\n",
      "\n",
      "iter   34   |   diff: 0.02562   |   V(s0) = 7.801   V(s1) = 10.941   V(s2) = 8.694\n",
      "\n",
      "iter   35   |   diff: 0.02306   |   V(s0) = 7.824   V(s1) = 10.964   V(s2) = 8.717\n",
      "\n",
      "iter   36   |   diff: 0.02075   |   V(s0) = 7.845   V(s1) = 10.985   V(s2) = 8.738\n",
      "\n",
      "iter   37   |   diff: 0.01867   |   V(s0) = 7.864   V(s1) = 11.004   V(s2) = 8.756\n",
      "\n",
      "iter   38   |   diff: 0.01681   |   V(s0) = 7.881   V(s1) = 11.021   V(s2) = 8.773\n",
      "\n",
      "iter   39   |   diff: 0.01513   |   V(s0) = 7.896   V(s1) = 11.036   V(s2) = 8.788\n",
      "\n",
      "iter   40   |   diff: 0.01361   |   V(s0) = 7.909   V(s1) = 11.049   V(s2) = 8.802\n",
      "\n",
      "iter   41   |   diff: 0.01225   |   V(s0) = 7.922   V(s1) = 11.062   V(s2) = 8.814\n",
      "\n",
      "iter   42   |   diff: 0.01103   |   V(s0) = 7.933   V(s1) = 11.073   V(s2) = 8.825\n",
      "\n",
      "iter   43   |   diff: 0.00992   |   V(s0) = 7.943   V(s1) = 11.083   V(s2) = 8.835\n",
      "\n",
      "iter   44   |   diff: 0.00893   |   V(s0) = 7.952   V(s1) = 11.092   V(s2) = 8.844\n",
      "\n",
      "iter   45   |   diff: 0.00804   |   V(s0) = 7.960   V(s1) = 11.100   V(s2) = 8.852\n",
      "\n",
      "iter   46   |   diff: 0.00724   |   V(s0) = 7.967   V(s1) = 11.107   V(s2) = 8.859\n",
      "\n",
      "iter   47   |   diff: 0.00651   |   V(s0) = 7.973   V(s1) = 11.113   V(s2) = 8.866\n",
      "\n",
      "iter   48   |   diff: 0.00586   |   V(s0) = 7.979   V(s1) = 11.119   V(s2) = 8.872\n",
      "\n",
      "iter   49   |   diff: 0.00527   |   V(s0) = 7.984   V(s1) = 11.125   V(s2) = 8.877\n",
      "\n",
      "iter   50   |   diff: 0.00475   |   V(s0) = 7.989   V(s1) = 11.129   V(s2) = 8.882\n",
      "\n",
      "iter   51   |   diff: 0.00427   |   V(s0) = 7.993   V(s1) = 11.134   V(s2) = 8.886\n",
      "\n",
      "iter   52   |   diff: 0.00384   |   V(s0) = 7.997   V(s1) = 11.137   V(s2) = 8.890\n",
      "\n",
      "iter   53   |   diff: 0.00346   |   V(s0) = 8.001   V(s1) = 11.141   V(s2) = 8.893\n",
      "\n",
      "iter   54   |   diff: 0.00311   |   V(s0) = 8.004   V(s1) = 11.144   V(s2) = 8.896\n",
      "\n",
      "iter   55   |   diff: 0.00280   |   V(s0) = 8.007   V(s1) = 11.147   V(s2) = 8.899\n",
      "\n",
      "iter   56   |   diff: 0.00252   |   V(s0) = 8.009   V(s1) = 11.149   V(s2) = 8.902\n",
      "\n",
      "iter   57   |   diff: 0.00227   |   V(s0) = 8.011   V(s1) = 11.152   V(s2) = 8.904\n",
      "\n",
      "iter   58   |   diff: 0.00204   |   V(s0) = 8.014   V(s1) = 11.154   V(s2) = 8.906\n",
      "\n",
      "iter   59   |   diff: 0.00184   |   V(s0) = 8.015   V(s1) = 11.155   V(s2) = 8.908\n",
      "\n",
      "iter   60   |   diff: 0.00166   |   V(s0) = 8.017   V(s1) = 11.157   V(s2) = 8.909\n",
      "\n",
      "iter   61   |   diff: 0.00149   |   V(s0) = 8.019   V(s1) = 11.159   V(s2) = 8.911\n",
      "\n",
      "iter   62   |   diff: 0.00134   |   V(s0) = 8.020   V(s1) = 11.160   V(s2) = 8.912\n",
      "\n",
      "iter   63   |   diff: 0.00121   |   V(s0) = 8.021   V(s1) = 11.161   V(s2) = 8.913\n",
      "\n",
      "iter   64   |   diff: 0.00109   |   V(s0) = 8.022   V(s1) = 11.162   V(s2) = 8.915\n",
      "\n",
      "iter   65   |   diff: 0.00098   |   V(s0) = 8.023   V(s1) = 11.163   V(s2) = 8.916\n",
      "\n",
      "Terminated\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "min_difference = 0.001\n",
    "\n",
    "state_values = {s : 0 for s in mdp.get_all_states()}\n",
    "new_state_values = {}\n",
    "for i in range(100):\n",
    "    for s in state_values:\n",
    "        new_state_values[s] = get_new_state_value(mdp, state_values, s, gamma)\n",
    "        assert isinstance(new_state_values, dict)\n",
    "    \n",
    "    diffs = [abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states()]\n",
    "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
    "    state_values = dict(new_state_values)\n",
    "    print(\"iter %4i   |   diff: %6.5f   |   \"%(i, diff), end=\"\")\n",
    "    print('   '.join(\"V(%s) = %.3f\"%(s, v) for s,v in state_values.items()), end='\\n\\n')\n",
    "    if diff < min_difference:\n",
    "        print(\"Terminated\"); break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41537b8",
   "metadata": {},
   "source": [
    "基于收敛之后的状态值函数，就可以得到最优策略:\n",
    "\n",
    "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12bdf2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
    "    \"\"\" Finds optimal action using formula above. \"\"\"\n",
    "    if mdp.is_terminal(state): return None\n",
    "     \n",
    "    actions = mdp.get_possible_actions(state)\n",
    "    \n",
    "    optimal_action = None\n",
    "    optimal_action_value = - float(\"inf\")\n",
    "    for action in actions:\n",
    "        action_value = get_action_value(mdp, state_values, state, action, gamma)\n",
    "        if action_value >= optimal_action_value:\n",
    "            optimal_action_value = action_value\n",
    "            optimal_action = action\n",
    "    \n",
    "    return optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea4d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_optimal_action(mdp, state_values, 's0', gamma) == 'a1'\n",
    "assert get_optimal_action(mdp, state_values, 's1', gamma) == 'a0'\n",
    "assert get_optimal_action(mdp, state_values, 's2', gamma) == 'a0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e30b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward:  0.9235\n"
     ]
    }
   ],
   "source": [
    "# Measure agent's average reward\n",
    "\n",
    "s = mdp.reset()\n",
    "rewards = []\n",
    "for _ in range(10000):\n",
    "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, gamma))\n",
    "    rewards.append(r)\n",
    "    \n",
    "print(\"average reward: \", np.mean(rewards))\n",
    "\n",
    "assert(0.85 < np.mean(rewards) < 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc07ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
