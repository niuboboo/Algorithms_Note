{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721e589d",
   "metadata": {},
   "source": [
    "## QLearning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b5d69",
   "metadata": {},
   "source": [
    "吃豆人项目地址: https://inst.eecs.berkeley.edu/~cs188/pacman/project_overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f37d086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random, math\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4722d",
   "metadata": {},
   "source": [
    "对于一个`Q-Learning`强化学习智能体来说，需要传入的参数有：\n",
    "\n",
    "1. 环境\n",
    "2. 智能体学习所需要的超参数，像学习率，折扣因子\n",
    "\n",
    "需要的方法有:\n",
    "\n",
    "1. 依据状态选择动作的策略\n",
    "2. 更新智能体的方法\n",
    "\n",
    "```python\n",
    "class QLearningAgent(object):\n",
    "    def __init__(self, env, learning_rate, epsilon, discount):\n",
    "        super(QLearningAgent, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        pass\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e12f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(object):\n",
    "    def __init__(self, env, args):\n",
    "        super(QLearningAgent, self).__init__()\n",
    "        self.legal_action = range(env.action_space.n)  # 获取合法的动作\n",
    "        \n",
    "        self.epsilon = args.epsilon\n",
    "        self.discount = args.gamma\n",
    "        \n",
    "        self.q_table = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.lr = args.lr\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        依据状态选择动作, 加入合法动作是为了方便之后用于复杂环境留的接口\n",
    "        \n",
    "        基于epsilon贪婪策略选择动作。\n",
    "        self.epsilon概率选择随机动作，1 - self.epsilon概率选择最好的动作。\n",
    "        \n",
    "        \"\"\"\n",
    "        possible_actions = self.legal_action\n",
    "        \n",
    "        if len(possible_actions) == 0:\n",
    "            print(\"return None\")\n",
    "            return None\n",
    "        \n",
    "        epsilon = self.epsilon\n",
    "        \n",
    "        if np.random.random() > epsilon:\n",
    "            possible_q_values = [self.q_table[state][action] for action in possible_actions]\n",
    "            index = np.argmax(possible_q_values)\n",
    "            \n",
    "            return possible_actions[index]\n",
    "        else:\n",
    "            \n",
    "            return random.choice(possible_actions)\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        Q值更新公式如下:\n",
    "        Q(s,a) := (1 - lr) * Q(s,a) + lr * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        next_state_possible_actions = self.legal_action\n",
    "        \n",
    "        next_v = np.max([self.q_table[next_state][next_action] for next_action in next_state_possible_actions])\n",
    "        \n",
    "        new_q = (1 - self.lr) * current_q + self.lr * (reward + self.discount * next_v)\n",
    "        \n",
    "        self.q_table[state][action] = new_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b371c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;除了上述的这种实现方法之外，还有一种是在计算下一个状态值函数的时候以`epsilon`概率选择最大的`V(next_state)`, `1-epsilon`概率选择平均的`V(next_state)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e42fac9",
   "metadata": {},
   "source": [
    "## Q-Learning用于离散状态空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c4623ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        action = agent.choose_action(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done: break\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8097134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"The parameter of Q-Learning\")\n",
    "    parser.add_argument(\"--gamma\", type=float, help=\"gamma value used for Bellman approximation\", default=0.99)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"learning rate used in the Adam optimizer\", default=0.5)\n",
    "    parser.add_argument(\"--epsilon\", type=float, help=\"epsilon for greedy\", default=0.25)\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "    print(\"observation_space {}\".format(env.observation_space))\n",
    "    print(\"action_space {}\".format(env.action_space))\n",
    "\n",
    "    agent = QLearningAgent(env, args)\n",
    "    \n",
    "    rewards = []\n",
    "    for i in range(1000):\n",
    "        rewards.append(play_and_train(env, agent))    \n",
    "        if i %100 ==0:\n",
    "            clear_output(True)\n",
    "            print(\"mean reward\",np.mean(rewards[max(0, len(rewards) - 100) : len(rewards)]))\n",
    "            plt.plot(rewards)\n",
    "            plt.show()\n",
    "            \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f95116",
   "metadata": {},
   "source": [
    "## Q-learning用于连续状态空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aebd9a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(observation):\n",
    "    pos_space = np.linspace(-1.2, 0.6, 12)\n",
    "    vel_space = np.linspace(-0.07, 0.07, 20)\n",
    "    \n",
    "    pos, vel =  observation\n",
    "    pos_bin = int(np.digitize(pos, pos_space))\n",
    "    vel_bin = int(np.digitize(vel, vel_space))\n",
    "\n",
    "    return (pos_bin, vel_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdfbc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(observation):\n",
    "    pos_space = np.linspace(-1.2, 0.6, 12)\n",
    "    vel_space = np.linspace(-0.07, 0.07, 20)\n",
    "    \n",
    "    pos, vel =  observation\n",
    "    pos_bin = int(np.digitize(pos, pos_space))\n",
    "    vel_bin = int(np.digitize(vel, vel_space))\n",
    "\n",
    "    return (pos_bin, vel_bin)\n",
    "\n",
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    total_reward = 0.0\n",
    "    state = get_state(env.reset())\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        action = agent.choose_action(state)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = get_state(next_state)\n",
    "        \n",
    "        agent.learn(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done: break\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8122a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "action_space Discrete(3)\n",
      "episode  0 score  -1000.0 epsilon 1.000\n",
      "episode  100 score  -1000.0 epsilon 0.996\n",
      "episode  200 score  -1000.0 epsilon 0.992\n",
      "episode  300 score  -1000.0 epsilon 0.988\n",
      "episode  400 score  -1000.0 epsilon 0.984\n",
      "episode  500 score  -1000.0 epsilon 0.980\n",
      "episode  600 score  -1000.0 epsilon 0.976\n",
      "episode  700 score  -1000.0 epsilon 0.972\n",
      "episode  800 score  -1000.0 epsilon 0.968\n",
      "episode  900 score  -1000.0 epsilon 0.964\n",
      "episode  1000 score  -1000.0 epsilon 0.960\n",
      "episode  1100 score  -1000.0 epsilon 0.956\n",
      "episode  1200 score  -1000.0 epsilon 0.952\n",
      "episode  1300 score  -1000.0 epsilon 0.948\n",
      "episode  1400 score  -1000.0 epsilon 0.944\n",
      "episode  1500 score  -1000.0 epsilon 0.940\n",
      "episode  1600 score  -1000.0 epsilon 0.936\n",
      "episode  1700 score  -1000.0 epsilon 0.932\n",
      "episode  1800 score  -969.0 epsilon 0.928\n",
      "episode  1900 score  -1000.0 epsilon 0.924\n",
      "episode  2000 score  -1000.0 epsilon 0.920\n",
      "episode  2100 score  -1000.0 epsilon 0.916\n",
      "episode  2200 score  -1000.0 epsilon 0.912\n",
      "episode  2300 score  -1000.0 epsilon 0.908\n",
      "episode  2400 score  -1000.0 epsilon 0.904\n",
      "episode  2500 score  -1000.0 epsilon 0.900\n",
      "episode  2600 score  -1000.0 epsilon 0.896\n",
      "episode  2700 score  -1000.0 epsilon 0.892\n",
      "episode  2800 score  -1000.0 epsilon 0.888\n",
      "episode  2900 score  -692.0 epsilon 0.884\n",
      "episode  3000 score  -813.0 epsilon 0.880\n",
      "episode  3100 score  -1000.0 epsilon 0.876\n",
      "episode  3200 score  -1000.0 epsilon 0.872\n",
      "episode  3300 score  -1000.0 epsilon 0.868\n",
      "episode  3400 score  -1000.0 epsilon 0.864\n",
      "episode  3500 score  -1000.0 epsilon 0.860\n",
      "episode  3600 score  -1000.0 epsilon 0.856\n",
      "episode  3700 score  -1000.0 epsilon 0.852\n",
      "episode  3800 score  -513.0 epsilon 0.848\n",
      "episode  3900 score  -1000.0 epsilon 0.844\n",
      "episode  4000 score  -1000.0 epsilon 0.840\n",
      "episode  4100 score  -1000.0 epsilon 0.836\n",
      "episode  4200 score  -1000.0 epsilon 0.832\n",
      "episode  4300 score  -1000.0 epsilon 0.828\n",
      "episode  4400 score  -860.0 epsilon 0.824\n",
      "episode  4500 score  -637.0 epsilon 0.820\n",
      "episode  4600 score  -1000.0 epsilon 0.816\n",
      "episode  4700 score  -1000.0 epsilon 0.812\n",
      "episode  4800 score  -1000.0 epsilon 0.808\n",
      "episode  4900 score  -1000.0 epsilon 0.804\n",
      "episode  5000 score  -1000.0 epsilon 0.800\n",
      "episode  5100 score  -1000.0 epsilon 0.796\n",
      "episode  5200 score  -786.0 epsilon 0.792\n",
      "episode  5300 score  -888.0 epsilon 0.788\n",
      "episode  5400 score  -1000.0 epsilon 0.784\n",
      "episode  5500 score  -1000.0 epsilon 0.780\n",
      "episode  5600 score  -1000.0 epsilon 0.776\n",
      "episode  5700 score  -901.0 epsilon 0.772\n",
      "episode  5800 score  -1000.0 epsilon 0.768\n",
      "episode  5900 score  -1000.0 epsilon 0.764\n",
      "episode  6000 score  -1000.0 epsilon 0.760\n",
      "episode  6100 score  -672.0 epsilon 0.756\n",
      "episode  6200 score  -428.0 epsilon 0.752\n",
      "episode  6300 score  -875.0 epsilon 0.748\n",
      "episode  6400 score  -584.0 epsilon 0.744\n",
      "episode  6500 score  -1000.0 epsilon 0.740\n",
      "episode  6600 score  -602.0 epsilon 0.736\n",
      "episode  6700 score  -1000.0 epsilon 0.732\n",
      "episode  6800 score  -1000.0 epsilon 0.728\n",
      "episode  6900 score  -1000.0 epsilon 0.724\n",
      "episode  7000 score  -1000.0 epsilon 0.720\n",
      "episode  7100 score  -465.0 epsilon 0.716\n",
      "episode  7200 score  -704.0 epsilon 0.712\n",
      "episode  7300 score  -860.0 epsilon 0.708\n",
      "episode  7400 score  -1000.0 epsilon 0.704\n",
      "episode  7500 score  -633.0 epsilon 0.700\n",
      "episode  7600 score  -791.0 epsilon 0.696\n",
      "episode  7700 score  -620.0 epsilon 0.692\n",
      "episode  7800 score  -551.0 epsilon 0.688\n",
      "episode  7900 score  -606.0 epsilon 0.684\n",
      "episode  8000 score  -859.0 epsilon 0.680\n",
      "episode  8100 score  -750.0 epsilon 0.676\n",
      "episode  8200 score  -406.0 epsilon 0.672\n",
      "episode  8300 score  -663.0 epsilon 0.668\n",
      "episode  8400 score  -487.0 epsilon 0.664\n",
      "episode  8500 score  -1000.0 epsilon 0.660\n",
      "episode  8600 score  -343.0 epsilon 0.656\n",
      "episode  8700 score  -615.0 epsilon 0.652\n",
      "episode  8800 score  -620.0 epsilon 0.648\n",
      "episode  8900 score  -421.0 epsilon 0.644\n",
      "episode  9000 score  -997.0 epsilon 0.640\n",
      "episode  9100 score  -885.0 epsilon 0.636\n",
      "episode  9200 score  -571.0 epsilon 0.632\n",
      "episode  9300 score  -362.0 epsilon 0.628\n",
      "episode  9400 score  -330.0 epsilon 0.624\n",
      "episode  9500 score  -377.0 epsilon 0.620\n",
      "episode  9600 score  -838.0 epsilon 0.616\n",
      "episode  9700 score  -398.0 epsilon 0.612\n",
      "episode  9800 score  -313.0 epsilon 0.608\n",
      "episode  9900 score  -588.0 epsilon 0.604\n",
      "episode  10000 score  -516.0 epsilon 0.600\n",
      "episode  10100 score  -238.0 epsilon 0.596\n",
      "episode  10200 score  -784.0 epsilon 0.592\n",
      "episode  10300 score  -297.0 epsilon 0.588\n",
      "episode  10400 score  -291.0 epsilon 0.584\n",
      "episode  10500 score  -513.0 epsilon 0.580\n",
      "episode  10600 score  -621.0 epsilon 0.576\n",
      "episode  10700 score  -410.0 epsilon 0.572\n",
      "episode  10800 score  -284.0 epsilon 0.568\n",
      "episode  10900 score  -303.0 epsilon 0.564\n",
      "episode  11000 score  -857.0 epsilon 0.560\n",
      "episode  11100 score  -485.0 epsilon 0.556\n",
      "episode  11200 score  -1000.0 epsilon 0.552\n",
      "episode  11300 score  -333.0 epsilon 0.548\n",
      "episode  11400 score  -301.0 epsilon 0.544\n",
      "episode  11500 score  -248.0 epsilon 0.540\n",
      "episode  11600 score  -428.0 epsilon 0.536\n",
      "episode  11700 score  -466.0 epsilon 0.532\n",
      "episode  11800 score  -324.0 epsilon 0.528\n",
      "episode  11900 score  -248.0 epsilon 0.524\n",
      "episode  12000 score  -327.0 epsilon 0.520\n",
      "episode  12100 score  -420.0 epsilon 0.516\n",
      "episode  12200 score  -314.0 epsilon 0.512\n",
      "episode  12300 score  -332.0 epsilon 0.508\n",
      "episode  12400 score  -229.0 epsilon 0.504\n",
      "episode  12500 score  -570.0 epsilon 0.500\n",
      "episode  12600 score  -696.0 epsilon 0.496\n",
      "episode  12700 score  -248.0 epsilon 0.492\n",
      "episode  12800 score  -400.0 epsilon 0.488\n",
      "episode  12900 score  -330.0 epsilon 0.484\n",
      "episode  13000 score  -360.0 epsilon 0.480\n",
      "episode  13100 score  -329.0 epsilon 0.476\n",
      "episode  13200 score  -264.0 epsilon 0.472\n",
      "episode  13300 score  -418.0 epsilon 0.468\n",
      "episode  13400 score  -493.0 epsilon 0.464\n",
      "episode  13500 score  -414.0 epsilon 0.460\n",
      "episode  13600 score  -378.0 epsilon 0.456\n",
      "episode  13700 score  -404.0 epsilon 0.452\n",
      "episode  13800 score  -835.0 epsilon 0.448\n",
      "episode  13900 score  -400.0 epsilon 0.444\n",
      "episode  14000 score  -401.0 epsilon 0.440\n",
      "episode  14100 score  -1000.0 epsilon 0.436\n",
      "episode  14200 score  -360.0 epsilon 0.432\n",
      "episode  14300 score  -241.0 epsilon 0.428\n",
      "episode  14400 score  -494.0 epsilon 0.424\n",
      "episode  14500 score  -238.0 epsilon 0.420\n",
      "episode  14600 score  -322.0 epsilon 0.416\n",
      "episode  14700 score  -234.0 epsilon 0.412\n",
      "episode  14800 score  -252.0 epsilon 0.408\n",
      "episode  14900 score  -348.0 epsilon 0.404\n",
      "episode  15000 score  -280.0 epsilon 0.400\n",
      "episode  15100 score  -1000.0 epsilon 0.396\n",
      "episode  15200 score  -491.0 epsilon 0.392\n",
      "episode  15300 score  -309.0 epsilon 0.388\n",
      "episode  15400 score  -279.0 epsilon 0.384\n",
      "episode  15500 score  -308.0 epsilon 0.380\n",
      "episode  15600 score  -286.0 epsilon 0.376\n",
      "episode  15700 score  -171.0 epsilon 0.372\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     total_rewards = np.zeros(n_games)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_games):\n\u001b[0;32m---> 20\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mplay_and_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         total_rewards[i] \u001b[38;5;241m=\u001b[39m score\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#         total_rewards[i] = score\u001b[39;00m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mplay_and_train\u001b[0;34m(env, agent, t_max)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t_max):\n\u001b[1;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m---> 19\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m get_state(next_state)\n\u001b[1;32m     22\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn(state, action, reward, next_state)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 17\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/gym/envs/classic_control/mountain_car.py:87\u001b[0m, in \u001b[0;36mMountainCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     85\u001b[0m position, velocity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m     86\u001b[0m velocity \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (action \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mcos(\u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m position) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgravity)\n\u001b[0;32m---> 87\u001b[0m velocity \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvelocity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_speed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m position \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m velocity\n\u001b[1;32m     89\u001b[0m position \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_position, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_position)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2152\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2085\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2087\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \n\u001b[1;32m   2151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/core/_methods.py:137\u001b[0m, in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mmin\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    136\u001b[0m     using_deprecated_nan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_clip_dep_is_scalar_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    139\u001b[0m     using_deprecated_nan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/core/_methods.py:97\u001b[0m, in \u001b[0;36m_clip_dep_is_scalar_nan\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnan\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"The parameter of Q-Learning\")\n",
    "    parser.add_argument(\"--gamma\", type=float, help=\"gamma value used for Bellman approximation\", default=0.99)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"learning rate used in the Adam optimizer\", default=0.1)\n",
    "    parser.add_argument(\"--epsilon\", type=float, help=\"epsilon for greedy\", default=1.0)\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env._max_episode_steps = 1000\n",
    "    print(\"observation_space {}\".format(env.observation_space))\n",
    "    print(\"action_space {}\".format(env.action_space))\n",
    "    \n",
    "    agent = QLearningAgent(env, args)\n",
    "    \n",
    "    n_games = 50000\n",
    "    rewards = []\n",
    "    total_rewards = np.zeros(n_games)\n",
    "#     total_rewards = np.zeros(n_games)\n",
    "    for i in range(n_games):\n",
    "        score = play_and_train(env, agent)\n",
    "        total_rewards[i] = score\n",
    "#         total_rewards[i] = score\n",
    "\n",
    "        agent.epsilon = agent.epsilon - 2 / n_games if agent.epsilon > 0.01 else 0.01\n",
    "        \n",
    "        if i % 100 ==0:\n",
    "            print('episode ', i, 'score ', score, 'epsilon %.3f' % agent.epsilon)\n",
    "#             if i %100 ==0:\n",
    "#             clear_output(True)\n",
    "#             print(\"mean reward \", mean_reward[-1])\n",
    "#             plt.plot(mean_reward)\n",
    "#             plt.show()\n",
    "    mean_rewards = np.zeros(n_games)\n",
    "    for t in range(n_games):\n",
    "        mean_rewards[t] = np.mean(total_rewards[max(0, t-50):(t+1)])\n",
    "    plt.plot(mean_rewards)\n",
    "    plt.show()\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7477e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_rewards = np.zeros(n_games)\n",
    "# for t in range(n_games):\n",
    "#     mean_rewards[t] = np.mean(total_rewards[max(0, t-50):(t+1)])\n",
    "# plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5aea19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
