{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ddb0f7",
   "metadata": {},
   "source": [
    "<img src=\"../../images/Transformer.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c4d32",
   "metadata": {},
   "source": [
    "Transformer整体上来看是分为两部分：encoder和decoder部分。encoder以输入字符作为输入，以状态作为输出。decoder以上一时刻的字符作为输入，把encode输出状态作为输入的一部分，最终返回字符的预测概率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4269e",
   "metadata": {},
   "source": [
    "encoder以输入的编码和位置编码来作为输入，然后encoder又由N层构成，也就是N个block构成，每个block里面又包含两部分，Multi-Head Attention，对序列自身的表征运算。第二部分是一个前馈神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82cd49f",
   "metadata": {},
   "source": [
    "decoder第一部分：以待预测字符和位置编码作为输入，之后经过带有mask的multi-head attention，这里带mask是因为预测的时候，不能看到之后的字符，而只能看见之前的字符。第二部分：是交叉注意力，以mask multi-head attention的输出作为query，以encoder的输出状态作为key和value来计算出encoder序列和decoder序列之间的关联性，来得到一个表征。decoder第三部分就是一个全连接层。整个网络最后接一个线性层连接到输出概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4bec00",
   "metadata": {},
   "source": [
    "由于transformer对于局部和全局的位置信息都不敏感，所以我们需要增加一个位置编码。由于encoder和decoder的block中有大量的残差连接，所以位置信息可以充分地传送到上层网络中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601e4f49",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1468a4b",
   "metadata": {},
   "source": [
    "在`encoder`中，首先将稀疏的`one-hot`向量送入到一个不带`bias`的全连接网络中，得到一个稠密向量，一方面可以节省内存，另一方面可以使得单词的表征更加丰富一些。这一层也称为嵌入层:\n",
    "\n",
    "```python\n",
    "# 这里Embeddings中的s表示两个一模一样的嵌入层，他们共享参数。\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        # d_model: 指词嵌入的维度。\n",
    "        # vocab: 指词表的大小。如果是英译法，那英文的词表大小就是源文本词表大小，法文词表大小为目标词表大小。\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        # 参数x: 因为词嵌入是首层，所以代表输入给模型的文本通过词汇映射后的张量。\n",
    "        \"\"\"\n",
    "        return self.lut(x) * math.sqrt(self.d_model) # math.sqrt(self.d_model)其缩放的作用\n",
    "```\n",
    "\n",
    "`nn.Embedding`实例化及其调用过程可以描述为如下形式:\n",
    "\n",
    "```python\n",
    "embedding = nn.Embedding(10, 3)\n",
    "input_word = torch.LongTensor([[1,2,4,1],[4,3,2,9]])\n",
    "print(embedding(input_word))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975801fd",
   "metadata": {},
   "source": [
    "### Position Encoding\n",
    "\n",
    "由于考虑的是序列建模问题，而`Transformer`对位置不敏感，所以引入`position encoding`，将其加到`word embedding`上。\n",
    "\n",
    "我们对`position encoding`有一些假设，希望`position encoding`是一个确定的，最好不要有不确定的，比如说将一个序列分成`0-1`之间的均匀划分，这样的话，训练长度不一致的话，每个位置上的编码也就不一样了，比如序列长度为`10`的话，第二个位置上是`0.1`，序列长度为`5`的话，又变成了`0.2`。还希望，对于不同长短的句子中，相同位置的距离一致，比如在一个长句子中，两个字符隔了两个单词，和相同的两个字符在一个短句子中隔了两个单词，他们之间的相对距离要保持一致。还希望这个位置编码可以推广到更长的测试句子。有了上述的三种假设之后，作者提出了sin和cos函数来表征这样的信息。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75cd8f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5b758",
   "metadata": {},
   "source": [
    "对于序列建模问题，有source sentence和target sentence。构建序列，序列字符以索引形式表示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cb39b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 3])\n",
      "tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "src_len = torch.randint(2, 5, (batch_size, )) # 源序列长度, 最小长度为2，最大长度为5\n",
    "tgt_len = torch.randint(2, 5, (batch_size, )) # 目标序列长度, 最小长度为2，最大长度为5\n",
    "print(src_len)\n",
    "print(tgt_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848566f4",
   "metadata": {},
   "source": [
    "输出结果解释：因为batch size为2，所以会有两个句子，每个数字都表示某个句子的长度。有了长度之后，我们就可以生成源序列和目标序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6298a08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([5, 6, 5, 5]), tensor([6, 1, 1])]\n",
      "[tensor([5, 3, 7]), tensor([5, 2, 4, 3])]\n"
     ]
    }
   ],
   "source": [
    "max_num_src_word = 8  # 源序列单词总数为8\n",
    "max_num_tgt_word = 8  # 源序列单词总数为8\n",
    "\n",
    "src_seq = [torch.randint(1, max_num_src_word, (L,)) for L in src_len]\n",
    "tgt_seq = [torch.randint(1, max_num_tgt_word, (L,)) for L in tgt_len]\n",
    "print(src_seq)\n",
    "print(tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f76fc0",
   "metadata": {},
   "source": [
    "这样我们就随机生成了一个batch size为2，源序列目标长度为src_len的，单词表大小为max_num_src_word的源序列。目标序列类似。但是为了保证输入到网络中，我们给定的序列是一样的，所以我们需要加padding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f5cb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([7, 4, 4, 2]), tensor([4, 3, 5, 0])]\n",
      "[tensor([3, 6, 6, 0]), tensor([7, 5, 5, 3])]\n"
     ]
    }
   ],
   "source": [
    "max_src_seq_len = max(src_len)  # 序列最大长度\n",
    "max_tgt_seq_len = max(tgt_len)  # 序列最大长度\n",
    "\n",
    "# pad左边不用pad，填0，右边pad\n",
    "src_seq = [F.pad(torch.randint(1, max_num_src_word, (L,)), (0, max_src_seq_len-L)) for L in src_len]\n",
    "tgt_seq = [F.pad(torch.randint(1, max_num_tgt_word, (L,)), (0, max_tgt_seq_len-L)) for L in tgt_len]\n",
    "print(src_seq)\n",
    "print(tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd0913",
   "metadata": {},
   "source": [
    "之后我们还需要将其cat起来变成一个二维张量,在cat之间需要用torch.unsqueeze对其升高一个维度:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6a58b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3, 5, 6],\n",
      "        [5, 2, 2, 0]])\n",
      "tensor([[3, 2, 7, 0],\n",
      "        [7, 3, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "max_src_seq_len = max(src_len)  # 序列最大长度\n",
    "max_tgt_seq_len = max(tgt_len)  # 序列最大长度\n",
    "\n",
    "# pad左边不用pad，填0，右边pad\n",
    "src_seq = [F.pad(torch.unsqueeze(torch.randint(1, max_num_src_word, (L,)), 0), (0, max_src_seq_len-L)) \\\n",
    "                                                                                            for L in src_len]\n",
    "tgt_seq = [F.pad(torch.unsqueeze(torch.randint(1, max_num_tgt_word, (L,)), 0), (0, max_tgt_seq_len-L)) \\\n",
    "                                                                                            for L in tgt_len]\n",
    "\n",
    "src_seq = torch.cat(src_seq, 0)\n",
    "tgt_seq = torch.cat(tgt_seq, 0)\n",
    "\n",
    "print(src_seq)\n",
    "print(tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7ac86",
   "metadata": {},
   "source": [
    "之后就需要去构造embedding,其官网地址:\n",
    "\n",
    "[https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding)\n",
    "\n",
    "```python\n",
    "CLASS torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "num_embeddings参数表示单词表的大小，embedding_dim为编码之后的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b154a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4440,  0.9642,  0.4904,  0.6679, -0.4161,  1.6588,  0.8813, -0.5609],\n",
      "        [-1.3250,  3.0019,  0.7034,  0.1382, -0.6140, -0.3905, -0.6831, -0.1695],\n",
      "        [ 1.6436, -0.1385, -0.3549,  0.0512, -1.6353, -0.0802,  0.9369, -1.3509],\n",
      "        [-0.3555, -0.1949,  0.2134,  0.2540, -2.1946,  0.1603,  1.0509,  0.4482],\n",
      "        [-1.1937, -0.0556,  1.0496, -0.1971,  0.8452, -0.6711,  0.6527,  0.4400],\n",
      "        [-0.7475, -0.5348, -1.4369,  0.1944,  0.4523, -0.5907,  1.3619,  0.3157],\n",
      "        [ 1.7535, -0.1772,  1.6677,  0.1393,  0.0733, -0.2864,  0.3537,  1.0370],\n",
      "        [ 0.3950,  0.6527, -0.2264, -0.1220,  0.5487,  0.0403, -0.0487, -2.1227]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model_dim = 8 # 原始论文中为512\n",
    "\n",
    "src_embedding_table = nn.Embedding(max_num_src_word, model_dim)\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_word, model_dim)\n",
    "\n",
    "print(src_embedding_table.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4441f806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 8])\n",
      "torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
    "print(src_embedding.size())\n",
    "print(tgt_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e012a",
   "metadata": {},
   "source": [
    "之后我们就需要去构造position embedding：\n",
    "\n",
    "$$\n",
    "\\text{PE}(i,\\delta) = \n",
    "\\begin{cases}\n",
    "\\sin(\\frac{i}{10000^{2\\delta'/d}}) & \\text{if } \\delta = 2\\delta'\\\\\n",
    "\\cos(\\frac{i}{10000^{2\\delta'/d}}) & \\text{if } \\delta = 2\\delta' + 1\\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc323ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[   1.,   10.,  100., 1000.]])\n"
     ]
    }
   ],
   "source": [
    "max_src_position_len = max_src_seq_len\n",
    "max_tgt_position_len = max_tgt_seq_len\n",
    "\n",
    "\n",
    "pos_matrix = torch.arange(max(max_src_position_len, max_tgt_position_len)).reshape(-1, 1)\n",
    "i_mat = torch.pow(10000, torch.arange(0, model_dim, 2).reshape(1, -1)/model_dim)  # 间隔是2\n",
    "print(pos_matrix)\n",
    "print(i_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5b557",
   "metadata": {},
   "source": [
    "之后我们再初始化一个pe_embedding_table，然后对其进行赋值即可:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a24c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# 取max_src_position_len和max_tgt_position_len中最大的是期望两个position能用一个embedding。\n",
    "\n",
    "pe_embedding_table = torch.zeros(max(max_src_position_len, max_tgt_position_len), model_dim)\n",
    "\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_matrix / i_mat)  # 赋值偶数列\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_matrix / i_mat)  # 赋值奇数列\n",
    "print(pe_embedding_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060c521",
   "metadata": {},
   "source": [
    "之后把pe_embedding_table中的值传入到nn.Embedding中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eadc8665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "          9.9955e-01,  3.0000e-03,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "pe_embedding = nn.Embedding(max(max_src_position_len, max_tgt_position_len), model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "print(pe_embedding.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e38967",
   "metadata": {},
   "source": [
    "之后就可以直接用pe_embedding作为一个函数对src_seq和tgt_seq进行embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126f7027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n",
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
      "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
      "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
      "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
      "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len]).to(torch.int32)\n",
    "\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "\n",
    "print(src_pe_embedding)\n",
    "print(tgt_pe_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7917ad",
   "metadata": {},
   "source": [
    "### Multi-head Self-attention\n",
    "\n",
    "Multi-head Self-attention可以使得建模能力更强，表征空间更加丰富。多头注意力机制由多组Q，K，V构成，每组单独计算一个attention向量。最后把每组的attention向量拼接起来，并进入到一个不带bias的FFN中，得到最终的向量。\n",
    "\n",
    "多头注意力机制会使得特征向量的特征维度降低，比如原来的特征向量维度是512的，如果分为8个head的话，那么每个head的维度就会变成64，这样做也是为了保证整体上的运算量没有太大的改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dfb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07448698",
   "metadata": {},
   "source": [
    "在encoder中，Q和K都是word embedding经过两个linear层得到Q和K。\n",
    "\n",
    "Scaled Dot-Product Attention:\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}}) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a656b4",
   "metadata": {},
   "source": [
    "对于两两单词而言，$Q$和$K$的相乘就是两个单词的内积。而一个query对所有的key依次做内积的话就是相似度的了，再softmax一下之后就是相似度的概率。而除以$\\sqrt{d_{k}}$是希望softmax出来的结果方差不要太大，同时也希望雅可比矩阵出来的导数不要变成0了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee6a8e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2184, 0.1966, 0.2013, 0.1828, 0.2009])\n",
      "tensor([9.9946e-01, 2.7264e-05, 2.8074e-04, 1.8708e-08, 2.3486e-04])\n"
     ]
    }
   ],
   "source": [
    "alpha1 = 0.1\n",
    "alpha2 = 10\n",
    "score = torch.randn(5)\n",
    "prob1 = F.softmax(score * alpha1, -1)\n",
    "prob2 = F.softmax(score * alpha2, -1)\n",
    "print(prob1)\n",
    "print(prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca69f00",
   "metadata": {},
   "source": [
    "可以看到当score放大时，方差是比较大的，对最终的概率影响也不是线形的。概率大的会相对更大，概率小的会相对更小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eef4b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1707, -0.0429, -0.0440, -0.0399, -0.0439],\n",
      "        [-0.0429,  0.1580, -0.0396, -0.0359, -0.0395],\n",
      "        [-0.0440, -0.0396,  0.1608, -0.0368, -0.0404],\n",
      "        [-0.0399, -0.0359, -0.0368,  0.1494, -0.0367],\n",
      "        [-0.0439, -0.0395, -0.0404, -0.0367,  0.1605]])\n",
      "tensor([[ 5.4252e-04, -2.7249e-05, -2.8059e-04, -1.8698e-08, -2.3473e-04],\n",
      "        [-2.7249e-05,  2.7263e-05, -7.6541e-09, -5.1004e-13, -6.4032e-09],\n",
      "        [-2.8059e-04, -7.6541e-09,  2.8066e-04, -5.2520e-12, -6.5936e-08],\n",
      "        [-1.8698e-08, -5.1004e-13, -5.2520e-12,  1.8708e-08, -4.3937e-12],\n",
      "        [-2.3473e-04, -6.4032e-09, -6.5936e-08, -4.3937e-12,  2.3481e-04]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x9/xgj8n5xd2bb0cv55w7c72bn40000gn/T/ipykernel_22180/4131119695.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(score)\n"
     ]
    }
   ],
   "source": [
    "def softmax_func(score):\n",
    "    return F.softmax(score)\n",
    "\n",
    "jaco_mat1 = torch.autograd.functional.jacobian(softmax_func, score * alpha1)\n",
    "jaco_mat2 = torch.autograd.functional.jacobian(softmax_func, score * alpha2)\n",
    "\n",
    "print(jaco_mat1)\n",
    "print(jaco_mat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d81ac7",
   "metadata": {},
   "source": [
    "可以看到，alpha比较大的时候，梯度会变成0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5b3be",
   "metadata": {},
   "source": [
    "如果encoder中需要构造mask的话，期望被mask的元素值为负无穷。mask的shape需要和score得分的维度一致。\n",
    "\n",
    "我们首先需要构造有效矩阵, 除了有效位置之外的位置补0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd09c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max_src_seq_len-L)), 0) \\\n",
    "                                               for L in src_len]), 2)\n",
    "print(valid_encoder_pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e56b5cf",
   "metadata": {},
   "source": [
    "这矩阵乘其自身的转置就能得到领接矩阵，也就是相关性:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3258b866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
    "print(valid_encoder_pos_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2e9bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "invalid_encoder_pos_matrix = 1 - valid_encoder_pos_matrix\n",
    "print(invalid_encoder_pos_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "050de80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]],\n",
      "\n",
      "        [[False, False, False,  True],\n",
      "         [False, False, False,  True],\n",
      "         [False, False, False,  True],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "print(mask_encoder_self_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd5cda7",
   "metadata": {},
   "source": [
    "True代表那些位置需要mask，False代表那些位置不需要mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "849dc218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 3])\n",
      "tensor([[[0.1168, 0.6235, 0.2864, 0.2388],\n",
      "         [0.3130, 0.5479, 0.1937, 0.9192],\n",
      "         [0.0649, 0.3943, 0.2376, 0.0828],\n",
      "         [0.1616, 0.2483, 0.9941, 0.3251]],\n",
      "\n",
      "        [[0.5100, 0.9872, 0.7432, 0.9793],\n",
      "         [0.1166, 0.8348, 0.6957, 0.9492],\n",
      "         [0.6588, 0.9512, 0.7108, 0.4154],\n",
      "         [0.3914, 0.9951, 0.1293, 0.8756]]])\n",
      "tensor([[[ 1.1677e-01,  6.2355e-01,  2.8638e-01,  2.3877e-01],\n",
      "         [ 3.1300e-01,  5.4785e-01,  1.9374e-01,  9.1919e-01],\n",
      "         [ 6.4862e-02,  3.9433e-01,  2.3756e-01,  8.2819e-02],\n",
      "         [ 1.6165e-01,  2.4830e-01,  9.9411e-01,  3.2507e-01]],\n",
      "\n",
      "        [[ 5.1005e-01,  9.8717e-01,  7.4319e-01, -1.0000e-09],\n",
      "         [ 1.1661e-01,  8.3479e-01,  6.9566e-01, -1.0000e-09],\n",
      "         [ 6.5880e-01,  9.5118e-01,  7.1082e-01, -1.0000e-09],\n",
      "         [-1.0000e-09, -1.0000e-09, -1.0000e-09, -1.0000e-09]]])\n",
      "tensor([[[0.2010, 0.3337, 0.2382, 0.2271],\n",
      "         [0.2006, 0.2537, 0.1780, 0.3677],\n",
      "         [0.2175, 0.3024, 0.2585, 0.2215],\n",
      "         [0.1796, 0.1959, 0.4130, 0.2115]],\n",
      "\n",
      "        [[0.2235, 0.3601, 0.2822, 0.1342],\n",
      "         [0.1747, 0.3582, 0.3117, 0.1554],\n",
      "         [0.2557, 0.3426, 0.2694, 0.1323],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]]])\n"
     ]
    }
   ],
   "source": [
    "score = torch.rand(batch_size, max(src_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_encoder_self_attention, -1e-9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "\n",
    "print(src_len)\n",
    "print(score)\n",
    "print(masked_score)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f727d5",
   "metadata": {},
   "source": [
    "### LayerNorm & Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a40b3",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network\n",
    "\n",
    "这个前馈神经网络只考虑每个单独位置上的建模，不同位置参数共享。\n",
    "\n",
    "#### Linear(large)\n",
    "\n",
    "#### Linear2(d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b946a",
   "metadata": {},
   "source": [
    "### LayerNorm & Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be1aea",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7727a1",
   "metadata": {},
   "source": [
    "### output word embeddding\n",
    "\n",
    "output word embeddding是另外一个序列的embedding，比如说中英文的翻译，encoder接收的是中文字符，decoder接收的英文字符，\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816bfef2",
   "metadata": {},
   "source": [
    "### Position Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6181e684",
   "metadata": {},
   "source": [
    "### Memory-base Multi-head Cross-attention\n",
    "\n",
    "Multi-head Cross-attention需要计算decoder和encoder的Multi-head的关联性。用decoder的MHA(multi head attention)作为query，用encoder的输出作为key和value来去算出上下文的表征。\n",
    "\n",
    "\n",
    "$Q @ K^{T}$的shape为[batch_size, tgt_seq_len, src_seq_len]，因此构造的mask也应该是这样的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f634f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.]],\n",
      "\n",
      "        [[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.]]])\n"
     ]
    }
   ],
   "source": [
    "valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max_tgt_seq_len-L)), 0) \\\n",
    "                                               for L in tgt_len]), 2)\n",
    "print(valid_decoder_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebe9cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_decoder_pos.transpose(1, 2))\n",
    "print(valid_cross_pos_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8b19697",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_cross_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m invalid_cross_pos_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mvalid_cross_pos\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(invalid_cross_pos_matrix)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_cross_pos' is not defined"
     ]
    }
   ],
   "source": [
    "invalid_cross_pos_matrix = 1 - valid_cross_pos\n",
    "print(invalid_cross_pos_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b76d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_cross_attention = invalid_cross_pos_matrix.to(torch.bool)\n",
    "print(mask_cross_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87497c90",
   "metadata": {},
   "source": [
    "### LayerNorm & Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77691d9",
   "metadata": {},
   "source": [
    "### Casual Multi-head Self-attention\n",
    "\n",
    "Casual Multi-head Self-attention是带有掩码的attention，使得其符合因果律。\n",
    "\n",
    "首先我们需要去构造decoder self-attention的mask。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26766428",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_matrix = [torch.tril(torch.ones(L, L)) for L in tgt_len]\n",
    "print(tri_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb43244",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_decoder_tri_matrix = [torch.unsqueeze(F.pad(torch.tril(torch.ones(L, L)), (0, max(tgt_len)-L, 0, max(tgt_len)-L)), 0) \\\n",
    "                            for L in tgt_len]\n",
    "print(valid_decoder_tri_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_decoder_tri_matrix = torch.cat(valid_decoder_tri_matrix, 0)\n",
    "print(valid_decoder_tri_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_decoder_tri_matrix = (1 - valid_decoder_tri_matrix).to(torch.bool)\n",
    "print(valid_decoder_tri_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0308ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.randn(batch_size, max(tgt_len), max(tgt_len))\n",
    "masked_score = score.masked_fill(valid_decoder_tri_matrix, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "print(tgt_len)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ad948",
   "metadata": {},
   "source": [
    "### scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debf462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, atten_mask):\n",
    "    score = torch.bmm(Q, K.transpose(-2, -1))/torch.sqrt(model_dim)\n",
    "    masked_score = score.maked_fill(atten_mask, -1e9)\n",
    "    prob = F.softmax(masked_score)\n",
    "    context = torch.bmm(prob, V)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1250d1a",
   "metadata": {},
   "source": [
    "### LayerNorm & Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776bf33",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network\n",
    "\n",
    "#### Linear(large)\n",
    "\n",
    "#### Linear2(d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2f319",
   "metadata": {},
   "source": [
    "### LayerNorm & Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51a7b4",
   "metadata": {},
   "source": [
    "## Transformer Masked Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdf0e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size=2, sequence_len=3, vocab_size = 4\n",
    "logits = torch.randn(2, 3, 4)\n",
    "label = torch.randint(0, 4, (2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f6f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = logits.transpose(1, 2)\n",
    "print(F.cross_entropy(logits, label))\n",
    "print(F.cross_entropy(logits, label, reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167be66",
   "metadata": {},
   "source": [
    "如果句子长度不一致的话，我们同样需要对预测输出做mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f231413",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_len = torch.Tensor([2, 3]).to(torch.int32)\n",
    "mask = torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(tgt_len)-L)), 0) for L in tgt_len])\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991eef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F.cross_entropy(logits, label, reduction=\"none\") * mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e6a1a",
   "metadata": {},
   "source": [
    "F.cross_entropy本身也有提供这样的功能参数ignore_index，默认值为-100。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29029f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label[0, 2] = -100\n",
    "print(F.cross_entropy(logits, label, reduction=\"none\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a29f4e",
   "metadata": {},
   "source": [
    "可以看到输出结果同样被mask掉了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02dde4c",
   "metadata": {},
   "source": [
    "## PyTorch官方API介绍\n",
    "\n",
    "Pytorch中Transformer的源码地址为:[https://github.com/pytorch/pytorch/blob/734a97a7c828d6bfe82764c9cae399c349828091/torch/nn/modules/transformer.py](https://github.com/pytorch/pytorch/blob/734a97a7c828d6bfe82764c9cae399c349828091/torch/nn/modules/transformer.py)\n",
    "\n",
    "我们可以采用torch.nn.Transformer来去调用这个transformer。\n",
    "\n",
    "```python\n",
    "Examples::\n",
    "        >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "        >>> src = torch.rand((10, 32, 512))\n",
    "        >>> tgt = torch.rand((20, 32, 512))\n",
    "        >>> out = transformer_model(src, tgt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a33871",
   "metadata": {},
   "source": [
    "### Transformer类中的__init__\n",
    "\n",
    "\n",
    "其init函数如下所示：d_model是整个transformer的特征维度，n_head是multi head self-attention头的数目。num_encoder_layers和num_decoder_layers指block的数目。dim_feedforward指全连接层中间的特征维度，multi-head attention的输出首先映射到dim_feedforward=2048这个大的特征空间，然后再把它映射回来到512这个特征空间，因为我们需要保证输出的维度是512，之后才能做残差连接。\n",
    "\n",
    "\n",
    "之后就需要在init函数中实例化一些模块，第一个要实例化的就是encoder，encoder是通过TransformerEncoder这个类去实现的，在这个类中我们需要传入encoder_layer，num_encoder_layers和encoder_norm。在TransformerEncoderLayer中需要实现multi-head self-attention的调用，还有残差连接和层归一化，还有全连接层网络来构成TransformerEncoderLayer。对于decoder也是一样，需要包括自注意力机制，交叉注意力机制和前馈神经网络。\n",
    "\n",
    "```python\n",
    "def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 custom_encoder: Optional[Any] = None, custom_decoder: Optional[Any] = None,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first, norm_first,\n",
    "                                                    **factory_kwargs)\n",
    "            encoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout,\n",
    "                                                    activation, layer_norm_eps, batch_first, norm_first,\n",
    "                                                    **factory_kwargs)\n",
    "            decoder_norm = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "```\n",
    "\n",
    "可以看出整个transformer主要由4个部件构成，TransformerEncoderLayer，TransformerEncoder，TransformerDecoderLayer，TransformerDecoder。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0589b0",
   "metadata": {},
   "source": [
    "### Transformer类中的forward\n",
    "\n",
    "在forward函数中，我们需要基于sorce sentence，target sentence，src_mask，tgt_mask来去算出最终的解码器的输出。src_mask是由于做一个mini-batch的序列的时候，每个minibitch的长度不一致，是为了之后做softmask的时候，无效位置的能量值设置为负无穷。tgt_mask是一个因果的mask。\n",
    "\n",
    "```python\n",
    "def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Take in and process masked source/target sequences.\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            src_mask: the additive mask for the src sequence (optional).\n",
    "            tgt_mask: the additive mask for the tgt sequence (optional).\n",
    "            memory_mask: the additive mask for the encoder output (optional).\n",
    "            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
    "            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
    "        Shape:\n",
    "            - src: :math:`(S, E)` for unbatched input, :math:`(S, N, E)` if `batch_first=False` or\n",
    "              `(N, S, E)` if `batch_first=True`.\n",
    "            - tgt: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
    "              `(N, T, E)` if `batch_first=True`.\n",
    "            - src_mask: :math:`(S, S)` or :math:`(N\\cdot\\text{num\\_heads}, S, S)`.\n",
    "            - tgt_mask: :math:`(T, T)` or :math:`(N\\cdot\\text{num\\_heads}, T, T)`.\n",
    "            - memory_mask: :math:`(T, S)`.\n",
    "            - src_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
    "            - tgt_key_padding_mask: :math:`(T)` for unbatched input otherwise :math:`(N, T)`.\n",
    "            - memory_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
    "            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n",
    "            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "            is provided, it will be added to the attention weight.\n",
    "            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
    "            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n",
    "            positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "            - output: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
    "              `(N, T, E)` if `batch_first=True`.\n",
    "            Note: Due to the multi-head attention architecture in the transformer model,\n",
    "            the output sequence length of a transformer is same as the input sequence\n",
    "            (i.e. target) length of the decode.\n",
    "            where S is the source sequence length, T is the target sequence length, N is the\n",
    "            batch size, E is the feature number\n",
    "        Examples:\n",
    "            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \"\"\"\n",
    "\n",
    "        is_batched = src.dim() == 3\n",
    "        if not self.batch_first and src.size(1) != tgt.size(1) and is_batched:\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "        elif self.batch_first and src.size(0) != tgt.size(0) and is_batched:\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "\n",
    "        if src.size(-1) != self.d_model or tgt.size(-1) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ded978",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TransformerEncoderLayer在其init函数中也包含一些参数，d_model整个transformer的特征维度，nhead指multi-head self-attention的头的数目。dim_feedforward指第一个全连接层的维度，第二个全连接层与d_model的维度一样，就是为了保证残差连接能够有效的进行。\n",
    "\n",
    "\n",
    "### EncoderLayer类中的init\n",
    "\n",
    "```python\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of the intermediate layer, can be a string\n",
    "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n",
    "            operations, respectivaly. Otherwise it's done after. Default: ``False`` (after).\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    Fast path:\n",
    "        forward() will use a special optimized implementation if all of the following\n",
    "        conditions are met:\n",
    "        - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor\n",
    "          argument ``requires_grad``\n",
    "        - training is disabled (using ``.eval()``)\n",
    "        - batch_first is ``True`` and the input is batched (i.e., ``src.dim() == 3``)\n",
    "        - norm_first is ``False`` (this restriction may be loosened in the future)\n",
    "        - activation is one of: ``\"relu\"``, ``\"gelu\"``, ``torch.functional.relu``, or ``torch.functional.gelu``\n",
    "        - at most one of ``src_mask`` and ``src_key_padding_mask`` is passed\n",
    "        - if src is a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_, neither ``src_mask``\n",
    "          nor ``src_key_padding_mask`` is passed\n",
    "        - the two ``LayerNorm`` instances have a consistent ``eps`` value (this will naturally be the case\n",
    "          unless the caller has manually modified one without modifying the other)\n",
    "        If the optimized implementation is in use, a\n",
    "        `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be\n",
    "        passed for ``src`` to represent padding more efficiently than using a padding\n",
    "        mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ will be\n",
    "        returned, and an additional speedup proportional to the fraction of the input that\n",
    "        is padding can be expected.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            activation = _get_activation_fn(activation)\n",
    "\n",
    "        # We can't test self.activation in forward() in TorchScript,\n",
    "        # so stash some information about it instead.\n",
    "        if activation is F.relu:\n",
    "            self.activation_relu_or_gelu = 1\n",
    "        elif activation is F.gelu:\n",
    "            self.activation_relu_or_gelu = 2\n",
    "        else:\n",
    "            self.activation_relu_or_gelu = 0\n",
    "        self.activation = activation\n",
    "```\n",
    "\n",
    "其中最重要的就是MultiheadAttention。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653db45d",
   "metadata": {},
   "source": [
    "### EncoderLayer类中的forward\n",
    "\n",
    "```python\n",
    "def forward(self, src: Tensor, src_mask: Optional[Tensor] = None,\n",
    "                src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "\n",
    "        if (src.dim() == 3 and not self.norm_first and not self.training and\n",
    "            self.self_attn.batch_first and\n",
    "            self.self_attn._qkv_same_embed_dim and self.activation_relu_or_gelu and\n",
    "            self.norm1.eps == self.norm2.eps and\n",
    "            ((src_mask is None and src_key_padding_mask is None)\n",
    "             if src.is_nested\n",
    "             else (src_mask is None or src_key_padding_mask is None))):\n",
    "            tensor_args = (\n",
    "                src,\n",
    "                self.self_attn.in_proj_weight,\n",
    "                self.self_attn.in_proj_bias,\n",
    "                self.self_attn.out_proj.weight,\n",
    "                self.self_attn.out_proj.bias,\n",
    "                self.norm1.weight,\n",
    "                self.norm1.bias,\n",
    "                self.norm2.weight,\n",
    "                self.norm2.bias,\n",
    "                self.linear1.weight,\n",
    "                self.linear1.bias,\n",
    "                self.linear2.weight,\n",
    "                self.linear2.bias,\n",
    "            )\n",
    "            if (not torch.overrides.has_torch_function(tensor_args) and\n",
    "                    # We have to use a list comprehension here because TorchScript\n",
    "                    # doesn't support generator expressions.\n",
    "                    all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]) and\n",
    "                    (not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]))):\n",
    "                return torch._transformer_encoder_layer_fwd(\n",
    "                    src,\n",
    "                    self.self_attn.embed_dim,\n",
    "                    self.self_attn.num_heads,\n",
    "                    self.self_attn.in_proj_weight,\n",
    "                    self.self_attn.in_proj_bias,\n",
    "                    self.self_attn.out_proj.weight,\n",
    "                    self.self_attn.out_proj.bias,\n",
    "                    self.activation_relu_or_gelu == 2,\n",
    "                    False,  # norm_first, currently not supported\n",
    "                    self.norm1.eps,\n",
    "                    self.norm1.weight,\n",
    "                    self.norm1.bias,\n",
    "                    self.norm2.weight,\n",
    "                    self.norm2.bias,\n",
    "                    self.linear1.weight,\n",
    "                    self.linear1.bias,\n",
    "                    self.linear2.weight,\n",
    "                    self.linear2.bias,\n",
    "                    src_mask if src_mask is not None else src_key_padding_mask,\n",
    "                )\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)\n",
    "            x = x + self._ff_block(self.norm2(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))\n",
    "            x = self.norm2(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "```\n",
    "\n",
    "可以看出，在encoder中做的是self-attention，q，k，v都是他自己，之后再接残差连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d040ea8",
   "metadata": {},
   "source": [
    "### Encoder中的init\n",
    "\n",
    "\n",
    "```python\n",
    "class TransformerEncoder(Module):\n",
    "    r\"\"\"TransformerEncoder is a stack of N encoder layers\n",
    "    Args:\n",
    "        encoder_layer: an instance of the TransformerEncoderLayer() class (required).\n",
    "        num_layers: the number of sub-encoder-layers in the encoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "        enable_nested_tensor: if True, input will automatically convert to nested tensor\n",
    "            (and convert back on output). This will improve the overall performance of\n",
    "            TransformerEncoder when padding rate is high. Default: ``True`` (enabled).\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = transformer_encoder(src)\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None, enable_nested_tensor=True):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "        self.enable_nested_tensor = enable_nested_tensor\n",
    "```\n",
    "\n",
    "encoder_layer是实力化之后的encoder layer，还有需要多少个bolck，也就是num_layers的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee26af",
   "metadata": {},
   "source": [
    "### Encoder中的forward\n",
    "\n",
    "```python\n",
    "def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layers in turn.\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        convert_to_nested = False\n",
    "        first_layer = self.layers[0]\n",
    "        if isinstance(first_layer, torch.nn.TransformerEncoderLayer):\n",
    "            if (not first_layer.norm_first and not first_layer.training and\n",
    "                    first_layer.self_attn.batch_first and\n",
    "                    first_layer.self_attn._qkv_same_embed_dim and first_layer.activation_relu_or_gelu and\n",
    "                    first_layer.norm1.eps == first_layer.norm2.eps and\n",
    "                    src.dim() == 3 and self.enable_nested_tensor) :\n",
    "                if src_key_padding_mask is not None and not output.is_nested and mask is None:\n",
    "                    tensor_args = (\n",
    "                        src,\n",
    "                        first_layer.self_attn.in_proj_weight,\n",
    "                        first_layer.self_attn.in_proj_bias,\n",
    "                        first_layer.self_attn.out_proj.weight,\n",
    "                        first_layer.self_attn.out_proj.bias,\n",
    "                        first_layer.norm1.weight,\n",
    "                        first_layer.norm1.bias,\n",
    "                        first_layer.norm2.weight,\n",
    "                        first_layer.norm2.bias,\n",
    "                        first_layer.linear1.weight,\n",
    "                        first_layer.linear1.bias,\n",
    "                        first_layer.linear2.weight,\n",
    "                        first_layer.linear2.bias,\n",
    "                    )\n",
    "                    if not torch.overrides.has_torch_function(tensor_args):\n",
    "                        if output.is_cuda or 'cpu' in str(output.device):\n",
    "                            convert_to_nested = True\n",
    "                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n",
    "\n",
    "        for mod in self.layers:\n",
    "            if convert_to_nested:\n",
    "                output = mod(output, src_mask=mask)\n",
    "            else:\n",
    "                output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if convert_to_nested:\n",
    "            output = output.to_padded_tensor(0.)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab312c0e",
   "metadata": {},
   "source": [
    "### DecoderLayer中的init\n",
    "\n",
    "TransformerDecoderLayer中init函数的第一个参数依旧是d_model，表示为整个transformer的特征维度nhead同样表示multi-head attention中有多少个头。但是这里需要实例化两个MultiheadAttention，第一个是自注意力机制，第二个是交叉注意力机制。\n",
    "\n",
    "```python\n",
    "class TransformerDecoderLayer(Module):\n",
    "    r\"\"\"TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of the intermediate layer, can be a string\n",
    "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "        norm_first: if ``True``, layer norm is done prior to self attention, multihead\n",
    "            attention and feedforward operations, respectivaly. Otherwise it's done after.\n",
    "            Default: ``False`` (after).\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = decoder_layer(tgt, memory)\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> memory = torch.rand(32, 10, 512)\n",
    "        >>> tgt = torch.rand(32, 20, 512)\n",
    "        >>> out = decoder_layer(tgt, memory)\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                                 **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "        self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.norm3 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.dropout3 = Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = _get_activation_fn(activation)\n",
    "        else:\n",
    "            self.activation = activation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e7949",
   "metadata": {},
   "source": [
    "\n",
    "### DecoderLayer中的forward\n",
    "\n",
    "```python\n",
    "def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder layer (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "\n",
    "        x = tgt\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)\n",
    "            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)\n",
    "            x = x + self._ff_block(self.norm3(x))\n",
    "        else:\n",
    "            x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))\n",
    "            x = self.norm2(x + self._mha_block(x, memory, memory_mask, memory_key_padding_mask))\n",
    "            x = self.norm3(x + self._ff_block(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: Tensor,\n",
    "                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # multihead attention block\n",
    "    def _mha_block(self, x: Tensor, mem: Tensor,\n",
    "                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.multihead_attn(x, mem, mem,\n",
    "                                attn_mask=attn_mask,\n",
    "                                key_padding_mask=key_padding_mask,\n",
    "                                need_weights=False)[0]\n",
    "        return self.dropout2(x)\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: Tensor) -> Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout3(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7960b",
   "metadata": {},
   "source": [
    "### Decoder中的init\n",
    "\n",
    "在TransformerDecoder这个类中，首先会有个init函数，\n",
    "\n",
    "```python\n",
    "class TransformerDecoder(Module):\n",
    "    r\"\"\"TransformerDecoder is a stack of N decoder layers\n",
    "    Args:\n",
    "        decoder_layer: an instance of the TransformerDecoderLayer() class (required).\n",
    "        num_layers: the number of sub-decoder-layers in the decoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = transformer_decoder(tgt, memory)\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "```\n",
    "\n",
    "decoder_layer是DecoderLayer的实例，num_layers指总共有多少个block。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b676bb",
   "metadata": {},
   "source": [
    "### Decoder中的forward\n",
    "\n",
    "```python\n",
    "def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210c2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
