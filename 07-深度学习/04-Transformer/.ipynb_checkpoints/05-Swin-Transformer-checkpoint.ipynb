{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c40d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ea6e27",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01d2e5f9",
   "metadata": {},
   "source": [
    "## Patch Embedding\n",
    "\n",
    "将一幅图片转化成一个`patch embedding`。\n",
    "\n",
    "主要有`naive`的实现版本和`conv2d`的实现版本。\n",
    "\n",
    "首先是将图像切分成一个个`patch`，与`vit`一样。每个`patch`就像`NLP`中的每个`token`一样。在论文中，`patch`的大小取$4 \\times 4$, 每个`patch`中都存在三个通道，因此每个`patch`的像素点都是$4 \\times 4 \\times 3 = 48$个像素点。这是原始像素点，想要得到`patch embedding`的话，我们还需要将其通过一个线性层映射到长度为$C$的向量上。\n",
    "\n",
    "<img src=\"../../images/07-swintransformer.jpg\" width=\"80%\">\n",
    "\n",
    "\n",
    "每一个`block`是不会对像素做改变的。在运用`patch merging`的时候才会使得像素发生改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a8aecd",
   "metadata": {},
   "source": [
    "1. 方法一：基于unfold API来模仿卷积思路来实现对图像分块，设置kernel size = patch size。得到格式为[bs, num_patch, patch_size]的向量。将张量与[bs, patch_size, model_dim_C]的权重矩阵相乘，得到[bs, num_patch, model_dim_C]的patch embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d176eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895809f",
   "metadata": {},
   "source": [
    "输入图像大小都是按照卷积操作的图像大小给定的，维度为:`[bs * channel * h * w]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2emb_naive(image, patch_size, weight):\n",
    "    patch = F.unfold(image, kernel_size=(patch_size, patch_size),\n",
    "                    stride=(patch_size, patch_size)).transpose(-1, -2)  # [bs, num_patch, patch_depth]\n",
    "    patch_embeding = patch @ weight\n",
    "    return patch_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ccfc6",
   "metadata": {},
   "source": [
    "2. 方法二：就是直接通过卷积操作来实现，省去了与权重相乘的过程，卷积输出的通道数就是编码之后的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a577c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2emb_conv(image, kernel, stride):\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride) # [bs, oc, oh, ow]\n",
    "    bs, oc, oh, ow = conv_output.shape\n",
    "    patch_embedding = conv_output.reshape((bs, oc, oh * ow)).transpose(-1, -2)\n",
    "    return patch_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e608ec",
   "metadata": {},
   "source": [
    "## SwinTransformer Block\n",
    "\n",
    "`SwinTransformer`中的`swin`是`shift`和`window`的结合。`SwinTransformer Block`主要包括两部分，一个是`Window Multi-head Self-attention`和`Shifted Windows Multi-head Self-attention`。\n",
    "\n",
    "把`patch`做成在不同的`window`内，在`window`内做`self-attention`。每一个`window`内的`self-attention`复杂度和`window`内的`patch`数目成平方关系。而`window`数目与图片大小成线性关系，因此算法整体的复杂度也是会与图片大小成线性关系。`window`与`window`之间的连接是通过`shift window`来去实现的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e20775",
   "metadata": {},
   "source": [
    "### Window Multi-head Self-attention\n",
    "\n",
    "&emsp;&emsp;这里就是构建多头的注意力机制，并计算其复杂度。\n",
    "\n",
    "1. 将输入经过`mlp`映射到$q，k，v$。输入$x$的维度为$L \\times C$，其中$L$为序列长度，$C$为特征大小。映射矩阵的维度为$C \\times C$，所以这两个矩阵相乘的复杂度为$LC^{2}$。有三个映射，所以其复杂度为$3LC^{2}$。将$q，k，v$拆分乘多头的形式，也就是维度$C$拆分成$\\frac{C}{n}$, 但是这里的多头计算互不影响，也就是头与头之间并不会做`attention`的计算，因此可以与`batch size`的那一维度进行统一的看待。\n",
    "2. 之后我们就需要计算`attention`计算过程的复杂度，首先是$q k^{T}$, 其中$q$的矩阵维度为$L \\times C$, $k^{T}$的矩阵维度为$C \\times L$, 因此这两个矩阵相乘的复杂度为$L^{2}C$。\n",
    "3. 之后将计算得到的概率与$v$相乘，也就是$L \\times L$的矩阵与$L \\times C$的矩阵相乘，同样计算其复杂度为$L^{2}C$。\n",
    "4. 得到最终的`[bs, L, C]`的数据与`mlp`相乘，也就是$L \\times C$的矩阵与$C \\times C$的矩阵相乘，复杂度为$LC^{2}$。\n",
    "\n",
    "&emsp;&emsp;此时，我们可以得出总体的复杂度为:\n",
    "\n",
    "$$\n",
    "4LC^{2} + 2L^{2}C\n",
    "$$\n",
    "\n",
    "可以看出，重头是在做`attention`部分，复杂度与序列长度成平方关系。\n",
    "\n",
    "在计算$q k^{T}$时，我们需要考虑掩码，让两两无效的位置之间能量为负无穷，经过`softmax`之后，概率就会变成`0`。掩码是在`shift window MHSA`中会用到，在`window MHSA`中不会用到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9950f8",
   "metadata": {},
   "source": [
    "在涉及模型，参数的定义的时候，我们需要写成`class`的形式，并且`module`需要继承自`nn.Module`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_head):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        \n",
    "        self.proj_linear_layer = nn.Linear(model_dim, 3*model_dim)\n",
    "        self.final_linear_layer = nn.Linear(model_dim, model_dim)\n",
    "    \n",
    "    def forward(self, input, additive_mask=None):\n",
    "        bs, seqlen, model_dim = input.shape\n",
    "        num_head = self.num_head\n",
    "        \n",
    "        head_dim = model_dim // self.num_head\n",
    "        proj_output = self.proj_linear_layer(input) # [bs, seqlen, 3 * model_dim]\n",
    "        q, k, v = proj_output.chunk(3, dim=-1)  # [bs, seqlen, model_dim]\n",
    "        \n",
    "        q = q.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)\n",
    "        q = q.reshape(bs * num_head, seqlen, head_dim)\n",
    "        \n",
    "        k = k.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)\n",
    "        k = k.reshape(bs * num_head, seqlen, head_dim)\n",
    "        \n",
    "        v = v.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)\n",
    "        v = v.reshape(bs * num_head, seqlen, head_dim)\n",
    "        \n",
    "        if additive_mask is None:\n",
    "            attn_prob = F.softmax(torch.bmm(q, k.transpose(-2, -1))/ math.sqrt(head_dim), dim=-1)\n",
    "        else:\n",
    "            additive_mask = additive_mask.tile((num_head, 1, 1))\n",
    "            attn_prob = F.softmax(torch.bmm(q, k.transpose(-2, -1))/ math.sqrt(head_dim) + additive_mask, dim=-1)\n",
    "            \n",
    "        output = torch.bmm(attn_prob, v)  # [bs * num_head, seqlen, head_dim]\n",
    "        output = output.reshape(bs, num_head, seqlen, head_dim).transpose(1, 2) # [bs, seqlen, num_head, head_dim]\n",
    "        output = output.reshape(bs, seqlen, model_dim)\n",
    "        \n",
    "        output = self.final_linear_layer(output)\n",
    "        return attn_prob, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263eb794",
   "metadata": {},
   "source": [
    "之后，我们就需要去构建带window的MHSA，并计算其复杂度\n",
    "1. 之前像素做成的patch进一步划分成一个个更大的window。首先需要将三维的patch embedding转换成图片的形式，也就是将[bs, num_patch, model_dim]转成[bs, height, width]的形式。高度乘以宽度需要是patch的总数。采用unfold这个API将patch划分成window。\n",
    "\n",
    "2. 划分成window之后，我们就可以在window内部去划分MHSA。window数目可以与bs统一对待，因为window数据之间并不做attention。\n",
    "\n",
    "假设窗的边长是$W$，那么窗内元素的数目就是$W^{2}$，那么计算窗内的总体复杂度是$4W^{2}C^{2} + 2W^{4}C$。假设patch的总数目是$L$, 那么窗的数目就是$L/W^{2}$。因此W-MHSA的复杂度为$4LC^{2} + 2LW^{2}C$。\n",
    "\n",
    "复杂度对比：\n",
    "\n",
    "`MHSA`：$4LC^{2} + 2L^{2}C$。\n",
    "\n",
    "`W-MHSA`：$4LC^{2} + 2LW^{2}C$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_multi_head_self_attention(patch_embedding, mhsa, window_size=4, num_head=2):\n",
    "    num_patch_in_window = window_size * window_size\n",
    "    bs, num_patch, patch_depth = patch_embedding.shape\n",
    "    image_height = image_width = int(math.sqrt(num_patch))\n",
    "    \n",
    "    patch_embedding = patch_embedding.transpose(-1, -2)\n",
    "    patch = patch_embedding.reshape(bs, patch_depth, image_height, image_width)\n",
    "    window = F.unfold(patch, kernel_size=(window_size, window_size), \n",
    "                      stride=(window_size, window_size)).transpose(-1, -2)\n",
    "    \n",
    "    bs, num_window, patch_depth_times_num_patch_in_window = window.shape\n",
    "    window = window.reshape(bs * num_window, patch_depth, num_patch_in_window).transpose(-2, -1)\n",
    "    \n",
    "    attn_prob, output = mhsa(window)  # [bs * num_window, num_patch_in_window, patch_depth]\n",
    "    \n",
    "    output = output.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7065cdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e26618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5924a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83a8f50d",
   "metadata": {},
   "source": [
    "### Shifted Windows Multi-head Self-attention\n",
    "\n",
    "1. 首先将上一步W-MHSA的结果转换成图片的形式，假设已经做了新的window的划分，这一步叫做shift window。为了保持window数目不变，从而有高效计算，需要将图片的patch往左和往上各自滑动半个窗口大小的步长，保持patch所属window类别不变。之后再将图片patch还原成window的数据格式。\n",
    "\n",
    "2. 由于cycle shift后，每个window虽然形状不规则，但部分window中存在原本不属于同一个窗口的patch，所以需要生成mask。\n",
    "\n",
    "3. 如何生成`mask`？构建`mask`时，首先需要去构建一个`shift-window`的`patch`所属的`window`类别矩阵，之后对该矩阵往上，往左滑动半个窗口大小的步长，之后通过`unfold`操作得到`[bs, num_window, num_patch_in_window]`形状的类别矩阵。之后将其扩充维度到`[bs, num_window, num_patch_in_window, 1]`这个维度。将该矩阵与其转置矩阵进行作差，得到同类关系矩阵（为0的位置上的patch属于同类，否则属于不同类）。对同类关系矩阵中非零位置用负无穷去填充，对于0的位置用0去填充，这样就构建好了MHSA所需的mask。此mask的形状为[bs, num_window, num_patch_in_window, num_patch_in_window]。\n",
    "\n",
    "4. 将window转换成三维的格式，[bs*num_window, num_patch_in_window, patch_depth]。\n",
    "5. 将三维格式的特征连同mask一起送入HSA中计算得到注意力的输出。\n",
    "6. 将注意力的输出转化成图片patch的格式，[bs, num_window, num_patch_in_window, patch_depth]。\n",
    "7. 未了恢复位置，需要将图片的patch往右和往下各自滑动半个窗口大小的步长，至此，SW-MHSA计算完毕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ae554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数，将transformer block的结果转换成图片的格式。\n",
    "def window2image(msa_output):\n",
    "    bs, num_window, num_patch_in_window, patch_depth = msa_output.shape\n",
    "    window_size = int(math.sqrt(num_patch_in_window))\n",
    "    \n",
    "    image_height = int(math.sqrt(num_window)) * window_size\n",
    "    image_width = image_height\n",
    "    \n",
    "    msa_output = msa_output.reshape(bs, int(math.sqrt(num_window)),\n",
    "                                        int(math.sqrt(num_window)),\n",
    "                                        window_size,\n",
    "                                        window_size,\n",
    "                                        patch_depth)\n",
    "    \n",
    "    msa_output = msa_output.transpose(2, 3) \n",
    "    image = msa_output.reshape(bs, image_height*image_width, patch_depth)\n",
    "    \n",
    "    image = image.transpose(-1, -2).reshape(bs, patch_depth, image_height, image_width) # 跟卷积格式一致\n",
    "    return image\n",
    "\n",
    "# 辅助函数，高效计算swmsa\n",
    "def shift_window(w_msa_output, window_size, shift_size, generate_mask=False):\n",
    "    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "    \n",
    "    w_msa_output = window2image(w_msa_output)  # [bs, depth, h, w]\n",
    "    bs, patch_depth, image_height, image_width = w_msa_output.shape\n",
    "    \n",
    "    rolled_w_msa_input = torch.roll(w_msa_output, shifts=(shift_size, shift_size), dims=(2, 3))\n",
    "    \n",
    "    shift_w_msa_input = rolled_w_msa_input.reshape(bs, patch_depth,\n",
    "                                                   int(math.sqrt(num_window)),\n",
    "                                                   window_size,\n",
    "                                                   int(math.sqrt(num_window)),\n",
    "                                                   window_size\n",
    "                                                  )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "def shift_window_multi_head_self_attention(w_msa_output, mhsa, window_size=4, num_head=2):\n",
    "    \"\"\"\n",
    "    w_msa_output: 上一步shift window的输出。\n",
    "    mhsa: 新的实例化的mhsa的对象。\n",
    "    \"\"\"\n",
    "    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "    \n",
    "    shifted_w_msa_input, additive_mask = shift_window(w_msa_output, window_size, \n",
    "                                                      shift_size=-window_size//2, generate_mask=True)\n",
    "    shifted_w_msa_input = shifted_w_msa_input.reshape(bs*num_window, num_patch_in_window, patch_depth)\n",
    "    \n",
    "    attn_prob, output = mhsa(shifted_w_msa_input, additive_mask=additive_mask)\n",
    "    output = output.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    \n",
    "    output, _ = shift_window(output, window_size, shift_size=window_size//2, generate_mask=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba669c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b1787a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ba22f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23c81dc9",
   "metadata": {},
   "source": [
    "## Patch Merging\n",
    "\n",
    "Patch Merging主要实现的是像素的降低，将周围的patch浓缩成一个patch。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8179b29",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "最后就是将swin transformer最后转换成一个分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fed72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
