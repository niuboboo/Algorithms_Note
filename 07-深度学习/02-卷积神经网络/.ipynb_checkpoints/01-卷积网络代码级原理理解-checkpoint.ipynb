{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ecd6cd",
   "metadata": {},
   "source": [
    "## 重新考察全连接层\n",
    "\n",
    "对于全连接网络，输入和输出都是一个向量，每个权重和每个输入都会有累乘的关系。\n",
    "\n",
    "如果，将输入和输出变形为矩阵(宽度，高度)，就是卷积操作。对应的，我们可以将权重变形为$4-D$的张量, 从$(h, w)$到$(h^{\\prime}, w^{\\prime})$。\n",
    "\n",
    "那么我们的输出可以表示为$h_{i, j}$:\n",
    "\n",
    "$$\n",
    "h_{i, j}=\\sum_{k l} w_{i, j, k, l} x_{k, l}=\\sum_{a b} v_{i, j, a, b} x_{i+a, j+b}\n",
    "$$\n",
    "\n",
    "其中$x_{k, l}$是我们的输入, 然后对$k, l$求和。之后对下标做一些变换: $v_{i,j,a,b} = w_{i, j, i+a, j+b}$, 来引出卷积的做法, 其中$v$是$w$的重新索引:。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a57ec",
   "metadata": {},
   "source": [
    "### 平移不变性\n",
    "\n",
    "&emsp;&emsp;假设$x$的位置的变换，这将导致权重$v$值的变换。因此想要实现平移不变性，$v$不应该依赖于$(i, j)$，解决方法是$v_{i,j,a,b}=v_{a, b}$:\n",
    "\n",
    "$$\n",
    "h_{i,j} = \\sum_{a,b} v_{a,b} x_{i+a, j+b}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;这就是2维卷积。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e593e9f",
   "metadata": {},
   "source": [
    "### 局部性\n",
    "\n",
    "&emsp;&emsp;当评估$h_{i,j}$时，我们不应该用远离$x_{i,j}$的参数。解决方案是：当$|a|, |b| > \\Delta$时，使得$v_{a, b}=0$:\n",
    "\n",
    "$$\n",
    "h_{i, j}=\\sum_{a=-\\Delta}^{\\Delta} \\sum_{b=-\\Delta}^{\\Delta} v_{a, b} x_{i+a, j+b}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b36dab",
   "metadata": {},
   "source": [
    "### 卷积计算示例\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ccbd1c",
   "metadata": {},
   "source": [
    "<img src=\"../../images/07-juanji.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874e5fbc",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上图中$19$的计算方式如下: $0 \\times 0 + 1 \\times 1 + 3 \\times 2 + 4 \\times 3 = 19$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f03bb0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;假设我们的\n",
    "\n",
    "1. 输入为$X$, 它的维度为$n_{h} \\times n_{w}$；\n",
    "2. 核为$W$, 它的纬度为$k_{h} \\times k_{w}$,；\n",
    "3. 偏差$b \\in \\mathbf{R}$。\n",
    "4. 输出为$Y$, 纬度为: $(n_{h} - k_{h} + 1) \\times (n_{w} - k_{w} + 1)$。\n",
    "\n",
    "&emsp;&emsp;其中的$W$和$b$是可学习参数。\n",
    "\n",
    "&emsp;&emsp;卷积层是将输入和核矩阵进行交叉相关计算，再加上偏移后得到输出。\n",
    "\n",
    "$1 \\times 1$卷积等价于全连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8be15",
   "metadata": {},
   "source": [
    "## 矩阵运算实现二维卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876d092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_feature_map = torch.randn(5, 5)\n",
    "kernel = torch.randn(3, 3)\n",
    "\n",
    "def matrix_multiplication_for_conv2d(input_feature_map, kernel, stride=1, padding=0):\n",
    "    if padding > 0:\n",
    "        input_feature_map = F.pad(input_feature_map, (padding, padding, padding, padding))\n",
    "    \n",
    "    input_h, input_w = input_feature_map.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # 卷积输出高度\n",
    "    output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # 卷积输出宽度\n",
    "    output = torch.zeros(output_h, output_w)\n",
    "    \n",
    "    for i in range(0, input_h - kernel_h + 1, stride):  # 对高度进行遍历\n",
    "        for j in range(0, input_w - kernel_w + 1, stride):  # 对宽度进行遍历\n",
    "            region = input_feature_map[i:i+kernel_h, j:j+kernel_w]  # 取出被核滑动到的区域\n",
    "            output[int(i/stride), int(j/stride)] = torch.sum(region * kernel)  # 点乘，并赋值给输出位置的元素\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd649a",
   "metadata": {},
   "source": [
    "调用PyTorch的API来验证一下书写结果是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "effc0872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5662,  1.5358,  0.1674],\n",
      "        [-1.2935, -3.0918, -0.7134],\n",
      "        [-2.6432, -2.5372, -2.9910]])\n",
      "tensor([[-2.5662,  1.5358,  0.1674],\n",
      "        [-1.2935, -3.0918, -0.7134],\n",
      "        [-2.6432, -2.5372, -2.9910]])\n"
     ]
    }
   ],
   "source": [
    "mat_mul_conv_output = matrix_multiplication_for_conv2d(input_feature_map, kernel)\n",
    "print(mat_mul_conv_output)\n",
    "\n",
    "pytorch_api_conv_output = F.conv2d(input_feature_map.reshape((1, 1, input_feature_map.shape[0], \n",
    "                                                              input_feature_map.shape[1])),\n",
    "                                  kernel.reshape((1, 1, kernel.shape[0], kernel.shape[1])))\n",
    "print(pytorch_api_conv_output.squeeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25152f2b",
   "metadata": {},
   "source": [
    "### 填充和步幅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072af033",
   "metadata": {},
   "source": [
    "&emsp;&emsp;填充和步幅是控制卷积层输出大小的两个参数。输出图片的大小为$(n_{h} - k_{h} + 1) \\times (n_{w} - k_{w} + 1)$, 因此卷积之后输出图片会越变越小，而深度学习考虑的是如何用更深的网络来训练模型，所以想要更深的网络的话，我们需要做填充：**在输入周围添加额外的行和列**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017e804",
   "metadata": {},
   "source": [
    "&emsp;&emsp;假设填充的为$p_{h}$行和$p_{w}$列，那么此时输出图片的大小为：$(n_{h} - k_{h} + p_{h} + 1) \\times (n_{w} - k_{w} + p_{w} + 1)$。\n",
    "\n",
    "&emsp;&emsp;通常来说取，$p_{h}=k_{h} - 1$, $p_{w} = k_{w} - 1$，此时的输入输出维度是相等的：\n",
    "\n",
    "- 当$k_{h}$为奇数：在上下两侧填充$p_{h} / 2$。\n",
    "- 当$k_{h}$为偶数：在上侧填充$「p_{h} / 2$，下侧填充$p_{h} / 2」$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562803c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对于填充来说，是希望可以控制图片不要变小地太快，但是对于一张很大的图片来说，我们希望它能够快速变小的话，就可以通过步幅来进行调整。不然的话，网络可能会太深，需要大量的计算得出较小的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197a6a82",
   "metadata": {},
   "source": [
    "&emsp;&emsp;给定高度$s_{h}$和宽度$s_{w}$的步幅，输出形状是：$((n_{h} - k_{h} + p_{h}) / s_{h} + 1) \\times ((n_{w} - k_{w} + p_{w}) / s_{w} + 1)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46191281",
   "metadata": {},
   "source": [
    "再来验证一下padding操作之后，是否会是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cea364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.5391,  1.5011, -3.9646, -0.9686, -1.4881],\n",
      "        [-4.0826, -2.5662,  1.5358,  0.1674,  1.4360],\n",
      "        [-1.4680, -1.2935, -3.0918, -0.7134,  2.4125],\n",
      "        [ 0.6498, -2.6432, -2.5372, -2.9910, -0.1529],\n",
      "        [-1.4495, -0.1266, -0.3789,  1.6911,  2.3553]])\n",
      "tensor([[ 3.5391,  1.5011, -3.9646, -0.9686, -1.4881],\n",
      "        [-4.0826, -2.5662,  1.5358,  0.1674,  1.4360],\n",
      "        [-1.4680, -1.2935, -3.0918, -0.7134,  2.4125],\n",
      "        [ 0.6498, -2.6432, -2.5372, -2.9910, -0.1529],\n",
      "        [-1.4495, -0.1266, -0.3789,  1.6911,  2.3553]])\n"
     ]
    }
   ],
   "source": [
    "mat_mul_conv_output = matrix_multiplication_for_conv2d(input_feature_map, kernel, padding=1)\n",
    "print(mat_mul_conv_output)\n",
    "\n",
    "pytorch_api_conv_output = F.conv2d(input_feature_map.reshape((1, 1, input_feature_map.shape[0], \n",
    "                                                              input_feature_map.shape[1])),\n",
    "                                  kernel.reshape((1, 1, kernel.shape[0], kernel.shape[1])),\n",
    "                                  padding=1)\n",
    "print(pytorch_api_conv_output.squeeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae86549",
   "metadata": {},
   "source": [
    "再来验证一下stride操作之后，是否会是一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16f8445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.5391, -3.9646, -1.4881],\n",
      "        [-1.4680, -3.0918,  2.4125],\n",
      "        [-1.4495, -0.3789,  2.3553]])\n",
      "tensor([[ 3.5391, -3.9646, -1.4881],\n",
      "        [-1.4680, -3.0918,  2.4125],\n",
      "        [-1.4495, -0.3789,  2.3553]])\n"
     ]
    }
   ],
   "source": [
    "mat_mul_conv_output = matrix_multiplication_for_conv2d(input_feature_map, kernel, stride=2, padding=1)\n",
    "print(mat_mul_conv_output)\n",
    "\n",
    "pytorch_api_conv_output = F.conv2d(input_feature_map.reshape((1, 1, input_feature_map.shape[0], \n",
    "                                                              input_feature_map.shape[1])),\n",
    "                                  kernel.reshape((1, 1, kernel.shape[0], kernel.shape[1])),\n",
    "                                  stride=2,\n",
    "                                  padding=1)\n",
    "print(pytorch_api_conv_output.squeeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f405d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce06b177",
   "metadata": {},
   "source": [
    "## 滑动矩阵拉直实现二维卷积\n",
    "\n",
    "我们可以将卷积过程中的矩阵运算拉平，然后用矩阵运算得到最终的结果。也就是将滑动到的卷积的区域拉成一个行向量，再将`kernel`拉成一个列向量，对其进行矩阵相乘得到最终的卷积结果。也就是将滑动相乘的计算过程，变成一个滑动的向量和`kernel`进行矩阵运算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580bdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_feature_map = torch.randn(5, 5)\n",
    "kernel = torch.randn(3, 3)\n",
    "\n",
    "def matrix_multiplication_for_conv2d_flatten(input_feature_map, kernel, stride=1, padding=0):\n",
    "    if padding > 0:\n",
    "        input_feature_map = F.pad(input_feature_map, (padding, padding, padding, padding))\n",
    "    \n",
    "    input_h, input_w = input_feature_map.shape\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # 卷积输出高度\n",
    "    output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # 卷积输出宽度\n",
    "    output = torch.zeros(output_h, output_w)\n",
    "    \n",
    "    # region_matrix存储所有拉平后的滑动过程中的特征区域\n",
    "    region_matrix = torch.zeros(output.numel(), kernel.numel())\n",
    "    kernel_matrix = kernel.reshape((kernel.numel(), 1))  # kernel的列向量形式\n",
    "    row_index = 0\n",
    "    for i in range(0, input_h - kernel_h + 1, stride):  # 对高度进行遍历\n",
    "        for j in range(0, input_w - kernel_w + 1, stride):  # 对宽度进行遍历\n",
    "            region = input_feature_map[i:i+kernel_h, j:j+kernel_w]  # 取出被核滑动到的区域\n",
    "            region_vector = torch.flatten(region)\n",
    "            region_matrix[row_index] = region_vector\n",
    "            row_index += 1\n",
    "    output_matrix = region_matrix @ kernel_matrix\n",
    "    output = output_matrix.reshape((output_h, output_w))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025c39d",
   "metadata": {},
   "source": [
    "之后我们验证一下两者运算的结果是否相同:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c7de823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0408,  4.8012,  0.4941],\n",
      "        [-1.9264, -0.7051,  0.8267],\n",
      "        [ 0.2934, -1.3267, -0.7641]])\n",
      "tensor([[-2.0408,  4.8012,  0.4941],\n",
      "        [-1.9264, -0.7051,  0.8267],\n",
      "        [ 0.2934, -1.3267, -0.7641]])\n"
     ]
    }
   ],
   "source": [
    "mat_mul_conv_output = matrix_multiplication_for_conv2d_flatten(input_feature_map, kernel, stride=2, padding=1)\n",
    "print(mat_mul_conv_output)\n",
    "\n",
    "pytorch_api_conv_output = F.conv2d(input_feature_map.reshape((1, 1, input_feature_map.shape[0], \n",
    "                                                              input_feature_map.shape[1])),\n",
    "                                  kernel.reshape((1, 1, kernel.shape[0], kernel.shape[1])),\n",
    "                                  stride=2,\n",
    "                                  padding=1)\n",
    "print(pytorch_api_conv_output.squeeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6026b3",
   "metadata": {},
   "source": [
    "## 添加batch size维度和channel 维度的卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac235acb",
   "metadata": {},
   "source": [
    "### 卷积层里的多输入多输出通道\n",
    "\n",
    "&emsp;&emsp;通道数通常是大家通常会仔细去设计的超参数:\n",
    "\n",
    "- **多个输入通道**:\n",
    "\n",
    "&emsp;&emsp;图像通常是多个输入通道的，假设我们的\n",
    "\n",
    "1. 输入为$X$, 它的维度为: $c_{i} \\times n_{h} \\times n_{w}$；\n",
    "2. 核为$W$, 它的纬度为: $c_{i} \\times k_{h} \\times k_{w}$；\n",
    "3. 偏差$b \\in \\mathbf{R}$。\n",
    "4. 输出为$Y$, 纬度为: $m_{h} \\times m_{w}$。\n",
    "\n",
    "&emsp;&emsp;也就是说，输出是一个单通道的。每一个通道的对应元素相加，得到最终的单通道的输出。\n",
    "\n",
    "- **多个输出通道**:\n",
    "\n",
    "&emsp;&emsp;我们可以用多个三维卷积核，每个核生成一个输出通道。\n",
    "\n",
    "1. 输入为$X$, 它的维度为: $c_{i} \\times n_{h} \\times n_{w}$；\n",
    "2. 核为$W$, 它的纬度为: $c_{o} \\times c_{i} \\times k_{h} \\times k_{w}$；\n",
    "3. 偏差$b \\in \\mathbf{R}$。\n",
    "4. 输出为$Y$, 纬度为: $c_{o} \\times m_{h} \\times m_{w}$。\n",
    "\n",
    "- **多个输入和输出通道**\n",
    "\n",
    "&emsp;&emsp;每个输出通道可以识别特定模式。输入通道核识别并组合输入中的模式。\n",
    "\n",
    "- $1 \\times 1$卷积层\n",
    "\n",
    "&emsp;&emsp;$k_{h} = k_{w} = 1$是一个受欢迎的选择，它不识别空间模式，只是融合通道。\n",
    "\n",
    "- **二维卷积层通用情况**:\n",
    "\n",
    "&emsp;&emsp;我们可以用多个三维卷积核，每个核生成一个输出通道。\n",
    "\n",
    "1. 输入为$X$, 它的维度为: $c_{i} \\times n_{h} \\times n_{w}$；\n",
    "2. 核为$W$, 它的纬度为: $c_{o} \\times c_{i} \\times k_{h} \\times k_{w}$；\n",
    "3. 偏差$c_{o} \\times c_{i}$。\n",
    "4. 输出为$Y$, 纬度为: $c_{o} \\times m_{h} \\times m_{w}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9802285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_feature_map = torch.randn(5, 5)\n",
    "kernel = torch.randn(3, 3)\n",
    "\n",
    "def matrix_multiplication_for_conv2d_full(input_feature_map, kernel, bias=0, stride=1, padding=0):\n",
    "    \n",
    "    # input, kernel都是4维的张量。\n",
    "    if padding > 0:\n",
    "        # pad操作是从里到外的，前两个padding是width维度，之后是height维度，之后是channel维度，再之后是batch size维度\n",
    "        input_feature_map = F.pad(input_feature_map, (padding, padding, padding, padding, 0, 0, 0, 0))\n",
    "    \n",
    "    batch_size, in_channel, input_h, input_w = input_feature_map.shape\n",
    "    out_channel, in_channel, kernel_h, kernel_w = kernel.shape\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channel)\n",
    "    \n",
    "    output_h = (math.floor((input_h - kernel_h)/stride) + 1)  # 卷积输出高度\n",
    "    output_w = (math.floor((input_w - kernel_w)/stride) + 1)  # 卷积输出宽度\n",
    "    output = torch.zeros(batch_size, out_channel, output_h, output_w) # 初始化输出矩阵\n",
    "    \n",
    "    for ind in range(batch_size):\n",
    "        for oc in range(out_channel):\n",
    "            for ic in range(in_channel):\n",
    "                for i in range(0, input_h-kernel_h + 1, stride):  # 对高度进行遍历\n",
    "                    \n",
    "                    for j in range(0, input_w-kernel_w + 1, stride):  # 对宽度进行遍历\n",
    "                        region = input_feature_map[ind, ic, i:i+kernel_h, j:j+kernel_w]  # 取出被核滑动到的区域\n",
    "                        # 点乘，并赋值给输出位置的元素\n",
    "                        output[ind, oc, int(i/stride), int(j/stride)] += torch.sum(region * kernel[oc, ic])  \n",
    "            output[ind, oc] += bias[oc]\n",
    "    return output\n",
    "\n",
    "input = torch.ones(2, 2, 5, 5)  # batch_size, in_channel, in_height, in_weidth\n",
    "\n",
    "kernel = torch.ones(3, 2, 3, 3) # out_channel, in_channel, kernel_h, kernel_w\n",
    "bias = torch.ones(3)\n",
    "pytorch_conv2d_api_out = F.conv2d(input=input, weight=kernel, bias=bias, padding=1, stride=2)\n",
    "mm_conv2d_full_output = matrix_multiplication_for_conv2d_full(input, kernel, bias=bias, padding=1, stride=2)\n",
    "print(torch.allclose(pytorch_conv2d_api_out, mm_conv2d_full_output))\n",
    "# print(pytorch_conv2d_api_out)\n",
    "# print(mm_conv2d_full_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba6c06",
   "metadata": {},
   "source": [
    "返回结果为True，所以实现的卷积操作和官方API实现的结果是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ddb4f5",
   "metadata": {},
   "source": [
    "## kernel填充并拉直实现二维卷积\n",
    "\n",
    "如果将kernel不拉平，而是将其填充，填充为输入特征图这么大，我们同样可以进行卷积运算，之后可以变形为转置卷积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c5decea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3145],\n",
      "        [ 0.8220],\n",
      "        [ 2.6329],\n",
      "        [ 2.0421]])\n",
      "tensor([[[[-0.3145,  0.8220],\n",
      "          [ 2.6329,  2.0421]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 通过对kernel进行展开来实现二维卷积，并推导出转置卷积，不考虑batchsize 、channel大小, 假设stride=1\n",
    "def get_kernel_matrix(kernel, input_size):\n",
    "    \"\"\"基于kernel和输入特征图的大小来得到填充拉直后的kernel堆叠的矩阵\"\"\"\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    input_h, input_w = input_size\n",
    "    num_out = (input_h - kernel_h + 1) * (input_w - kernel_w + 1)  # 输出特征图元素个数\n",
    "    result = torch.zeros((num_out, input_h * input_w))  # 初始化结果矩阵\n",
    "    count = 0\n",
    "    for i in range(0, input_h - kernel_h + 1):\n",
    "        for j in range(0, input_w - kernel_w + 1):\n",
    "            # 先列，后行\n",
    "            padded_kernel = F.pad(kernel, (j, input_w-kernel_w-j, i, input_h-kernel_h-i))\n",
    "            result[count] = padded_kernel.flatten()\n",
    "            count += 1\n",
    "    return result\n",
    "\n",
    "kernel = torch.randn(3, 3)\n",
    "input = torch.randn(4, 4)\n",
    "kernel_matrix = get_kernel_matrix(kernel, input.shape)  # 4 * 16\n",
    "\n",
    "mm_conv2d_output = kernel_matrix @ input.reshape((-1, 1))  # 通过矩阵相乘来计算卷积。\n",
    "pytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "print(mm_conv2d_output)\n",
    "print(pytorch_conv2d_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e686299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb8351a3",
   "metadata": {},
   "source": [
    "## 转置卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ecc26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c843f0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.6518],\n",
      "        [ 2.1010],\n",
      "        [ 0.5174],\n",
      "        [ 0.9915],\n",
      "        [ 3.2721],\n",
      "        [-0.2065],\n",
      "        [-4.1480],\n",
      "        [-3.9849],\n",
      "        [ 2.2477],\n",
      "        [ 0.6916],\n",
      "        [-0.5959],\n",
      "        [ 0.7970],\n",
      "        [ 0.4235],\n",
      "        [-0.5867],\n",
      "        [-0.7200],\n",
      "        [ 0.5554]])\n",
      "tensor([[[[ 2.6518,  2.1010,  0.5174,  0.9915],\n",
      "          [ 3.2721, -0.2065, -4.1480, -3.9849],\n",
      "          [ 2.2477,  0.6916, -0.5959,  0.7970],\n",
      "          [ 0.4235, -0.5867, -0.7200,  0.5554]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 通过对kernel进行展开来实现二维卷积，并推导出转置卷积，不考虑batchsize 、channel大小, 假设stride=1\n",
    "def get_kernel_matrix(kernel, input_size):\n",
    "    \"\"\"基于kernel和输入特征图的大小来得到填充拉直后的kernel堆叠的矩阵\"\"\"\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    input_h, input_w = input_size\n",
    "    num_out = (input_h - kernel_h + 1) * (input_w - kernel_w + 1)  # 输出特征图元素个数\n",
    "    result = torch.zeros((num_out, input_h * input_w))  # 初始化结果矩阵\n",
    "    count = 0\n",
    "    for i in range(0, input_h - kernel_h + 1):\n",
    "        for j in range(0, input_w - kernel_w + 1):\n",
    "            # 先列，后行\n",
    "            padded_kernel = F.pad(kernel, (j, input_w-kernel_w-j, i, input_h-kernel_h-i))\n",
    "            result[count] = padded_kernel.flatten()\n",
    "            count += 1\n",
    "    return result\n",
    "\n",
    "kernel = torch.randn(3, 3)\n",
    "input = torch.randn(4, 4)\n",
    "kernel_matrix = get_kernel_matrix(kernel, input.shape)  # 4 * 16\n",
    "\n",
    "mm_conv2d_output = kernel_matrix @ input.reshape((-1, 1))  # 通过矩阵相乘来计算卷积。\n",
    "pytorch_conv2d_output = F.conv2d(input.unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "mm_transpose_conv2d_output = kernel_matrix.transpose(-1, -2) @ mm_conv2d_output  # 通过矩阵运算得到转置卷积\n",
    "pytorch_transposed_conv2d_output = F.conv_transpose2d(pytorch_conv2d_output, kernel.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "print(mm_transpose_conv2d_output)\n",
    "print(pytorch_transposed_conv2d_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721aa05",
   "metadata": {},
   "source": [
    "## 空洞卷积与群卷积\n",
    "\n",
    "dilation和group默认都等于1。dilation控制输入特征图是否是紧凑的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f51af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2102, -2.4184,  0.9865,  0.8620,  0.6155, -1.1641, -0.3031],\n",
      "        [ 1.3455, -0.5780, -1.9160, -2.2002,  0.9185, -0.0121,  0.4882],\n",
      "        [-2.2679,  0.7489,  0.1389,  0.6044, -1.8629,  1.1583, -1.4995],\n",
      "        [ 0.7683,  1.0415, -0.3213,  1.3037, -1.4020,  0.2959,  0.8235],\n",
      "        [ 1.7221,  0.0756, -1.6799,  1.4399,  0.1692,  1.0769,  0.4627],\n",
      "        [-0.5466,  1.2555,  0.1157, -0.2629, -0.6744, -2.4479, -0.5838],\n",
      "        [-1.2917, -1.4710, -1.4848,  0.2151,  0.6331,  0.7295,  1.1923]])\n",
      "tensor([[ 1.2102, -2.4184,  0.9865],\n",
      "        [ 1.3455, -0.5780, -1.9160],\n",
      "        [-2.2679,  0.7489,  0.1389]])\n",
      "tensor([[ 1.2102,  0.9865,  0.6155],\n",
      "        [-2.2679,  0.1389, -1.8629],\n",
      "        [ 1.7221, -1.6799,  0.1692]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "a = torch.randn(7, 7)\n",
    "\n",
    "print(a)\n",
    "print(a[0:3, 0:3])  # dilation=1\n",
    "print(a[0:5:2, 0:5:2])  # dilation=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab6a46",
   "metadata": {},
   "source": [
    "groups不等于1的话，就会将大的卷积分成小卷积。比如in_channel, out_channel = 2, 4， groups=2的话，我们就会有两组卷积: 两组卷积都是sub_in_channel, sub_out_channel = 1, 2，整体来看输入输出是没有维度上的差异的。也就是groups大于1的话，通道融合不需要完全充分，只需要在一个groups内进行融合，最后拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4774423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_feature_map = torch.randn(5, 5)\n",
    "kernel = torch.randn(3, 3)\n",
    "\n",
    "def matrix_multiplication_for_conv2d_final(input_feature_map, kernel, bias=0, stride=1, padding=0, \\\n",
    "                                           dilation=1, groups=1):\n",
    "    \n",
    "    \n",
    "    # input, kernel都是4维的张量。\n",
    "    if padding > 0:\n",
    "        # pad操作是从里到外的，前两个padding是width维度，之后是height维度，之后是channel维度，再之后是batch size维度\n",
    "        input_feature_map = F.pad(input_feature_map, (padding, padding, padding, padding, 0, 0, 0, 0))\n",
    "    \n",
    "    batch_size, in_channel, input_h, input_w = input_feature_map.shape\n",
    "    out_channel, _, kernel_h, kernel_w = kernel.shape\n",
    "    \n",
    "    assert out_channel % groups == 0 and in_channel % groups == 0, \"groups必须要同时被输入通道数和输出通道数整除\"\n",
    "    \n",
    "    input_feature_map = input_feature_map.reshape((batch_size, groups, in_channel// groups, input_h, input_w))\n",
    "    \n",
    "    kernel = kernel.reshape((groups, out_channel//groups, in_channel//groups, kernel_h, kernel_w))\n",
    "    \n",
    "    kernel_h = (kernel_h-1)*(dilation-1) + kernel_h  # 插入空洞之后的高宽\n",
    "    kernel_w = (kernel_w-1)*(dilation-1) + kernel_w\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(out_channel)\n",
    "    \n",
    "    output_h = math.floor((input_h - kernel_h)/stride) + 1  # 卷积输出高度\n",
    "    output_w = math.floor((input_w - kernel_w)/stride) + 1  # 卷积输出宽度\n",
    "    output = torch.zeros(batch_size, groups, out_channel//groups, output_h, output_w) # 初始化输出矩阵\n",
    "    \n",
    "    for ind in range(batch_size):\n",
    "        for g in range(groups):\n",
    "            for oc in range(out_channel//groups):\n",
    "                for ic in range(in_channel//groups):\n",
    "                    for i in range(0, input_h-kernel_h + 1, stride):  # 对高度进行遍历\n",
    "                        for j in range(0, input_w-kernel_w + 1, stride):  # 对宽度进行遍历\n",
    "                            # 取出被核滑动到的区域\n",
    "                            region = input_feature_map[ind, g, ic, i:i+kernel_h:dilation, j:j+kernel_w:dilation]  \n",
    "                            # 点乘，并赋值给输出位置的元素\n",
    "                            output[ind, g, oc, int(i/stride), int(j/stride)] += torch.sum(region * kernel[g, oc, ic])  \n",
    "                output[ind, g, oc] += bias[g * (out_channel // groups) + oc]  # 考虑偏置\n",
    "    output = output.reshape((batch_size, out_channel, output_h, output_w))\n",
    "    return output\n",
    "\n",
    "\n",
    "## 测试\n",
    "\n",
    "kernel_size = 3\n",
    "bs, in_channel, input_h, input_w = 2, 2, 7, 7\n",
    "out_channel = 4\n",
    "groups, dilation, stride, padding = 2, 2, 2, 1\n",
    "\n",
    "\n",
    "input = torch.randn(bs, in_channel, input_h, input_w)  # batch_size, in_channel, in_height, in_weidth\n",
    "\n",
    "# out_channel, in_channel//groups, kernel_h, kernel_w\n",
    "kernel = torch.randn(out_channel, in_channel//groups, kernel_size, kernel_size) \n",
    "bias = torch.randn(out_channel)\n",
    "\n",
    "\n",
    "pytorch_conv2d_api_out = F.conv2d(input=input, weight=kernel, bias=bias, padding=padding, stride=stride,\\\n",
    "                                 dilation=dilation, groups=groups)\n",
    "mm_conv2d_final_output = matrix_multiplication_for_conv2d_final(input, kernel, bias=bias, padding=padding, \\\n",
    "                                                              stride=stride, dilation=dilation, groups=groups)\n",
    "\n",
    "print(torch.allclose(pytorch_conv2d_api_out, mm_conv2d_final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a02640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d0fe2dc",
   "metadata": {},
   "source": [
    "## 池化层\n",
    "\n",
    "&emsp;&emsp;卷积层对于位置信息是非常敏感的，而实际的图像因为照明，物体的位置，比例，外观等因图像而异，所以我们需要一定的平移不变性。\n",
    "\n",
    " 二维最大池化\n",
    "\n",
    "&emsp;&emsp;每次返回二维窗口中的最大值。因此最大池化层会允许输入的元素发生小小的偏移，并且会有一定的模糊效果，但是它没有可学习的参数，输出通道数会等于输入通道数。\n",
    "\n",
    "平均池化层\n",
    "\n",
    "&emsp;&emsp;把最大的那个操作子变成平均。最大池化层获取的是每个窗口中最强的那个信号，平均池化层的信号强化会弱很多，但是会有一个比较柔和化的效果。\n",
    "\n",
    "小结\n",
    "\n",
    "&emsp;&emsp;它会缓解卷积层带来的位置敏感性。同样有窗口大小、填充、和步幅作为超参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41ec18",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29df2f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;实现池化层的正向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4344a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def pool2d(X, pool_size, mode=\"max\"):\n",
    "    x_h, x_w = X.shape\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((x_h - p_h + 1, x_w - p_w + 1))  # 创建输出\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == \"max\":\n",
    "                Y[i, j] = X[i:i+p_h, j:j+p_w].max()\n",
    "            elif mode == \"avg\":\n",
    "                Y[i, j] = X[i:i+p_h, j:j+p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1560620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704f7e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证最大池化\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afe5ed61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证平均池化\n",
    "pool2d(X, (2, 2), mode=\"avg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e72e79",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`torch`中的代码实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e721061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape(1, 1, 4, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a5caf",
   "metadata": {},
   "source": [
    "&emsp;&emsp;深度学习中的步幅与池化窗口相同(torch框架是这样的)，也就是移动的时候，下一个池化区域与上一个池化区域没有重叠部分:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "315e0aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3)  # 设定一个3 X 3的窗口\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935d65b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;填充和步幅可以手动设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19931787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d6f91",
   "metadata": {},
   "source": [
    "&emsp;&emsp;设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b006b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  3.],\n",
       "          [ 9., 11.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d((2, 3), padding=(1, 1), stride=(2, 3))\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae3c63",
   "metadata": {},
   "source": [
    "&emsp;&emsp;池化层在每个通道上单独运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92bd96f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.cat((X, X+1), 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "876c14d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d827a7d",
   "metadata": {},
   "source": [
    "## 问题\n",
    "\n",
    "- 输入和输出的通道数如果不变的话，通道数一般也不变；输入和输出的通道数减半的时候，输出的通道数通常会加一倍，也就是空间信息压缩了，把提取的信息在更多的通道里面存储下来。\n",
    "\n",
    "1. **网络越深，padding 0越多，是否会影响性能？**\n",
    "\n",
    "&emsp;&emsp;计算性能增加了一点点，对于模型性能是不会影响的，因为0对于卷积是没有多少影响的。\n",
    "\n",
    "2. **每个通道的卷积核都不一样吗？同一层不同通道的卷积核大小必须一样吗？**\n",
    "\n",
    "&emsp;&emsp;每个通道的卷积核是不一样的，同一层不同通道的卷积核大小必须一样。\n",
    "\n",
    "3. **计算卷积时，bias的有无，对结果影响大吗？bias的作用怎么理解？**\n",
    "\n",
    "&emsp;&emsp;bias是有一些作用的，但是它的作用会变得越来越低。比如像数据的均值不为0的时候，bias就可以去等价于均值的负数。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. 核大小、填充、步幅这几个超参数的重要程度排序是怎样的？\n",
    "\n",
    "- 一般来说填充就是取核的大小减去1，使得输入输出的大小一样。\n",
    "- 步幅通常选择为1，通常计算量很大的话，我们不会选择步幅为1的情况。\n",
    "\n",
    "2. 卷积核的边长为什么一般取奇数？\n",
    "\n",
    "- 卷积核的边长取奇数的原因在于填充的时候能够使得上下填充是对称的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddd05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
