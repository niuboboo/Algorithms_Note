{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06763c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "055161f6",
   "metadata": {},
   "source": [
    "## 交叉熵损失函数\n",
    "\n",
    "- [torch.nn.CrossEntropyLoss损失函数的官网链接](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
    "\n",
    "这里的`torch.nn.CrossEntropyLoss`是一个类，因此想要调用的话，首先需要实例化。\n",
    "\n",
    "\n",
    "参数：\n",
    "\n",
    "1. `weight=None`: 如果某些类别不均衡的话，`weight`能够对其稍微处理一下，使其整体均衡一下。\n",
    "\n",
    "2. `size_average=None`:\n",
    "\n",
    "3. `ignore_index=- 100`：类似`padding`的操作，使其能够处理长度不均匀的情况。\n",
    "\n",
    "4. `reduce=None`: \n",
    "\n",
    "5. `reduction='mean'`: 也可以是`sum`或者`None`，如果是`None`的话，返回的就是每个样本的`Loss`，而不是对整个`minibatch`做的平均。\n",
    "\n",
    "6. `label_smoothing=0.0`: 把目标类别的概率值降低一点，降低出来的这些概率值，将其随机分配到其它类别上。\n",
    "\n",
    "交叉熵是用于衡量两个分布之间的差距的。分布$q$相对于分布$p$的交叉熵公式如下所示:\n",
    "\n",
    "$$\n",
    "H(p, q)=-\\mathrm{E}_{p}[\\log q]\n",
    "$$\n",
    "\n",
    "也就是对$log q$求$p$分布下的期望值。\n",
    "\n",
    "将其离散化的话，可以表示为:\n",
    "\n",
    "$$\n",
    "H(p, q)=-\\sum_{x \\in \\mathcal{X}} p(x) log q(x) \n",
    "$$\n",
    "\n",
    "可以看到如果将$p(x)$换成$q(x)$的话，这里就是随机变量的熵，这里是两个分布，所以称之为交叉熵。\n",
    "\n",
    "又由于是分类问题，传入的标签是`one-hot`类型的数据，所以目标分布$p(x)$只会有一项是`1`，其余都是`0`，就不用算了。因此，在`PyTorch`中，交叉熵损失函数被定义为如下形式:\n",
    "\n",
    "$$\n",
    "l_{n} = -w_{y_{n}} log \\frac{exp(x_{n,y_{n}})}{\\sum_{c=1}^{C} exp(x_{n,c})} \\dot \\{y_{n} \\neq ignore\\_index\\}\n",
    "$$\n",
    "\n",
    "其中$x$是输入，$y$是输出，$w$是权重，$C$是类别数，$N$是`mini-batch`参数。\n",
    "\n",
    "当然，在知识蒸馏中，我们的target分布并不一定是one-hot类别的，而是对于每个类别都有一个概率值。此时公式变为:\n",
    "\n",
    "$$\n",
    "l_{n} = -w_{y_{n}} log \\frac{exp(x_{n,y_{n}})}{\\sum_{c=1}^{C} exp(x_{n,c})} y_{n, c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2684ff",
   "metadata": {},
   "source": [
    "### 调用与实战\n",
    "\n",
    "- `Input`: 输入数据的形状可以是$(C)$, $(N, C)$, $(N, C, d_{1}, d_{2}, \\cdots, d_{K})$，其中$K \\geq 1$。对于$(C)$而言，就是一个样本，我们期望的`target`数据，加上`batch`之后，就是$(N, C)$，而如果是$(N, C, d_{1}, d_{2}, \\cdots, d_{K})$的话，可以理解为对于一个视频的每个像素都进行分类，后面的$d_{1}, d_{2}, \\cdots, d_{K}$可以是通道数，图像的高度和宽度等等，但是必须要类别数在第二维度。\n",
    "\n",
    "- `Target`: `Target`可以有两种，一种是类别索引，另一种是各个类别对应的概率值。如果是类别索引的话，`shape`是$()$,$(N)$,或者是$(N, d_{1}, d_{2}, \\cdots, d_{K})$，但是要注意类别索引是从`0`开始的。如果是概率值的话，`Target`的`shape`和输入是一样的。\n",
    "\n",
    "\n",
    "**需要注意的是，这里的`Input`期望是未归一化的数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8be5d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77663c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e4acd",
   "metadata": {},
   "source": [
    "#### target数据是index类型数据时："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dace3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn(batch_size, num_classes)\n",
    "target_index = torch.randint(num_classes, size=(batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a83a3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c736ffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss for index target is 1.7796131372451782\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss_value = cross_entropy_loss_fn(logits, target_index)\n",
    "print(f\"Cross Entropy Loss for index target is {cross_entropy_loss_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cd125",
   "metadata": {},
   "source": [
    "#### target数据是probability类型数据时:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63638cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_logits = torch.randn(batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14d1c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy Loss for probability is 1.6599925756454468\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_loss_value = cross_entropy_loss_fn(logits, torch.softmax(target_logits, -1))\n",
    "print(f\"Cross Entropy Loss for probability is {cross_entropy_loss_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8da14",
   "metadata": {},
   "source": [
    "## 负对数似然损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46006e10",
   "metadata": {},
   "source": [
    "- [torch.nn.NLLLoss损失函数的官网链接](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss)\n",
    "\n",
    "这里的`torch.nn.NLLLoss`是一个类，因此想要调用的话，首先需要实例化。负对数似然函数通常也是用来训练分类问题，函数原型如下:\n",
    "\n",
    "```python\n",
    "class torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "它同样提供可选参数`weight`。输入数据类型与交叉熵损失函数类似但需要是每一个类别的对数概率值。`target`必须是一个类别索引，而不能是概率值了。具体的使用方法如下所示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c027f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative log-likelihood loss: 1.7796125411987305\n"
     ]
    }
   ],
   "source": [
    "nll_fn = torch.nn.NLLLoss()\n",
    "nll_loss = nll_fn(torch.log(torch.softmax(logits, dim=-1) + 1e-7), target_index)\n",
    "print(f\"negative log-likelihood loss: {nll_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a01345",
   "metadata": {},
   "source": [
    "可以看到，这里的输出结果与交叉熵损失函数中`target`数据时`index`类型数据时产生的`loss`是一致的。这是因为，交叉熵就是负对数似然:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\log (\\mathcal{L}(\\theta))=\\frac{1}{N} \\log \\prod_{i} q_{\\theta}(X=i)^{N p(X=i)}=\\sum_{i} p(X=i) \\log q_{\\theta}(X=i)=-H(p, q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8473061",
   "metadata": {},
   "source": [
    "## KL散度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817c153",
   "metadata": {},
   "source": [
    "交叉熵等于信息熵加上一个KL（Kullback-Leibler）散度。对于离散空间的KL散度的定义如下所示:\n",
    "\n",
    "$$\n",
    "D_{\\mathrm{KL}}(P \\| Q)=\\sum_{x \\in \\mathcal{X}} P(x) \\log \\left(\\frac{P(x)}{Q(x)}\\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f3d7a",
   "metadata": {},
   "source": [
    "- [torch.nn.KLDivLoss的官网链接](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html?highlight=kld#torch.nn.KLDivLoss)\n",
    "\n",
    "函数原型为:\n",
    "\n",
    "```python\n",
    "class torch.nn.KLDivLoss(size_average=None, reduce=None, reduction='mean', log_target=False)\n",
    "```\n",
    "\n",
    "其官网给定的计算公式如下:\n",
    "\n",
    "$$\n",
    "L\\left(y_{\\text {pred }}, y_{\\text {true }}\\right)=y_{\\text {true }} \\cdot \\log \\frac{y_{\\text {true }}}{y_{\\text {pred }}}=y_{\\text {true }} \\cdot\\left(\\log y_{\\text {true }}-\\log y_{\\text {pred }}\\right)\n",
    "$$\n",
    "\n",
    "因此，在传入数据的时候, `input`需要在前, 并且期望`input`是属于`log`空间。如果`log_target`设置为`True`的话，则`target`也需要是在`log`空间中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "812debd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL loss: 0.2005188912153244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinyzqh/miniforge3/lib/python3.9/site-packages/torch/nn/functional.py:2886: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "kld_loss_fn = torch.nn.KLDivLoss()\n",
    "kld_loss = kld_loss_fn(torch.log(torch.softmax(logits, dim=-1)), torch.softmax(target_logits, dim=-1))\n",
    "print(f\"KL loss: {kld_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9176f",
   "metadata": {},
   "source": [
    "###  验证\n",
    "\n",
    "$$\n",
    "H(p, q) = H(p) + D_{KL}(p || q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f755898d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss_sample: tensor([2.1617, 1.1582])\n"
     ]
    }
   ],
   "source": [
    "ce_loss_fn_sample = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "ce_loss_sample = ce_loss_fn_sample(logits, torch.softmax(target_logits, dim=-1))\n",
    "print(f\"ce_loss_sample: {ce_loss_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "412400fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kld_loss_sample : tensor([1.2465, 0.3577])\n"
     ]
    }
   ],
   "source": [
    "kld_loss_fn_sample = torch.nn.KLDivLoss(reduction='none')\n",
    "kld_loss_sample = kld_loss_fn_sample(torch.log(torch.softmax(logits, dim=-1)), \n",
    "                                     torch.softmax(target_logits, dim=-1)).sum(-1)\n",
    "print(f\"kld_loss_sample : {kld_loss_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b08d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_information_entropy tensor([0.9153, 0.8005])\n"
     ]
    }
   ],
   "source": [
    "target_information_entropy = torch.distributions.Categorical(probs=torch.softmax(target_logits, dim=-1)).entropy()\n",
    "print(f\"target_information_entropy {target_information_entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9e3edd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1617, 1.1582])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(kld_loss_sample + target_information_entropy)\n",
    "print(torch.allclose(ce_loss_sample, kld_loss_sample + target_information_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be4821",
   "metadata": {},
   "source": [
    "如果目标分布是one-hot类型，那么target_information_entropy就会为0，此时优化交叉熵与优化KL散度是一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebaa89d",
   "metadata": {},
   "source": [
    "## BCELoss\n",
    "\n",
    "BCELoss（binary cross entropy）的官方链接为:[torch.nn.BCELoss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html?highlight=bceloss#torch.nn.BCELoss)\n",
    "\n",
    "BCELosswithlogits的API中自带softmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c95ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss_fn = torch.nn.BCELoss()\n",
    "logits = torch.randn(batch_size)\n",
    "prob_1 = torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1eb88bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(2, size=(batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3c5e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce_loss : 0.7650077939033508\n"
     ]
    }
   ],
   "source": [
    "bce_loss = bce_loss_fn(prob_1, target.float())\n",
    "print(f\"bce_loss : {bce_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85791fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll_loss_binary : 0.7650077939033508\n"
     ]
    }
   ],
   "source": [
    "prob_0 = 1-prob_1.unsqueeze(-1)\n",
    "prob = torch.cat([prob_0, prob_1.unsqueeze(-1)], dim=-1)\n",
    "nll_loss_binary = nll_fn(torch.log(prob), target)\n",
    "print(f\"nll_loss_binary : {nll_loss_binary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
