{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220cc23e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13935a97",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一台机器可以安装多个GPU(1-16)，在训练和预测的时候，我们将一个小批量计算切分到多个GPU上来加速。常用的切分方案有：\n",
    "\n",
    "1. **数据并行**：将小批量分成n块，每个GPU拿到完整参数，计算这一块数据的梯度，之后将各个小批量的梯度加和在一起。\n",
    "2. **模型并行**：将模型分成n块，每个GPU拿到一块模型计算它的前向和方向结果。比如有一个100层的残差网络，有两块GPU的话，那就是第0块GPU拿到前面50层，第一块GPU拿到后面50层。这种方式主要是用于模型在单GPU放不下的情况。\n",
    "3. **通道并行(数据+模型并行)**："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba263069",
   "metadata": {},
   "source": [
    "1. 小批量分到多GPU计算后，模型结果通常是通过梯度的方式合并在一起。模型只有一份。\n",
    "2. 数据拆分并行之后，中间需要存储的东西需要增加模型和梯度，因为需要将模型拷贝到各个GPU上，由于Batch_Size也会变小，所以，**性能会有所损失**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d89c1d",
   "metadata": {},
   "source": [
    "## 单机单卡\n",
    "\n",
    "1. 模型拷贝, `model.cuda()`。\n",
    "2. 数据拷贝(每步), `data = data.cuda()`。\n",
    "3. 基于`torch.cuda.is_avaliable()`来判断是否可用。\n",
    "4. 这里模型保存与加载与`cpu`版本基本一致，`torch.save`模型、优化器、其它变量。模型加载时，可以指定是将模型加载到哪个`GPU`上去`torch.load(file.pt,  map_location=torch.device(\"cuda\"/ \"cuda:0/ \"cpu\"))`。\n",
    "\n",
    "单机单卡对于几千万模型以内的参数都够用了。在编写好模型之后，通常要做两个拷贝工作，模型拷贝和数据拷贝。模型拷贝的话，就是将模型的参数和`buffer`拷贝到`GPU上`，训练数据同样要拷贝到`GPU`上。模型拷贝是原地操作，而数据拷贝是得到一个拷贝后的对象`data = data.cuda()`。\n",
    "\n",
    "有时候机子上可能不止一张卡，我们可以通过命令行`CUDA_VISIBLE_DEVICE=\"\"`来限制`GPU`卡的使用。或者通过`os.environ['CUDA_VISIBLE_DEVICES'] = \"2,1,3,4\"`来进行设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf0ab9",
   "metadata": {},
   "source": [
    "### 单机单卡代码修改部分\n",
    "\n",
    "1. 判断`GPU`是否可用\n",
    "\n",
    "```python\n",
    "if torch.cuda.is_available():\n",
    "    logging.warning(\"Cuda is available!\")\n",
    "else:\n",
    "    logging.warning(\"Cuda is not available!\")\n",
    "    return\n",
    "```\n",
    "\n",
    "2. 模型拷贝，和数据拷贝，在`train`中：\n",
    "\n",
    "```python\n",
    "model.cuda()\n",
    "```\n",
    "\n",
    "和\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(resume, map_location=torch.device(\"cpu\"))  # 可以是cpu，cuda，cuda：index\n",
    "```\n",
    "\n",
    "除此之外，还需要对数据进行拷贝:\n",
    "\n",
    "```python\n",
    "token_index = token_index.cuda()  # 数据拷贝\n",
    "target = target.cuda()  # 数据拷贝\n",
    "```\n",
    "\n",
    "这里调用的是`Tensor.cuda()`方法，返回的是`Tensor`在`Cuda`中的一个复制品。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ef6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "!pip install torchtext #安装指令\n",
    "from torchtext.datasets.imdb import NUM_LINES\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARN,\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = 15000\n",
    "# 第一期： 编写GCNN模型代码\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embedding_dim=64, num_class=2):\n",
    "        super(GCNN, self).__init__()\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding_table.weight)\n",
    "\n",
    "        self.conv_A_1 = nn.Conv1d(embedding_dim, 64, 15, stride=7)\n",
    "        self.conv_B_1 = nn.Conv1d(embedding_dim, 64, 15, stride=7)\n",
    "\n",
    "        self.conv_A_2 = nn.Conv1d(64, 64, 15, stride=7)\n",
    "        self.conv_B_2 = nn.Conv1d(64, 64, 15, stride=7)\n",
    "\n",
    "        self.output_linear1 = nn.Linear(64, 128)\n",
    "        self.output_linear2 = nn.Linear(128, num_class)\n",
    "\n",
    "    def forward(self, word_index):\n",
    "        # 定义GCN网络的算子操作流程，基于句子单词ID输入得到分类logits输出\n",
    "\n",
    "        # 1. 通过word_index得到word_embedding\n",
    "        # word_index shape:[bs, max_seq_len]\n",
    "        word_embedding = self.embedding_table(word_index) #[bs, max_seq_len, embedding_dim]\n",
    "\n",
    "        # 2. 编写第一层1D门卷积模块\n",
    "        word_embedding = word_embedding.transpose(1, 2) #[bs, embedding_dim, max_seq_len]\n",
    "        A = self.conv_A_1(word_embedding)\n",
    "        B = self.conv_B_1(word_embedding)\n",
    "        H = A * torch.sigmoid(B) #[bs, 64, max_seq_len]\n",
    "\n",
    "        A = self.conv_A_2(H)\n",
    "        B = self.conv_B_2(H)\n",
    "        H = A * torch.sigmoid(B) #[bs, 64, max_seq_len]\n",
    "\n",
    "        # 3. 池化并经过全连接层\n",
    "        pool_output = torch.mean(H, dim=-1) #平均池化，得到[bs, 64]\n",
    "        linear1_output = self.output_linear1(pool_output)\n",
    "        logits = self.output_linear2(linear1_output) #[bs, 2]\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    \"\"\" 简单版embeddingbag+DNN模型 \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embed_dim=64, num_class=2):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, token_index):\n",
    "        embedded = self.embedding(token_index) # shape: [bs, embedding_dim]\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "\n",
    "# step2 构建IMDB DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def yield_tokens(train_data_iter, tokenizer):\n",
    "    for i, sample in enumerate(train_data_iter):\n",
    "        label, comment = sample\n",
    "        yield tokenizer(comment)\n",
    "\n",
    "train_data_iter = IMDB(root='.data', split='train') # Dataset类型的对象\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter, tokenizer), min_freq=20, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(0)\n",
    "print(f\"单词表大小: {len(vocab)}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\" 对DataLoader所生成的mini-batch进行后处理 \"\"\"\n",
    "    target = []\n",
    "    token_index = []\n",
    "    max_length = 0\n",
    "    for i, (label, comment) in enumerate(batch):\n",
    "        tokens = tokenizer(comment)\n",
    "\n",
    "        token_index.append(vocab(tokens))\n",
    "        if len(tokens) > max_length:\n",
    "            max_length = len(tokens)\n",
    "\n",
    "        if label == \"pos\":\n",
    "            target.append(0)\n",
    "        else:\n",
    "            target.append(1)\n",
    "\n",
    "    token_index = [index + [0]*(max_length-len(index)) for index in token_index]\n",
    "    return (torch.tensor(target).to(torch.int64), torch.tensor(token_index).to(torch.int32))\n",
    "\n",
    "\n",
    "# step3 编写训练代码\n",
    "def train(train_data_loader, eval_data_loader, model, optimizer, num_epoch, log_step_interval, save_step_interval, eval_step_interval, save_path, resume=\"\"):\n",
    "    \"\"\" 此处data_loader是map-style dataset \"\"\"\n",
    "    start_epoch = 0\n",
    "    start_step = 0\n",
    "    if resume != \"\":\n",
    "        #  加载之前训过的模型的参数文件\n",
    "        logging.warning(f\"loading from {resume}\")\n",
    "        checkpoint = torch.load(resume, map_location=torch.device(\"cpu\"))  # 可以是cpu，cuda，cuda：index\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        start_step = checkpoint['step']\n",
    "    \n",
    "    model.cuda()\n",
    "    \n",
    "    for epoch_index in range(start_epoch, num_epoch):\n",
    "        ema_loss = 0.\n",
    "        num_batches = len(train_data_loader)\n",
    "\n",
    "        for batch_index, (target, token_index) in enumerate(train_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            step = num_batches*(epoch_index) + batch_index + 1\n",
    "            \n",
    "            \n",
    "            token_index = token_index.cuda()  # 数据拷贝\n",
    "            target = target.cuda()  # 数据拷贝\n",
    "            \n",
    "            logits = model(token_index)\n",
    "            \n",
    "            \n",
    "            bce_loss = F.binary_cross_entropy(torch.sigmoid(logits), F.one_hot(target, num_classes=2).to(torch.float32))\n",
    "            ema_loss = 0.9*ema_loss + 0.1*bce_loss\n",
    "            bce_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % log_step_interval == 0:\n",
    "                logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, ema_loss: {ema_loss.item()}\")\n",
    "\n",
    "            if step % save_step_interval == 0:\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                save_file = os.path.join(save_path, f\"step_{step}.pt\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch_index,\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': bce_loss,\n",
    "                }, save_file)\n",
    "                logging.warning(f\"checkpoint has been saved in {save_file}\")\n",
    "\n",
    "            if step % eval_step_interval == 0:\n",
    "                logging.warning(\"start to do evaluation...\")\n",
    "                model.eval()\n",
    "                ema_eval_loss = 0\n",
    "                total_acc_account = 0\n",
    "                total_account = 0\n",
    "                for eval_batch_index, (eval_target, eval_token_index) in enumerate(eval_data_loader):\n",
    "                    total_account += eval_target.shape[0]\n",
    "                    eval_logits = model(eval_token_index)\n",
    "                    total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()\n",
    "                    eval_bce_loss = F.binary_cross_entropy(torch.sigmoid(eval_logits), F.one_hot(eval_target, num_classes=2).to(torch.float32))\n",
    "                    ema_eval_loss = 0.9*ema_eval_loss + 0.1*eval_bce_loss\n",
    "                acc = total_acc_account/total_account\n",
    "\n",
    "                logging.warning(f\"eval_ema_loss: {ema_eval_loss.item()}, eval_acc: {acc.item()}\")\n",
    "                model.train()\n",
    "\n",
    "# step4 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        logging.warning(\"Cuda is available!\")\n",
    "    else:\n",
    "        logging.warning(\"Cuda is not available!\")\n",
    "        return\n",
    "    \n",
    "    model = GCNN()\n",
    "    #  model = TextClassificationModel()\n",
    "    print(\"模型总参数:\", sum(p.numel() for p in model.parameters()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_data_iter = IMDB(root='.data', split='train') # Dataset类型的对象\n",
    "    train_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(train_data_iter), batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    eval_data_iter = IMDB(root='.data', split='test') # Dataset类型的对象\n",
    "    eval_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(eval_data_iter), batch_size=8, collate_fn=collate_fn)\n",
    "    resume = \"\"\n",
    "\n",
    "    train(train_data_loader, eval_data_loader, model, optimizer, num_epoch=10, log_step_interval=20, save_step_interval=500, eval_step_interval=300, save_path=\"./logs_imdb_text_classification\", resume=resume)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135aae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c43761ac",
   "metadata": {},
   "source": [
    "## 单机多卡DataParallel版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314b27f",
   "metadata": {},
   "source": [
    "对于单机多卡，我们需要用`torch.cuda.device_count()`来计算出`GPU`的数目。\n",
    "\n",
    "之前版本采用的是`torch.nn.DataParallel`来去实现, 这个`API`是通过多进程控制多卡的训练的，所以它的问题是：效率较慢，不支持多机的情况，不支持模型并行。\n",
    "\n",
    "对于这种方式，我们需要改动的地方在于将模型用`DataParallel`包裹起来, 然后在内部指定`device_ids`。\n",
    "\n",
    "```python\n",
    "model = DataParallel(model.cuda(), device_ids=[0, 1, 2, 3])\n",
    "```\n",
    "\n",
    "之后的话，其源码中就会将模型复制到每个卡上，数据也进行切分，送到每个卡上进行训练。\n",
    "\n",
    "这里需要注意的是，在模型保存时，我们调用的是`model.module.state_dict()`。加载时，`torch.load`需要注意`map_location`使用。\n",
    "\n",
    "此处的`batch size`应该是每个`GPU`的`batch size`的总和。\n",
    "\n",
    "相比于单机单卡，需要修改的代码有:\n",
    "\n",
    "```python\n",
    "if torch.cuda.is_available():\n",
    "    logging.warning(\"Cuda is available!\")\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        logging.warning(\"Find {} GPUS\".format(torch.cuda.device_count()))\n",
    "    else:\n",
    "        logging.warning(\"Too Few GPU!\")\n",
    "        return\n",
    "\n",
    "else:\n",
    "    logging.warning(\"Cuda is not available!\")\n",
    "    return\n",
    "```\n",
    "\n",
    "```python\n",
    "model = nn.DataParallel(model.cuda(), device_ids=[0, 1])  # 模型拷贝，放入DataParallel。\n",
    "```\n",
    "\n",
    "```python\n",
    "torch.save({\n",
    "                    'epoch': epoch_index,\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.module.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': bce_loss,\n",
    "                }, save_file)\n",
    "```\n",
    "\n",
    "```python\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97143d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "!pip install torchtext #安装指令\n",
    "from torchtext.datasets.imdb import NUM_LINES\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARN,\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = 15000\n",
    "# 第一期： 编写GCNN模型代码\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embedding_dim=64, num_class=2):\n",
    "        super(GCNN, self).__init__()\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding_table.weight)\n",
    "\n",
    "        self.conv_A_1 = nn.Conv1d(embedding_dim, 64, 15, stride=7)\n",
    "        self.conv_B_1 = nn.Conv1d(embedding_dim, 64, 15, stride=7)\n",
    "\n",
    "        self.conv_A_2 = nn.Conv1d(64, 64, 15, stride=7)\n",
    "        self.conv_B_2 = nn.Conv1d(64, 64, 15, stride=7)\n",
    "\n",
    "        self.output_linear1 = nn.Linear(64, 128)\n",
    "        self.output_linear2 = nn.Linear(128, num_class)\n",
    "\n",
    "    def forward(self, word_index):\n",
    "        # 定义GCN网络的算子操作流程，基于句子单词ID输入得到分类logits输出\n",
    "\n",
    "        # 1. 通过word_index得到word_embedding\n",
    "        # word_index shape:[bs, max_seq_len]\n",
    "        word_embedding = self.embedding_table(word_index) #[bs, max_seq_len, embedding_dim]\n",
    "\n",
    "        # 2. 编写第一层1D门卷积模块\n",
    "        word_embedding = word_embedding.transpose(1, 2) #[bs, embedding_dim, max_seq_len]\n",
    "        A = self.conv_A_1(word_embedding)\n",
    "        B = self.conv_B_1(word_embedding)\n",
    "        H = A * torch.sigmoid(B) #[bs, 64, max_seq_len]\n",
    "\n",
    "        A = self.conv_A_2(H)\n",
    "        B = self.conv_B_2(H)\n",
    "        H = A * torch.sigmoid(B) #[bs, 64, max_seq_len]\n",
    "\n",
    "        # 3. 池化并经过全连接层\n",
    "        pool_output = torch.mean(H, dim=-1) #平均池化，得到[bs, 64]\n",
    "        linear1_output = self.output_linear1(pool_output)\n",
    "        logits = self.output_linear2(linear1_output) #[bs, 2]\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    \"\"\" 简单版embeddingbag+DNN模型 \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embed_dim=64, num_class=2):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, token_index):\n",
    "        embedded = self.embedding(token_index) # shape: [bs, embedding_dim]\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "\n",
    "# step2 构建IMDB DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def yield_tokens(train_data_iter, tokenizer):\n",
    "    for i, sample in enumerate(train_data_iter):\n",
    "        label, comment = sample\n",
    "        yield tokenizer(comment)\n",
    "\n",
    "train_data_iter = IMDB(root='.data', split='train') # Dataset类型的对象\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter, tokenizer), min_freq=20, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(0)\n",
    "print(f\"单词表大小: {len(vocab)}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\" 对DataLoader所生成的mini-batch进行后处理 \"\"\"\n",
    "    target = []\n",
    "    token_index = []\n",
    "    max_length = 0\n",
    "    for i, (label, comment) in enumerate(batch):\n",
    "        tokens = tokenizer(comment)\n",
    "\n",
    "        token_index.append(vocab(tokens))\n",
    "        if len(tokens) > max_length:\n",
    "            max_length = len(tokens)\n",
    "\n",
    "        if label == \"pos\":\n",
    "            target.append(0)\n",
    "        else:\n",
    "            target.append(1)\n",
    "\n",
    "    token_index = [index + [0]*(max_length-len(index)) for index in token_index]\n",
    "    return (torch.tensor(target).to(torch.int64), torch.tensor(token_index).to(torch.int32))\n",
    "\n",
    "\n",
    "# step3 编写训练代码\n",
    "def train(train_data_loader, eval_data_loader, model, optimizer, num_epoch, log_step_interval, save_step_interval, eval_step_interval, save_path, resume=\"\"):\n",
    "    \"\"\" 此处data_loader是map-style dataset \"\"\"\n",
    "    start_epoch = 0\n",
    "    start_step = 0\n",
    "    if resume != \"\":\n",
    "        #  加载之前训过的模型的参数文件\n",
    "        logging.warning(f\"loading from {resume}\")\n",
    "        checkpoint = torch.load(resume, map_location=torch.device(\"cpu\"))  # 可以是cpu，cuda，cuda：index\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        start_step = checkpoint['step']\n",
    "    \n",
    "    model = nn.DataParallel(model.cuda(), device_ids=[0, 1])  # 模型拷贝，放入DataParallel。\n",
    "    \n",
    "    for epoch_index in range(start_epoch, num_epoch):\n",
    "        ema_loss = 0.\n",
    "        num_batches = len(train_data_loader)\n",
    "\n",
    "        for batch_index, (target, token_index) in enumerate(train_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            step = num_batches*(epoch_index) + batch_index + 1\n",
    "            \n",
    "            \n",
    "            token_index = token_index.cuda()  # 数据拷贝\n",
    "            target = target.cuda()  # 数据拷贝\n",
    "            \n",
    "            logits = model(token_index)\n",
    "            \n",
    "            \n",
    "            bce_loss = F.binary_cross_entropy(torch.sigmoid(logits), F.one_hot(target, num_classes=2).to(torch.float32))\n",
    "            ema_loss = 0.9*ema_loss + 0.1*bce_loss\n",
    "            bce_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % log_step_interval == 0:\n",
    "                logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, ema_loss: {ema_loss.item()}\")\n",
    "\n",
    "            if step % save_step_interval == 0:\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                save_file = os.path.join(save_path, f\"step_{step}.pt\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch_index,\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.module.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': bce_loss,\n",
    "                }, save_file)\n",
    "                logging.warning(f\"checkpoint has been saved in {save_file}\")\n",
    "\n",
    "            if step % eval_step_interval == 0:\n",
    "                logging.warning(\"start to do evaluation...\")\n",
    "                model.eval()\n",
    "                ema_eval_loss = 0\n",
    "                total_acc_account = 0\n",
    "                total_account = 0\n",
    "                for eval_batch_index, (eval_target, eval_token_index) in enumerate(eval_data_loader):\n",
    "                    total_account += eval_target.shape[0]\n",
    "                    eval_logits = model(eval_token_index)\n",
    "                    total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()\n",
    "                    eval_bce_loss = F.binary_cross_entropy(torch.sigmoid(eval_logits), F.one_hot(eval_target, num_classes=2).to(torch.float32))\n",
    "                    ema_eval_loss = 0.9*ema_eval_loss + 0.1*eval_bce_loss\n",
    "                acc = total_acc_account/total_account\n",
    "\n",
    "                logging.warning(f\"eval_ema_loss: {ema_eval_loss.item()}, eval_acc: {acc.item()}\")\n",
    "                model.train()\n",
    "\n",
    "# step4 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        logging.warning(\"Cuda is available!\")\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            logging.warning(\"Find {} GPUS\".format(torch.cuda.device_count()))\n",
    "        else:\n",
    "            logging.warning(\"Too Few GPU!\")\n",
    "            return\n",
    "            \n",
    "    else:\n",
    "        logging.warning(\"Cuda is not available!\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    model = GCNN()\n",
    "    #  model = TextClassificationModel()\n",
    "    print(\"模型总参数:\", sum(p.numel() for p in model.parameters()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_data_iter = IMDB(root='.data', split='train') # Dataset类型的对象\n",
    "    train_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(train_data_iter), batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    eval_data_iter = IMDB(root='.data', split='test') # Dataset类型的对象\n",
    "    eval_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(eval_data_iter), batch_size=8, collate_fn=collate_fn)\n",
    "    resume = \"\"\n",
    "\n",
    "    train(train_data_loader, eval_data_loader, model, optimizer, num_epoch=10, log_step_interval=20, save_step_interval=500, eval_step_interval=300, save_path=\"./logs_imdb_text_classification\", resume=resume)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52069fb",
   "metadata": {},
   "source": [
    "## 单机多卡DDP版本\n",
    "\n",
    "`torch.nn.parallel.DistributedDataParallel`这个版本是比较推荐的。它的效率会高很多。它的代码编写流程如下:\n",
    "\n",
    "1. 初始化进程组:\n",
    "\n",
    "```python\n",
    "torch.distribute.init_process_group(\"nccl\", world_size=n_gpus, rank=args.local_rank)\n",
    "```\n",
    "\n",
    "其中参数`\"nccl\"`是指定的`GPU`之间的通信方式。第二个参数是`world_size`，指当前节点上有多少张`GPU`卡，`rank=args.local_rank`指定当前进程在第几个GPU上。\n",
    "\n",
    "2. 设置当前节点能够使用的`GPU`卡的名称\n",
    "\n",
    "```python\n",
    "torch.cuda.set_device(args.local_rank)\n",
    "```\n",
    "\n",
    "`torch.cuda.set_device(args.local_rank)`这条语句类似于`CUDA_VISIBLE_DEVICE`环境变量。\n",
    "\n",
    "3. 第三步的话，我们就可以对这个模型进行包裹:\n",
    "\n",
    "```python\n",
    "model = DistributedDataParallel(model.cuda(args.local_rank), device_ids=[args.local_rank])\n",
    "```\n",
    "\n",
    "除此之外，我们还需要一个`train_sampler`, `train_sampler`是为了对每张卡的数据进行分配。\n",
    "\n",
    "```python\n",
    "train_sampler = DistributedSample(train_dataset)\n",
    "```\n",
    "\n",
    "源码位于`torch/utils/data/distributed.py`文件中。\n",
    "\n",
    "```python\n",
    "train_dataloader = DataLoader(..., sample=train_sampler)\n",
    "```\n",
    "\n",
    "之后需要做数据并行，local_rank是通过命令行传入进来的。\n",
    "\n",
    "```python\n",
    "data = data.cuda(args.local_rank)\n",
    "```\n",
    "\n",
    "\n",
    "之后的话就是需要在命令行进行程序的启动:\n",
    "\n",
    "```python\n",
    "python -m torch.distributed.launch --nproc_per_node=n_gpus train.py\n",
    "```\n",
    "\n",
    "`--nproc_per_node=n_gpus`参数指定的就是这个节点用到几张卡。\n",
    "\n",
    "对于模型的保存于加载部分，torch.save在local_rank=0的位置进行保存，同样注意调用model.module.state_dict()。torch.load使用的时候注意map_location。\n",
    "\n",
    "- 注意事项：\n",
    "\n",
    "train.py中要有接受local_rank的参数选项，launch会传入这个参数。\n",
    "\n",
    "在每个周期开始处，我们需要调用train_sampler.set_epoch(epoch)可以使得数据充分打乱。\n",
    "\n",
    "有了sampler，就不要在DataLoader中设置shuffle=True。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c9c4e",
   "metadata": {},
   "source": [
    "需要添加的代码有:\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", help=\"local device id on current node\", type=int)\n",
    "args = parser.parse_args()\n",
    "n_gpus = 2\n",
    "torch.distributed.init_process_group(\"nccl\", world_size=n_gpus, rank=args.local_rank)\n",
    "torch.cuda.set_device(args.local_rank)\n",
    "```\n",
    "\n",
    "模型处理:\n",
    "\n",
    "```python\n",
    "model = nn.parallel.DistributedDataParallel(model.cuda(local_rank), device_ids=[local_rank])   \n",
    "```\n",
    "\n",
    "其中，local_rank参数是传入给定的。\n",
    "\n",
    "数据处理部分:\n",
    "\n",
    "```python\n",
    "# 这里不需要shuffle，需要一个sample\n",
    "train_sampler = DistributedSampler(train_dataset)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, sampler=train_sampler)\n",
    "eval_data_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, sampler=train_sampler)\n",
    "```\n",
    "\n",
    "数据拷贝:\n",
    "\n",
    "```python\n",
    "token_index = token_index.cuda(local_rank)  # 数据拷贝\n",
    "target = target.cuda(local_rank)  # 数据拷贝\n",
    "```\n",
    "\n",
    "保存模型\n",
    "\n",
    "```python\n",
    "if step % save_step_interval == 0 and local_rank=0:\n",
    "```\n",
    "\n",
    "只在GPU 0上进行保存，不需要在每个进程上都进行保存。\n",
    "\n",
    "```python\n",
    "train_sampler.set_epoch(epoch_index)  # 为了让每张卡在每个周期中得到的数据是随机的\n",
    "```\n",
    "\n",
    "终端运行命令:\n",
    "\n",
    "```python\n",
    "python -m torch.distributed.launch --nproc_per_node=2 train.py\n",
    "```\n",
    "\n",
    "两张卡运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "!pip install torchtext #安装指令\n",
    "from torchtext.datasets.imdb import NUM_LINES\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARN,\n",
    "    stream=sys.stdout,\n",
    "    format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = 15000\n",
    "# 第一期： 编写GCNN模型代码\n",
    "class GCNN(nn.Module):\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embedding_dim=64, num_class=2):\n",
    "        super(GCNN, self).__init__()\n",
    "\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding_table.weight)\n",
    "\n",
    "        self.conv_A_1 = nn.Conv1d(embedding_dim, 64, 15, stride=7)\n",
    "        self.conv_B_1 = nn.Conv1d(embedding_dim, 64, 15, stride=7)\n",
    "\n",
    "        self.conv_A_2 = nn.Conv1d(64, 64, 15, stride=7)\n",
    "        self.conv_B_2 = nn.Conv1d(64, 64, 15, stride=7)\n",
    "\n",
    "        self.output_linear1 = nn.Linear(64, 128)\n",
    "        self.output_linear2 = nn.Linear(128, num_class)\n",
    "\n",
    "    def forward(self, word_index):\n",
    "        # 定义GCN网络的算子操作流程，基于句子单词ID输入得到分类logits输出\n",
    "\n",
    "        # 1. 通过word_index得到word_embedding\n",
    "        # word_index shape:[bs, max_seq_len]\n",
    "        word_embedding = self.embedding_table(word_index) #[bs, max_seq_len, embedding_dim]\n",
    "\n",
    "        # 2. 编写第一层1D门卷积模块\n",
    "        word_embedding = word_embedding.transpose(1, 2) #[bs, embedding_dim, max_seq_len]\n",
    "        A = self.conv_A_1(word_embedding)\n",
    "        B = self.conv_B_1(word_embedding)\n",
    "        H = A * torch.sigmoid(B) #[bs, 64, max_seq_len]\n",
    "\n",
    "        A = self.conv_A_2(H)\n",
    "        B = self.conv_B_2(H)\n",
    "        H = A * torch.sigmoid(B) #[bs, 64, max_seq_len]\n",
    "\n",
    "        # 3. 池化并经过全连接层\n",
    "        pool_output = torch.mean(H, dim=-1) #平均池化，得到[bs, 64]\n",
    "        linear1_output = self.output_linear1(pool_output)\n",
    "        logits = self.output_linear2(linear1_output) #[bs, 2]\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    \"\"\" 简单版embeddingbag+DNN模型 \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE, embed_dim=64, num_class=2):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, token_index):\n",
    "        embedded = self.embedding(token_index) # shape: [bs, embedding_dim]\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "\n",
    "# step2 构建IMDB DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def yield_tokens(train_data_iter, tokenizer):\n",
    "    for i, sample in enumerate(train_data_iter):\n",
    "        label, comment = sample\n",
    "        yield tokenizer(comment)\n",
    "\n",
    "train_data_iter = IMDB(root='.data', split='train') # Dataset类型的对象\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data_iter, tokenizer), min_freq=20, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(0)\n",
    "print(f\"单词表大小: {len(vocab)}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\" 对DataLoader所生成的mini-batch进行后处理 \"\"\"\n",
    "    target = []\n",
    "    token_index = []\n",
    "    max_length = 0\n",
    "    for i, (label, comment) in enumerate(batch):\n",
    "        tokens = tokenizer(comment)\n",
    "\n",
    "        token_index.append(vocab(tokens))\n",
    "        if len(tokens) > max_length:\n",
    "            max_length = len(tokens)\n",
    "\n",
    "        if label == \"pos\":\n",
    "            target.append(0)\n",
    "        else:\n",
    "            target.append(1)\n",
    "\n",
    "    token_index = [index + [0]*(max_length-len(index)) for index in token_index]\n",
    "    return (torch.tensor(target).to(torch.int64), torch.tensor(token_index).to(torch.int32))\n",
    "\n",
    "\n",
    "# step3 编写训练代码\n",
    "def train(local_rank, train_dataset, eval_dataset, model, optimizer, num_epoch, log_step_interval, save_step_interval, eval_step_interval, save_path, resume=\"\"):\n",
    "    \"\"\" 此处data_loader是map-style dataset \"\"\"\n",
    "    start_epoch = 0\n",
    "    start_step = 0\n",
    "    if resume != \"\":\n",
    "        #  加载之前训过的模型的参数文件\n",
    "        logging.warning(f\"loading from {resume}\")\n",
    "        checkpoint = torch.load(resume, map_location=torch.device(\"cpu\"))  # 可以是cpu，cuda，cuda：index\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        start_step = checkpoint['step']\n",
    "    \n",
    "    model = nn.parallel.DistributedDataParallel(model.cuda(local_rank), device_ids=[local_rank])  # 模型拷贝，放入DataParallel。\n",
    "    # 这里不需要shuffle，需要一个sample\n",
    "    train_sampler = DistributedSampler(train_dataset)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, sampler=train_sampler)\n",
    "    eval_data_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, sampler=train_sampler)\n",
    "    \n",
    "    for epoch_index in range(start_epoch, num_epoch):\n",
    "        ema_loss = 0.\n",
    "        num_batches = len(train_data_loader)\n",
    "        train_sampler.set_epoch(epoch_index)  # 为了让每张卡在每个周期中得到的数据是随机的\n",
    "\n",
    "        for batch_index, (target, token_index) in enumerate(train_data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            step = num_batches*(epoch_index) + batch_index + 1\n",
    "            \n",
    "            \n",
    "            token_index = token_index.cuda(local_rank)  # 数据拷贝\n",
    "            target = target.cuda(local_rank)  # 数据拷贝\n",
    "            \n",
    "            logits = model(token_index)\n",
    "            \n",
    "            \n",
    "            bce_loss = F.binary_cross_entropy(torch.sigmoid(logits), F.one_hot(target, num_classes=2).to(torch.float32))\n",
    "            ema_loss = 0.9*ema_loss + 0.1*bce_loss\n",
    "            bce_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % log_step_interval == 0:\n",
    "                logging.warning(f\"epoch_index: {epoch_index}, batch_index: {batch_index}, ema_loss: {ema_loss.item()}\")\n",
    "\n",
    "            if step % save_step_interval == 0 and local_rank=0:\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                save_file = os.path.join(save_path, f\"step_{step}.pt\")\n",
    "                torch.save({\n",
    "                    'epoch': epoch_index,\n",
    "                    'step': step,\n",
    "                    'model_state_dict': model.module.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': bce_loss,\n",
    "                }, save_file)\n",
    "                logging.warning(f\"checkpoint has been saved in {save_file}\")\n",
    "\n",
    "            if step % eval_step_interval == 0:\n",
    "                logging.warning(\"start to do evaluation...\")\n",
    "                model.eval()\n",
    "                ema_eval_loss = 0\n",
    "                total_acc_account = 0\n",
    "                total_account = 0\n",
    "                for eval_batch_index, (eval_target, eval_token_index) in enumerate(eval_data_loader):\n",
    "                    total_account += eval_target.shape[0]\n",
    "                    eval_logits = model(eval_token_index)\n",
    "                    total_acc_account += (torch.argmax(eval_logits, dim=-1) == eval_target).sum().item()\n",
    "                    eval_bce_loss = F.binary_cross_entropy(torch.sigmoid(eval_logits), F.one_hot(eval_target, num_classes=2).to(torch.float32))\n",
    "                    ema_eval_loss = 0.9*ema_eval_loss + 0.1*eval_bce_loss\n",
    "                acc = total_acc_account/total_account\n",
    "\n",
    "                logging.warning(f\"eval_ema_loss: {ema_eval_loss.item()}, eval_acc: {acc.item()}\")\n",
    "                model.train()\n",
    "\n",
    "# step4 测试代码\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        logging.warning(\"Cuda is available!\")\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            logging.warning(\"Find {} GPUS\".format(torch.cuda.device_count()))\n",
    "        else:\n",
    "            logging.warning(\"Too Few GPU!\")\n",
    "            return\n",
    "            \n",
    "    else:\n",
    "        logging.warning(\"Cuda is not available!\")\n",
    "        return\n",
    "    \n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--local_rank\", help=\"local device id on current node\", type=int)\n",
    "    args = parser.parse_args()\n",
    "    n_gpus = 2\n",
    "    torch.distributed.init_process_group(\"nccl\", world_size=n_gpus, rank=args.local_rank)\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    \n",
    "    \n",
    "    model = GCNN()\n",
    "    #  model = TextClassificationModel()\n",
    "    print(\"模型总参数:\", sum(p.numel() for p in model.parameters()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    train_data_iter = IMDB(root='.data', split='train') # Dataset类型的对象\n",
    "    train_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(train_data_iter), batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    eval_data_iter = IMDB(root='.data', split='test') # Dataset类型的对象\n",
    "    eval_data_loader = torch.utils.data.DataLoader(to_map_style_dataset(eval_data_iter), batch_size=8, \\\n",
    "                                                   collate_fn=collate_fn)\n",
    "    resume = \"\"\n",
    "\n",
    "    train(args.local_rank, to_map_style_dataset(train_data_iter), to_map_style_dataset(eval_data_iter), \\\n",
    "          model, optimizer, num_epoch=10, log_step_interval=20, save_step_interval=500, eval_step_interval=300, \\\n",
    "          save_path=\"./logs_imdb_text_classification\", resume=resume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec8b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0afedfd5",
   "metadata": {},
   "source": [
    "## 多机多卡及模型并行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0c0e3",
   "metadata": {},
   "source": [
    "多机多卡采用的也是`torch.nn.parallel.DistributedDataParallel`方法编写，代码编写流程于单机多卡中的一致。但是在执行时，需要在多个机器上进行运行，执行命令（以两个节点为例，每个节点处有n_gpus个GPU）。\n",
    "\n",
    "```python\n",
    "python -m torch.distributed.launch --nproc_per_node=n_gpus --nnodes=2 --node_rank=0 --master_add=\"主节点IP\" --master_port=\"主节点端口\" train.py\n",
    "```\n",
    "\n",
    "```python\n",
    "python -m torch.distributed.launch --nproc_per_node=n_gpus --nnodes=2 --node_rank=1 --master_add=\"主节点IP\" --master_port=\"主节点端口\" train.py\n",
    "```\n",
    "\n",
    "参数--nnodes表示机器数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb432d",
   "metadata": {},
   "source": [
    "## 模型并行\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bddd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69a2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1793a38a",
   "metadata": {},
   "source": [
    "## 多GPU训练实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe4225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae149c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "    h3_linear = torch.mm(h2, params[4]) + params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c0347",
   "metadata": {},
   "source": [
    "### 向多个设备分发参数\n",
    "\n",
    "&emsp;&emsp;给定的只有一个模型的参数，它存在于主内存中。在多`GPU`训练的时候，我们需要将参数挪动到`GPU`上才能做计算。`get_params`函数是说给定参数和设备，然后将参数放到某个设备上去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9482bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params, device):\n",
    "    new_params = [p.clone().to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de80bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 weight: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "b1 grad: None\n"
     ]
    }
   ],
   "source": [
    "new_params = get_params(params, d2l.try_gpu(0))  # 尝试将参数放在GPU 0上面。\n",
    "print('b1 weight:', new_params[1])\n",
    "print('b1 grad:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524bda3",
   "metadata": {},
   "source": [
    "### allreduce 函数将所有向量相加\n",
    "\n",
    "&emsp;&emsp;将所有GPU上的data累加起来汇总。比如有4块GPU，将1，2，3块GPU的梯度信息发送到第0块GPU上，计算完成后，再将计算结果再发送回各个GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a7ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allreduce(data):\n",
    "    for i in range(1, len(data)):  # 汇总梯度信息\n",
    "        data[0][:] += data[i].to(data[0].device)\n",
    "    for i in range(1, len(data)):  # 分发梯度信息\n",
    "        data[i] = data[0].to(data[i].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c8475a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before allreduce:\n",
      " tensor([[1., 1.]]) \n",
      " tensor([[2., 2.]])\n",
      "after allreduce:\n",
      " tensor([[3., 3.]]) \n",
      " tensor([[3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\n",
    "print('before allreduce:\\n', data[0], '\\n', data[1])\n",
    "allreduce(data)\n",
    "print('after allreduce:\\n', data[0], '\\n', data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dfdbe",
   "metadata": {},
   "source": [
    "### 小批量数据均匀分布在多个GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3879ec7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch._C' has no attribute '_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x9/xgj8n5xd2bb0cv55w7c72bn40000gn/T/ipykernel_3666/232027417.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdevices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load into'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# None, clearing the cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mscatter_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/parallel/scatter_gather.py\u001b[0m in \u001b[0;36mscatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mScatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_namedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscatter_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Perform CPU to GPU copies in a background stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mstreams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_gpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Synchronize with the copy stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstreams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/nn/parallel/comm.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mdevices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch._C' has no attribute '_scatter'"
     ]
    }
   ],
   "source": [
    "data = torch.arange(20).reshape(4, 5)\n",
    "devices = [torch.device('cuda:0'), torch.device('cuda:1')]\n",
    "split = nn.parallel.scatter(data, devices)\n",
    "print('input :', data)\n",
    "print('load into', devices)\n",
    "print('output:', split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18c1ea",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果数据不能被整除的话，那么最后一个GPU得到的数据就会少一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1586a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(X, y, devices):\n",
    "    \"\"\"将`X`和`y`拆分到多个设备上\"\"\"\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (nn.parallel.scatter(X, devices),\n",
    "            nn.parallel.scatter(y, devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff823f",
   "metadata": {},
   "source": [
    "### 在一个小批量上实现多GPU训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee28ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, device_params, devices, lr):\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    # 在每个GPU上分别计算损失\n",
    "    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n",
    "          for X_shard, y_shard, device_W in zip(\n",
    "              X_shards, y_shards, device_params)]\n",
    "    for l in ls:  # 反向传播在每个GPU上分别执行\n",
    "        l.backward()\n",
    "    # 将每个GPU的所有梯度相加，并将其广播到所有GPU\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce([device_params[c][i].grad for c in range(len(devices))])\n",
    "    # 在每个GPU上分别更新模型参数\n",
    "    for param in device_params:\n",
    "        d2l.sgd(param, lr, X.shape[0]) # 在这里，我们使用全尺寸的小批量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1be5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    # 将模型参数复制到`num_gpus`个GPU\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    num_epochs = 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    timer = d2l.Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            # 为单个小批量执行多GPU训练\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize()\n",
    "        timer.stop()\n",
    "        # 在GPU 0上评估模型\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n",
    "            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51967ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=2, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b815bb3",
   "metadata": {},
   "source": [
    "### 多GPU的简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37730712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"稍加修改的 ResNet-18 模型。\"\"\"\n",
    "    def resnet_block(in_channels, out_channels, num_residuals,\n",
    "                     first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(d2l.Residual(in_channels, out_channels,\n",
    "                                        use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(d2l.Residual(out_channels, out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层。\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU())\n",
    "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(nn.Flatten(),\n",
    "                                       nn.Linear(512, num_classes)))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18(10)\n",
    "# 获取GPU列表\n",
    "devices = d2l.try_all_gpus()\n",
    "# 我们将在训练代码实现中初始化网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "    # 在多个 GPU 上设置模型\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebcfe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=1, batch_size=256, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=2, batch_size=512, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f5aa7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd2118eb",
   "metadata": {},
   "source": [
    "## 分布式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40ae7e",
   "metadata": {},
   "source": [
    "数据并行的分布式和单机多卡的方式并没有本质的区别。\n",
    "\n",
    "分布式系统中，数据是放在分布式文件系统上。所有的机器都可以分开读取存放在不同磁盘上的样本。有多台机器、每台机器里面有多个GPU，每个机器叫做worker。\n",
    "\n",
    "对于多个worker首先需要读取数据，与之前不同的是，现在是通过网络将数据读取进来。收发梯度的时候也是通过网络，从一台机器上跑到另外一台机器上。\n",
    "\n",
    "- [http://zh-v2.d2l.ai/chapter_computational-performance/parameterserver.html](http://zh-v2.d2l.ai/chapter_computational-performance/parameterserver.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993be32",
   "metadata": {},
   "source": [
    "分布式同步数据并行是多`GPU`数据并行在多台机器上的扩展，网络通讯通常是瓶颈。为了考虑通信，往往采用大的`Batch_Size`，需要注意使用特别大的批量大小时，同时要考虑收敛效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578641f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d5956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5c23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8bf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5113d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1dad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc537ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49daa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
