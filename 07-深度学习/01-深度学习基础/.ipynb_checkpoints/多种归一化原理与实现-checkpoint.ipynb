{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3a6d19",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "\n",
    "`Batch Normalization`做的是通道级别的归一化，贯穿整个`mini-batch`。`per channel across mini-batch`。\n",
    "\n",
    "`torch.nn.BatchNorm1d()`\n",
    "\n",
    "`torch.nn.BatchNorm2d()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ee640",
   "metadata": {},
   "source": [
    "### torch.nn.BatchNorm1d API介绍\n",
    "\n",
    "\n",
    "`PyTorch`中`torch.nn.BatchNorm1d`的官网地址为:[torch.nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html?highlight=batchnorm#torch.nn.BatchNorm1d)\n",
    "\n",
    "\n",
    "$$\n",
    "y=\\frac{x-\\mathrm{E}[x]}{\\sqrt{\\operatorname{Var}[x]+\\epsilon}} * \\gamma+\\beta\n",
    "$$\n",
    "\n",
    "均值和方差是在`mini-batches`上的每个维度上实现的。$\\gamma$和$\\beta$是大小为$C$的可学习参数，其中$C$是输入的特征维度，默认$\\gamma=1$，$\\beta=0$。标准差的计算是有偏估计`biased estimator`的，与`torch.var(input, unbiased=False)`的计算方式相同。\n",
    "\n",
    "其`API`如下所示：\n",
    "\n",
    "```python\n",
    "torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "参数解释：\n",
    "\n",
    "1. `num_features`：输入数据的特征数。\n",
    "2. `eps`: 用于保障数值计算稳定性的一个值。`Default: 1e-5`。\n",
    "3. `momentum`: 用于计算移动平均，$\\hat{x}_{new} = (1-momentum) \\times \\hat{x} + momentum \\times \\hat{x}_{t}$。\n",
    "4. `affine`: 设置为`True`的话，将会具有可学习参数，默认为`True`。\n",
    "5. `track_running_stats`: 设置为`True`时，将会跟踪记录`mean`和`variance`，设置为`False`时，将不会跟踪此统计量。默认为`True`。\n",
    "\n",
    "\n",
    "输入数据的`shape`可以为$(N, C)$或者$(N, C, L)$，其中$N$是`batch size`，$C$是特征数，或者称之为通道数，$L$是序列长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d526748",
   "metadata": {},
   "source": [
    "### 手动实现并验证BatchNorm1d \n",
    "\n",
    "设置`batch_size = 2`, 序列长度`time_steps = 3`, 编码维度`embedding_dim = 4`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ab6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62e41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "time_steps = 3\n",
    "embedding_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa9af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.randn(batch_size, time_steps, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4f946",
   "metadata": {},
   "source": [
    "此时输入数据的维度为$(N, L, C)$,与`PyTorch`官网中要求的$(N, C, L)$不一致，所以我们需要将其转置成`PyTorch`中需要的格式，处理完之后再转换回来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd9288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4825, -0.2434,  0.9296,  0.4209],\n",
      "         [-0.9038, -0.0946, -0.6311, -0.7476],\n",
      "         [ 0.4411, -0.0744,  0.0289, -1.4386]],\n",
      "\n",
      "        [[ 1.2614,  2.0340, -1.8445,  1.7335],\n",
      "         [ 1.0898, -1.2977,  1.0617, -0.2914],\n",
      "         [-1.4060, -0.3239,  0.4553,  0.3231]]])\n"
     ]
    }
   ],
   "source": [
    "batch_norm_op = nn.BatchNorm1d(embedding_dim, affine=False)\n",
    "batch_norm_out = batch_norm_op(input_data.transpose(-2, -1)).transpose(-2, -1)\n",
    "print(batch_norm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65997fc7",
   "metadata": {},
   "source": [
    "之后，我们手写一下来验证其计算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb72793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4825, -0.2434,  0.9296,  0.4209],\n",
      "         [-0.9038, -0.0946, -0.6311, -0.7476],\n",
      "         [ 0.4411, -0.0744,  0.0289, -1.4386]],\n",
      "\n",
      "        [[ 1.2614,  2.0340, -1.8445,  1.7335],\n",
      "         [ 1.0898, -1.2977,  1.0617, -0.2914],\n",
      "         [-1.4060, -0.3239,  0.4553,  0.3231]]])\n"
     ]
    }
   ],
   "source": [
    "bn_mean = input_data.mean(dim=(0, 1), keepdim=True)\n",
    "bn_std = input_data.std(dim=(0, 1), unbiased=False, keepdim=True)\n",
    "verify_out = (input_data - bn_mean) / (bn_std + 1e-5)\n",
    "print(verify_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8080c",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "[Layer Normalization](https://arxiv.org/abs/1607.06450)的论文。\n",
    "\n",
    "`torch.nn.LayerNorm()`\n",
    "\n",
    "`LayerNorm`是对每个`sample`，每一层都做归一化处理。在`NLP`中一般用`Layer Normal`。因为`Batch Normalization`处理的话，对于可变序列长度并不好处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0097a5",
   "metadata": {},
   "source": [
    "### torch.nn.LayerNorm API介绍\n",
    "\n",
    "其官方API连接：[https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html?highlight=layernorm#torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html?highlight=layernorm#torch.nn.LayerNorm)\n",
    "\n",
    "其`API`如下所示：\n",
    "\n",
    "```python\n",
    "torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "$$\n",
    "y=\\frac{x-\\mathrm{E}[x]}{\\sqrt{\\operatorname{Var}[x]+\\epsilon}} * \\gamma+\\beta\n",
    "$$\n",
    "\n",
    "参数解释：\n",
    "\n",
    "1. `normalized_shape`：期望做归一化的维度(`int or list or torch.Size`)，期望的输入大小是$[* \\times \\text { normalized_shape }[0] \\times \\text { normalized_shape }[1] \\times \\ldots  \\text { normalized_shape }[-1]]$\n",
    "2. `eps`: 用于维持数值稳定性的很小的值，默认为`1e-5`。\n",
    "3. `elementwise_affine`: 设置为`True`时，将会有可学习的参数。\n",
    "\n",
    "\n",
    "输入数据的维度为$(N, *)$，输出数据的维度同样为$(N, *)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68cdd7",
   "metadata": {},
   "source": [
    "### 手动实现并验证LayerNorm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b7ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2967, -0.5167,  1.3580,  0.4553],\n",
      "         [-1.5035,  1.1911, -0.2123,  0.5247],\n",
      "         [ 0.4726,  0.3426,  0.8821, -1.6973]],\n",
      "\n",
      "        [[ 0.1560,  0.8059, -1.6775,  0.7156],\n",
      "         [ 0.3657, -1.4409,  1.3160, -0.2408],\n",
      "         [-1.5923, -0.0989,  0.9564,  0.7348]]])\n"
     ]
    }
   ],
   "source": [
    "layer_norm_op = nn.LayerNorm(embedding_dim, elementwise_affine=False)\n",
    "layer_norm_out = layer_norm_op(input_data)\n",
    "print(layer_norm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b7b0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.2967, -0.5167,  1.3580,  0.4553],\n",
      "         [-1.5036,  1.1911, -0.2123,  0.5247],\n",
      "         [ 0.4726,  0.3426,  0.8822, -1.6973]],\n",
      "\n",
      "        [[ 0.1560,  0.8059, -1.6775,  0.7156],\n",
      "         [ 0.3657, -1.4409,  1.3160, -0.2408],\n",
      "         [-1.5923, -0.0989,  0.9564,  0.7348]]])\n"
     ]
    }
   ],
   "source": [
    "ln_mean = input_data.mean(dim=-1, keepdim=True)\n",
    "ln_std = input_data.std(dim=-1, keepdim=True, unbiased=False)\n",
    "verify_out = (input_data - ln_mean) / (ln_std + 1e-5)\n",
    "print(verify_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ee610",
   "metadata": {},
   "source": [
    "## Instance Normalization\n",
    "\n",
    "`torch.nn.InstanceNorm1d()`\n",
    "\n",
    "`torch.nn.InstanceNorm2d()`\n",
    "\n",
    "### torch.nn.InstanceNorm1d API介绍\n",
    "\n",
    "实例归一化一般用于风格迁移上。用一句话概括的话就是`per sample`，`per channel`。也就是对于每个样本的每个维度单独去计算均值和方差。\n",
    "\n",
    "\n",
    "其官网API链接地址[https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html?highlight=instancenorm#torch.nn.InstanceNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html?highlight=instancenorm#torch.nn.InstanceNorm1d)\n",
    "\n",
    "\n",
    "其函数原型如下所示：\n",
    "\n",
    "```python\n",
    "torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "默认情况下是没有可学习参数的。\n",
    "\n",
    "输入数据的维度为$(N, C, L)$或者是$(C, L)$。输出数据的维度同样为$(N, C, L)$或者是$(C, L)$\n",
    "\n",
    "### 手动实现并验证InstanceNorm1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ebf0197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2980, -1.4041,  1.2826,  1.3152],\n",
      "         [-1.0482,  0.5681, -1.1572, -0.2074],\n",
      "         [ 1.3462,  0.8360, -0.1254, -1.1078]],\n",
      "\n",
      "        [[ 0.7763,  1.3559, -1.3863,  1.3508],\n",
      "         [ 0.6355, -1.0260,  0.9354, -1.0379],\n",
      "         [-1.4119, -0.3298,  0.4509, -0.3129]]])\n"
     ]
    }
   ],
   "source": [
    "ins_norm_op = nn.InstanceNorm1d(embedding_dim)\n",
    "ins_norm_out = ins_norm_op(input_data.transpose(-2, -1)).transpose(-2, -1)\n",
    "print(ins_norm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcedcb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2980, -1.4055,  1.2826,  1.3152],\n",
      "         [-1.0482,  0.5687, -1.1572, -0.2074],\n",
      "         [ 1.3462,  0.8368, -0.1254, -1.1078]],\n",
      "\n",
      "        [[ 0.7763,  1.3559, -1.3863,  1.3508],\n",
      "         [ 0.6355, -1.0260,  0.9353, -1.0379],\n",
      "         [-1.4119, -0.3298,  0.4509, -0.3129]]])\n"
     ]
    }
   ],
   "source": [
    "ins_mean = input_data.mean(dim=1, keepdim=True)\n",
    "ins_std = input_data.std(dim=1, keepdim=True, unbiased=False)\n",
    "verify_out = (input_data - ins_mean) / (ins_std + 1e-5)\n",
    "print(verify_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fbca9",
   "metadata": {},
   "source": [
    "可以看到它是对每个样本上的，不同序列长度上不同维度上做的归一化。对于`NLP`任务上，也就是对每个时刻上都有的东西消除掉了，也就是把一段文本的风格消除掉了。对于语音序列的话，就是将说话人的身份消除掉了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50af0d9",
   "metadata": {},
   "source": [
    "## Group Normalization\n",
    "\n",
    "`torch.nn.GroupNorm()`\n",
    "\n",
    "其官网`API`链接为: [https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html?highlight=groupnorm#torch.nn.GroupNorm](https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html?highlight=groupnorm#torch.nn.GroupNorm)\n",
    "\n",
    "### torch.nn.GroupNorm API介绍\n",
    "\n",
    "其做的事情是`per sample`，`per group`。其与`Layer Normalization`最像，但是需要先将特征`group`，也是一个与`batch size`无关的归一化。\n",
    "\n",
    "函数原型为：\n",
    "\n",
    "```python\n",
    "torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "其参数有：\n",
    "\n",
    "1. `num_groups` (int)：需要划分的`groups`的数目。\n",
    "2. `eps`：用于维持数值稳定性，默认值为: `1e-5`。\n",
    "3. `affine`：是否设置可学习参数，默认为`True`。\n",
    "\n",
    "输入数据的维度为$(N, C, *)$，其中$C$为通道数。输出数据的维度同样为$(N, C, *)$。\n",
    "\n",
    "### 手动实现并验证GroupNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291263f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9486,  0.4206,  1.6924,  0.7353],\n",
      "         [-1.7732,  0.7022, -0.9040, -0.4951],\n",
      "         [ 0.8587,  0.7404,  0.1941, -1.2227]],\n",
      "\n",
      "        [[ 0.6266,  1.5210, -2.0115,  1.0052],\n",
      "         [ 0.4940, -0.9675,  0.8824, -0.2710],\n",
      "         [-1.4339, -0.2401,  0.2786,  0.1163]]])\n"
     ]
    }
   ],
   "source": [
    "num_group = 2\n",
    "group_norm_op = nn.GroupNorm(num_group, embedding_dim, affine=False)\n",
    "group_norm_out = group_norm_op(input_data.transpose(-2, -1)).transpose(-2, -1)\n",
    "print(group_norm_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1418312",
   "metadata": {},
   "source": [
    "`group norm`手写稍微麻烦一点，首先需要将输入划分成`group`组，之后再做归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "792275ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9486,  0.4206,  1.6924,  0.7353],\n",
      "         [-1.7732,  0.7022, -0.9040, -0.4951],\n",
      "         [ 0.8587,  0.7404,  0.1941, -1.2227]],\n",
      "\n",
      "        [[ 0.6266,  1.5210, -2.0115,  1.0052],\n",
      "         [ 0.4940, -0.9675,  0.8824, -0.2710],\n",
      "         [-1.4339, -0.2401,  0.2786,  0.1163]]])\n"
     ]
    }
   ],
   "source": [
    "group_input_datas = torch.split(input_data, split_size_or_sections = embedding_dim // num_group, dim=-1)\n",
    "\n",
    "results = []\n",
    "for g_input_data in group_input_datas:\n",
    "    gn_mean = g_input_data.mean(dim=(1, 2), keepdim=True)\n",
    "    gn_std = g_input_data.std(dim=(1, 2), keepdim=True, unbiased=False)\n",
    "    gn_result = (g_input_data - gn_mean) / (gn_std + 1e-5)\n",
    "    results.append(gn_result)\n",
    "verify_out = torch.cat(results, dim=-1)\n",
    "print(verify_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792cae2",
   "metadata": {},
   "source": [
    "## Weight Normalization\n",
    "\n",
    "\n",
    "之前的输入都是对输入处理做归一化，权重归一化是对权重进行归一化的。\n",
    "\n",
    "torch.nn.utils.weight_norm()\n",
    "\n",
    "\n",
    "### torch.nn.utils.weight_norm API介绍\n",
    "\n",
    "这里用到的是一个函数，其函数原型是:\n",
    "\n",
    "```python\n",
    "torch.nn.utils.weight_norm(module, name='weight', dim=0)\n",
    "```\n",
    "\n",
    "只不过这里将`module`包起来，返回的值仍然是`module`。我们会对参数做如下变化\n",
    "\n",
    "$$\n",
    "\\mathbf{w}=g \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}\n",
    "$$\n",
    "\n",
    "参数:\n",
    "\n",
    "1. `module (Module)`: 包含的`module`。\n",
    "2. `name` (str, optional): 权重的名字。\n",
    "3. `dim` (int, optional): 需要去计算norm的维度。\n",
    "\n",
    "官网`API`链接为: [https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html?highlight=weight%20norm#torch.nn.utils.weight_norm](https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html?highlight=weight%20norm#torch.nn.utils.weight_norm)\n",
    "\n",
    "### 手动实现并验证weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6b977c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2556, -0.1400,  0.5388],\n",
      "         [ 0.1185,  0.5188,  0.0871],\n",
      "         [ 0.1729,  0.0255,  0.0408]],\n",
      "\n",
      "        [[-0.1160, -0.2442, -0.6752],\n",
      "         [-0.2803, -0.2215,  0.2214],\n",
      "         [ 0.2664,  0.2560,  0.5585]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(embedding_dim, 3, bias=False)\n",
    "wn_linear = torch.nn.utils.weight_norm(linear)\n",
    "wn_linear_output = wn_linear(input_data)\n",
    "print(wn_linear_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe8f6b",
   "metadata": {},
   "source": [
    "之后我们手写一下`weight norm`。这里只计算方向，不计算幅值，幅值采用weight_norm的weight_g。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12a0a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2556, -0.1400,  0.5388],\n",
      "         [ 0.1185,  0.5188,  0.0871],\n",
      "         [ 0.1729,  0.0255,  0.0408]],\n",
      "\n",
      "        [[-0.1160, -0.2442, -0.6752],\n",
      "         [-0.2803, -0.2215,  0.2214],\n",
      "         [ 0.2664,  0.2560,  0.5585]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#weight_direction = linear.weight / (linear.weight.norm(dim=1, keepdim=True))\n",
    "weight_direction = linear.weight/(linear.weight.norm(dim=1, keepdim=True))\n",
    "weight_magnitude = torch.tensor([linear.weight[i, :].norm() for i in torch.arange(linear.weight.shape[0])], dtype=torch.float32).unsqueeze(-1)\n",
    "verify_wn_linear_output = input_data @ (weight_direction.transpose(-1, -2)) * (weight_magnitude.transpose(-1, -2))\n",
    "print(verify_wn_linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5f941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
