{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1d271b",
   "metadata": {},
   "source": [
    "## 循环神经网络 RNN\n",
    "\n",
    "&emsp;&emsp;在潜变量自回归模型中，我们采用潜变量$h_{t}$总结过去的信息。\n",
    "\n",
    "$$\n",
    "p(h_{t} | h_{t-1}, x_{t-1}) \\\\\n",
    "p(x_{t} | h_{t},x_{t-1})\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;接下来就是说如何把这个模型做成`RNN`:\n",
    "\n",
    "1. 针对$p(h_{t} | h_{t-1}, x_{t-1})$这一项, 我们可以建模成：\n",
    "\n",
    "$$\n",
    "h_{t} = \\tanh(W_{hh}h_{t-1} + W_{hx}x_{t-1} + b_{h})\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;上式中，如果去掉$W_{hh}h_{t-1}$这一项的话，就会退化成`MLP`了。\n",
    "\n",
    "2. 针对$p(x_{t} | h_{t},x_{t-1})$这一项，我们的输出可以描述为:\n",
    "\n",
    "$$\n",
    "o_{t} = \\phi(W_{ho}h_{t} + b_{o})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07955f",
   "metadata": {},
   "source": [
    "## torch.nn.RNN\n",
    "\n",
    "在`PyTorch`中，`RNN`的`API`地址为：[torch.nn.RNN(*args, **kwargs)](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN)\n",
    "\n",
    "其需要的参数有:\n",
    "\n",
    "1. `input_size`: 输入数据$x$的特征大小。\n",
    "2. `hidden_size`: 隐藏状态$h$的特征大小。\n",
    "3. `num_layers`: 默认只有一层，可以通过改变`num_layers`改变层数的大小。\n",
    "4. `nonlinearity`: 非线性激活函数，可以为`tanh`或者`relu`，默认为`tanh`。\n",
    "5. `bias`: 如果设置为`False`的话，就没有`bias`，默认为`True`。\n",
    "6. `batch_first`: 若设置为`True`，则提供的数据维度为`(batch, seq, feature)`，否则为`(seq, batch, feature)`，默认为`False`。\n",
    "7. `dropout`：如果为非`0`的值，则会设置`dropout`，默认为`0`。\n",
    "8. `bidirectional`: 是否为双向。若为双向的结构，则输出的结果大小为 `2 * hidden_size`。\n",
    "\n",
    "\n",
    "实例化之后，输入数据有两部分：`input,h_0`。输入数据维度为：$(L, N, H_{in})$, 如果`batch_first=False`, 则维度为：$(N, L, H_{in})$。`h_0`的数据`shape`为`(D * num_layers, N, H_out)`。\n",
    "\n",
    "其中：$N=batch\\ size, L=sequence\\ length, D = 2 \\ if\\ bidrectional=True,\\  otherwise = 1$, $H_{in}=input\\_size, H_{out}=hidden\\_size$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47085625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "bs, T = 2, 3 # 批大小，输入序列长度\n",
    "input_size, hidden_size = 2, 3  # 输入特征大小，隐含层特征大小\n",
    "\n",
    "input_data = torch.randn(bs, T, input_size)  # 随机初始化输入特征数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90820ecd",
   "metadata": {},
   "source": [
    "### 单向，单层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0a660",
   "metadata": {},
   "source": [
    "#### 双向，单层的API调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e11890a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0658, -0.7007, -0.7684],\n",
      "         [-0.7843, -0.8803, -0.1957],\n",
      "         [-0.7789, -0.8376, -0.1086]],\n",
      "\n",
      "        [[-0.1183, -0.9732, -0.8664],\n",
      "         [-0.7132, -0.8307, -0.3912],\n",
      "         [-0.5061, -0.8704, -0.5720]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.7789, -0.8376, -0.1086],\n",
      "         [-0.5061, -0.8704, -0.5720]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h_prev = torch.zeros(bs, hidden_size)  # 初始化隐含状态\n",
    "single_rnn = nn.RNN(input_size, hidden_size, batch_first=True)  # 输入特征维度为4， 输出特征维度为3。\n",
    "output, hn = single_rnn(input_data, h_prev.unsqueeze(0))  # 不指定h_0的话，则默认会给0。\n",
    "print(output)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e6147",
   "metadata": {},
   "source": [
    "从上述输出结果可以看到，输出结果为`batch size`为`1`，序列长度为`2`， 每个序列的输出的维度都是`3`，最后一个序列的输出就是`hn`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a199ee",
   "metadata": {},
   "source": [
    "#### 双向，单层RNN前向传播理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab8f6ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(input_data, weight_ih, bias_ih, weight_hh, bias_hh, h_prev):\n",
    "    bs, T, input_size = input_data.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim)  # 初始化一个输出(状态)矩阵\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input_data[:, t, :].unsqueeze(2)  # 获取当前时刻输入特征， bs * input_size * 1\n",
    "        \n",
    "        w_ih_batch = weight_ih.unsqueeze(0).tile(bs, 1, 1)  # bs * h_dim * input_size\n",
    "        w_hh_batch = weight_hh.unsqueeze(0).tile(bs, 1, 1)  # bs * h_dim * h_dim\n",
    "        \n",
    "        w_times_x = torch.bmm(w_ih_batch, x).squeeze(-1)  # bs * h_dim\n",
    "        w_times_h = torch.bmm(w_hh_batch, h_prev.unsqueeze(2)).squeeze(-1)  # bs * h_dim\n",
    "        \n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "        \n",
    "        h_out[:,t,:] = h_prev\n",
    "    \n",
    "    return h_out, h_prev.unsqueeze(0)  # 单向单层的，但是官方给的是三维的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a1e8484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.0172,  0.3831],\n",
      "        [ 0.5762, -0.0787],\n",
      "        [ 0.1433, -0.4506]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.1881,  0.3242, -0.2890],\n",
      "        [ 0.2778,  0.1097,  0.4763],\n",
      "        [-0.0576,  0.1187,  0.2664]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([-0.2626, -0.4763, -0.2100], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.4453, -0.2321, -0.0434], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k, v in single_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb04ea9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "batch2 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output, hn \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih_l0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih_l0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msingle_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh_l0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_rnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh_l0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_prev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(hn)\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mrnn_forward\u001b[0;34m(input_data, weight_ih, bias_ih, weight_hh, bias_hh, h_prev)\u001b[0m\n\u001b[1;32m     10\u001b[0m w_hh_batch \u001b[38;5;241m=\u001b[39m weight_hh\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtile(bs, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# bs * h_dim * h_dim\u001b[39;00m\n\u001b[1;32m     12\u001b[0m w_times_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(w_ih_batch, x)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# bs * h_dim\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m w_times_h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_hh_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_prev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# bs * h_dim\u001b[39;00m\n\u001b[1;32m     15\u001b[0m h_prev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(w_times_x \u001b[38;5;241m+\u001b[39m bias_ih \u001b[38;5;241m+\u001b[39m w_times_h \u001b[38;5;241m+\u001b[39m bias_hh)\n\u001b[1;32m     17\u001b[0m h_out[:,t,:] \u001b[38;5;241m=\u001b[39m h_prev\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch2 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "output, hn = rnn_forward(input_data, single_rnn.weight_ih_l0, single_rnn.bias_ih_l0, \\\n",
    "                         single_rnn.weight_hh_l0, single_rnn.bias_hh_l0, h_prev)\n",
    "print(output)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13b83b",
   "metadata": {},
   "source": [
    "可以看到输出结果与官方`API`的调用结果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17554cc",
   "metadata": {},
   "source": [
    "### 双向，单层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11059437",
   "metadata": {},
   "source": [
    "#### 双向，单层的API调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d77a61b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2765,  0.2758, -0.1308,  0.0694, -0.5915,  0.9058],\n",
      "         [-0.2322, -0.4363, -0.6479,  0.3023, -0.2813,  0.6778],\n",
      "         [-0.5839, -0.5484, -0.3730,  0.0880, -0.4876,  0.6034]],\n",
      "\n",
      "        [[ 0.0128, -0.5697, -0.5231,  0.2691, -0.1512,  0.9403],\n",
      "         [-0.6673, -0.4781, -0.3145,  0.3161, -0.4270,  0.7228],\n",
      "         [-0.2692, -0.1458,  0.0146,  0.0541, -0.5821,  0.7424]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.5839, -0.5484, -0.3730],\n",
      "         [-0.2692, -0.1458,  0.0146]],\n",
      "\n",
      "        [[ 0.0694, -0.5915,  0.9058],\n",
      "         [ 0.2691, -0.1512,  0.9403]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bidirectional_rnn = nn.RNN(input_size, hidden_size, batch_first=True, bidirectional=True)  # 输入特征维度为4， 输出特征维度为3， 1层。\n",
    "h_prev = torch.zeros(2, bs, hidden_size)  # 初始化隐含状态\n",
    "bi_output, bi_hn = bidirectional_rnn(input_data, h_prev)  # 不指定h_0的话，则默认会给0。\n",
    "print(bi_output)\n",
    "print(bi_hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10f2c5",
   "metadata": {},
   "source": [
    "#### 双向，单层RNN前向传播理解\n",
    "\n",
    "双向`RNN`的前向计算过程是什么样的呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36af0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_rnn_forward(input_data, weight_ih, bias_ih, weight_hh, bias_hh, h_prev,\\\n",
    "                             weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, \\\n",
    "                              h_prev_reverse):\n",
    "    \n",
    "    bs, T, input_size = input_data.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim * 2)  # 初始化一个输出(状态)矩阵, 双向是两倍特征大小。\n",
    "    \n",
    "    forward_output = rnn_forward(input_data, weight_ih, bias_ih, weight_hh, bias_hh, h_prev)[0]\n",
    "    \n",
    "    backward_output = rnn_forward(torch.flip(input_data, [1]), weight_ih_reverse, weight_hh_reverse, \\\n",
    "                                  bias_ih_reverse, bias_hh_reverse, h_prev_reverse)[0]  # backward layer\n",
    "    \n",
    "    h_out[:, :, :h_dim] = forward_output\n",
    "    h_out[:, :, h_dim:] = backward_output\n",
    "\n",
    "    return h_out, h_out[:, -1, :].reshape((bs, 2, h_dim)).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceda86d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.1343,  0.0687],\n",
      "        [ 0.4123,  0.2929],\n",
      "        [ 0.1976,  0.2232]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.4889,  0.2893,  0.3699],\n",
      "        [-0.2880,  0.4017,  0.0363],\n",
      "        [-0.3799, -0.3433, -0.0959]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.0991, -0.4202, -0.0296], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.5049,  0.2322, -0.4667], requires_grad=True)\n",
      "weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[-0.0778, -0.0266],\n",
      "        [-0.2008, -0.1091],\n",
      "        [-0.1094,  0.2378]], requires_grad=True)\n",
      "weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[-0.5555, -0.1929,  0.2554],\n",
      "        [ 0.5047, -0.5450, -0.2321],\n",
      "        [ 0.3033,  0.1103,  0.3149]], requires_grad=True)\n",
      "bias_ih_l0_reverse Parameter containing:\n",
      "tensor([-0.3708, -0.2364,  0.4141], requires_grad=True)\n",
      "bias_hh_l0_reverse Parameter containing:\n",
      "tensor([ 0.4217, -0.4190,  0.4108], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k, v in bidirectional_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad5a8b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2765,  0.2758, -0.1308,  0.0880, -0.4876,  0.6034],\n",
      "         [-0.2322, -0.4363, -0.6479,  0.3023, -0.2813,  0.6778],\n",
      "         [-0.5839, -0.5484, -0.3730,  0.0694, -0.5915,  0.9058]],\n",
      "\n",
      "        [[ 0.0128, -0.5697, -0.5231,  0.0541, -0.5821,  0.7424],\n",
      "         [-0.6673, -0.4781, -0.3145,  0.3161, -0.4270,  0.7228],\n",
      "         [-0.2692, -0.1458,  0.0146,  0.2691, -0.1512,  0.9403]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[-0.5839, -0.5484, -0.3730],\n",
      "         [-0.2692, -0.1458,  0.0146]],\n",
      "\n",
      "        [[ 0.0694, -0.5915,  0.9058],\n",
      "         [ 0.2691, -0.1512,  0.9403]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output,hn = bidirectional_rnn_forward(input_data, bidirectional_rnn.weight_ih_l0, bidirectional_rnn.bias_ih_l0,\\\n",
    "                         bidirectional_rnn.weight_hh_l0, bidirectional_rnn.bias_hh_l0, h_prev[0], \\\n",
    "                         bidirectional_rnn.weight_ih_l0_reverse, bidirectional_rnn.bias_ih_l0_reverse, \\\n",
    "                         bidirectional_rnn.weight_hh_l0_reverse, bidirectional_rnn.bias_hh_l0_reverse, h_prev[1])\n",
    "print(output)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171baec",
   "metadata": {},
   "source": [
    "### RNNCell\n",
    "\n",
    "`RNN`其实就是多次的`RNNCell`的计算。\n",
    "\n",
    "函数原型为:[torch.nn.RNNCell](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html?highlight=rnncell#torch.nn.RNNCell)\n",
    "\n",
    "```python\n",
    "CLASS torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', device=None, dtype=None)\n",
    "```\n",
    "\n",
    "举例为:\n",
    "\n",
    "```python\n",
    "rnn = nn.RNNCell(10, 20)\n",
    "input = torch.randn(6, 3, 10)\n",
    "hx = torch.randn(3, 20)\n",
    "output = []\n",
    "for i in range(6):\n",
    "    hx = rnn(input[i], hx)\n",
    "    output.append(hx)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175f853",
   "metadata": {},
   "source": [
    "## Vanilla-RNN-代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb6358",
   "metadata": {},
   "source": [
    "### Numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d1c3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()\n",
    "\n",
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read() # 将文件中的所有数据读取进来。\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    \n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_control(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e6cbb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len text:  15294\n",
      "len text_vocab:  75\n",
      "one_hot shape:  (15294, 75)\n"
     ]
    }
   ],
   "source": [
    "text, text_vocab = get_vocab('../consumer.h', lower = False)\n",
    "one_hot = embed_to_control(text, text_vocab)\n",
    "print('len text: ', len(text))\n",
    "print('len text_vocab: ', len(text_vocab))\n",
    "print('one_hot shape: ', one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab9cc1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`tanh`激活函数为:\n",
    "\n",
    "$$\n",
    "\\tanh x=\\frac{\\sinh x}{\\cosh x}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;导数为:\n",
    "\n",
    "$$\n",
    "(\\tanh x)^{\\prime}=\\operatorname{sech}^{2} x=1-\\tanh ^{2} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57574421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bae637",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{t} = \\tanh(W_{hh}h_{t-1} + W_{hx}x_{t-1} + b_{h})\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_{t} = \\phi(W_{ho}h_{t} + b_{o})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6fdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_rnn_recurrent(x, prev_state, W_hx, W_hh, W_ho):\n",
    "    mul_hx = np.dot(x, W_hx.T)\n",
    "    \n",
    "    # 处理隐藏状态部分。\n",
    "    mul_hh = np.dot(prev_state, W_hh.T)\n",
    "    add_previous_now = mul_hx + mul_hh\n",
    "    current_state = tanh(add_previous_now)\n",
    "    \n",
    "    # 处理输出部分。\n",
    "    mul_o = np.dot(current_state, W_ho.T)\n",
    "    return (mul_hx, mul_hh, add_previous_now, current_state, mul_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11728e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: np.max(x)取的是二维数组x中的最大值。\n",
    "    \"\"\"\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c168f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;多分类的交叉熵损失如下:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{N} \\sum_{i} L_{i}=-\\frac{1}{N} \\sum_{i} \\sum_{c=1}^{M} y_{i c} \\log \\left(p_{i c}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中$M$表示类别的数量，$y_{ic}$是符号函数(0或者1), 如果样本$i$的真实类别等于$c$取1，否者取0。$p_{ic}$表示观测样本$i$属于类别$c$的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1b94430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat + 1e-9))) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74b2b0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`RNN`的反向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2ea97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_multiply_gate(w, x, dz):\n",
    "    \"\"\"\n",
    "    w shape = (75, 128)\n",
    "    x shape = (64, 128)\n",
    "    dz shape = (64, 75)\n",
    "    \"\"\"\n",
    "    dw = np.dot(dz.T, x) # shape = (75, 128)\n",
    "    dx = np.dot(w.T, dz.T) # shape = (128, 64)\n",
    "    return dw, dx\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2\n",
    "\n",
    "def backward_rnn_recurrent(x, prev_state, W_hx, W_hh, W_ho, d_mu_o, saved_graph):\n",
    "    mul_hx, mul_hh, add_previous_now, current_state, mul_o = saved_graph\n",
    "    \n",
    "    dW_ho, d_CurrentState = backward_multiply_gate(W_ho, current_state, d_mu_o)\n",
    "    \n",
    "    dadd_previous_now = tanh(add_previous_now, True) * d_CurrentState.T\n",
    "    \n",
    "    dmul_hh, dmul_hx = backward_add_gate(mul_hh, mul_hx, dadd_previous_now)\n",
    "    dW_hh, dprev_state = backward_multiply_gate(W_hh, prev_state, dmul_hh)\n",
    "    dW_hx, dx = backward_multiply_gate(W_hx, x, dmul_hx)\n",
    "    \n",
    "    return (dprev_state, dW_hx, dW_hh, dW_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c89142ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension is : 75\n",
      "epoch 50, loss 4.18223108646421, accuracy 0.11197916666666667\n",
      "epoch 100, loss 3.842034400735851, accuracy 0.22005208333333334\n",
      "epoch 150, loss 3.541025696688905, accuracy 0.21614583333333334\n",
      "epoch 200, loss 3.3996178844139746, accuracy 0.22395833333333334\n",
      "epoch 250, loss 3.3777040144544883, accuracy 0.25390625\n",
      "epoch 300, loss 3.186191625076074, accuracy 0.3203125\n",
      "epoch 350, loss 3.0572455963409877, accuracy 0.3046875\n",
      "epoch 400, loss 3.059383976735328, accuracy 0.30078125\n",
      "epoch 450, loss 3.012591920136936, accuracy 0.3268229166666667\n",
      "epoch 500, loss 2.7137002191439987, accuracy 0.37890625\n",
      "epoch 550, loss 2.726418368790578, accuracy 0.359375\n",
      "epoch 600, loss 2.7612495795096694, accuracy 0.3723958333333333\n",
      "epoch 650, loss 2.6242703595614034, accuracy 0.3841145833333333\n",
      "epoch 700, loss 2.408088679008324, accuracy 0.45703125\n",
      "epoch 750, loss 2.4681698348737746, accuracy 0.4205729166666667\n",
      "epoch 800, loss 2.3893067146578, accuracy 0.453125\n",
      "epoch 850, loss 2.3606142764147546, accuracy 0.4622395833333333\n",
      "epoch 900, loss 2.3990483434104304, accuracy 0.4270833333333333\n",
      "epoch 950, loss 2.2887920906230796, accuracy 0.4466145833333333\n",
      "epoch 1000, loss 2.1388509292273565, accuracy 0.5234375\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "sequence_length = int(12)\n",
    "dimension = one_hot.shape[1]\n",
    "print('dimension is :', dimension)\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "hidden_dim = 128\n",
    "\n",
    "W_hx = np.random.randn(hidden_dim, dimension) / np.sqrt(hidden_dim)\n",
    "W_hh = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "W_ho = np.random.randn(dimension, hidden_dim) / np.sqrt(hidden_dim)\n",
    "\n",
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    batch_id = random.sample(possible_batch_id, batch_size)  # 随机采样，选择batch_id。\n",
    "    prev_s = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "    \n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_rnn_recurrent(batch_x[:, n, :], prev_s, W_hx, W_hh, W_ho))\n",
    "        prev_s = layers[-1][3]\n",
    "        out_logits[:, n, :] = layers[-1][-1]\n",
    "    \n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)), axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs, axis=1) == y)\n",
    "    \n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    \n",
    "    # 之后需要开始计算反向更新部分了。\n",
    "    \n",
    "    # 1. 计算梯度:\n",
    "    delta = probs  # 取网络的输出结果。\n",
    "    delta[range(y.shape[0]), y] -= 1  # 将网络输出结果中对应标签的那个概率减去1。\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    dW_hx = np.zeros(W_hx.shape)\n",
    "    dW_hh = np.zeros(W_hh.shape)\n",
    "    dW_ho = np.zeros(W_ho.shape)\n",
    "    prev_state = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        d_mul_o = delta[:, n, :] # shape = (batch_size, dimension)\n",
    "        \n",
    "        dprev_s, dW_hx_t, dW_hh_t, dW_ho_t = backward_rnn_recurrent(batch_x[:,n,:], prev_state, \n",
    "                                                         W_hx, W_hh, W_ho, d_mul_o, layers[n])\n",
    "        prev_state = layers[n][3]\n",
    "        dW_hx += dW_hx_t\n",
    "        dW_hh += dW_hh_t\n",
    "        dW_ho += dW_ho_t\n",
    "    \n",
    "    # 更新\n",
    "    W_hx -= learning_rate * dW_hx \n",
    "    W_hh -= learning_rate * dW_hh\n",
    "    W_ho -= learning_rate * dW_ho\n",
    "    if(i + 1) % 50 == 0:\n",
    "        print(\"epoch {}, loss {}, accuracy {}\".format(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd545d",
   "metadata": {},
   "source": [
    "### Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "634f4762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(75, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=75, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_dim))\n",
    "        out, hn = self.rnn(x, None)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "rnn = RNN(input_dim = dimension, hidden_dim = hidden_dim, num_layers=1, output_dim = dimension)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b9b5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 3.401153564453125, accuracy 0.11328125\n",
      "epoch 100, loss 3.234724998474121, accuracy 0.20833333333333334\n",
      "epoch 150, loss 2.519819498062134, accuracy 0.3919270833333333\n",
      "epoch 200, loss 2.083822011947632, accuracy 0.4622395833333333\n",
      "epoch 250, loss 1.9139952659606934, accuracy 0.5377604166666666\n",
      "epoch 300, loss 1.7888798713684082, accuracy 0.5533854166666666\n",
      "epoch 350, loss 1.6153966188430786, accuracy 0.5963541666666666\n",
      "epoch 400, loss 1.2576789855957031, accuracy 0.6861979166666666\n",
      "epoch 450, loss 1.2486813068389893, accuracy 0.6822916666666666\n",
      "epoch 500, loss 1.3055974245071411, accuracy 0.66015625\n",
      "epoch 550, loss 1.154812216758728, accuracy 0.70703125\n",
      "epoch 600, loss 1.0780938863754272, accuracy 0.7317708333333334\n",
      "epoch 650, loss 0.9139544367790222, accuracy 0.7669270833333334\n",
      "epoch 700, loss 1.019874930381775, accuracy 0.7369791666666666\n",
      "epoch 750, loss 1.0062402486801147, accuracy 0.7369791666666666\n",
      "epoch 800, loss 0.7421910762786865, accuracy 0.8125\n",
      "epoch 850, loss 0.9106511473655701, accuracy 0.7486979166666666\n",
      "epoch 900, loss 0.9044910073280334, accuracy 0.7486979166666666\n",
      "epoch 950, loss 0.8377729058265686, accuracy 0.765625\n",
      "epoch 1000, loss 0.7003571391105652, accuracy 0.7981770833333334\n"
     ]
    }
   ],
   "source": [
    "# 分开定义softmax运算和交叉熵损失函数会造成数值不稳定。\n",
    "# 因此PyTorch提供了一个具有良好数值稳定性且包括softmax运算和交叉熵计算的函数。\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.5\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)  # 随机采样，选择batch_id。\n",
    "    prev_s = np.zeros((batch_size, hidden_dim))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "    \n",
    "    # 从Numpy转成torch之后送入神经网络中去。\n",
    "    output = rnn(torch.from_numpy(batch_x))  # torch.Size([64, 12, 75])\n",
    "    label = torch.argmax(torch.from_numpy(batch_y).view(-1, dimension), dim=1) # shape = 786\n",
    "    \n",
    "    accuracy = np.mean(torch.argmax(output.view(-1, dimension), axis=1).numpy() == label.numpy())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output.view(-1, dimension), label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i + 1) % 50 == 0:\n",
    "        print(\"epoch {}, loss {}, accuracy {}\".format(i+1, loss.item(), accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74296201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdf923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbada5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd32d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
