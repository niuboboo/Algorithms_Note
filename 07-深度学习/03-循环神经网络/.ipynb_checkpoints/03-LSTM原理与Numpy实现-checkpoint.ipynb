{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804b996c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564359c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7231ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdfe4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e99f0f4e",
   "metadata": {},
   "source": [
    "## Vanilla-LSTM-代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd75135",
   "metadata": {},
   "source": [
    "### Numpy实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832de5ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763da4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len text:  15294\n",
      "len text_vocab:  75\n",
      "one_hot shape:  (15294, 75)\n",
      "dimension is : 75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()\n",
    "\n",
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read() # 将文件中的所有数据读取进来。\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    \n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_control(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot\n",
    "\n",
    "\n",
    "text, text_vocab = get_vocab('../consumer.h', lower = False)\n",
    "one_hot = embed_to_control(text, text_vocab)\n",
    "print('len text: ', len(text))\n",
    "print('len text_vocab: ', len(text_vocab))\n",
    "print('one_hot shape: ', one_hot.shape)\n",
    "\n",
    "epoch = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "sequence_length = int(12)\n",
    "dimension = one_hot.shape[1]\n",
    "print('dimension is :', dimension)\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de877d5b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`tanh`激活函数为:\n",
    "\n",
    "$$\n",
    "\\tanh x=\\frac{\\sinh x}{\\cosh x}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;导数为:\n",
    "\n",
    "$$\n",
    "(\\tanh x)^{\\prime}=\\operatorname{sech}^{2} x=1-\\tanh ^{2} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb74007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8891cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: np.max(x)取的是二维数组x中的最大值。\n",
    "    \"\"\"\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e5444",
   "metadata": {},
   "source": [
    "&emsp;&emsp;多分类的交叉熵损失如下:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{N} \\sum_{i} L_{i}=-\\frac{1}{N} \\sum_{i} \\sum_{c=1}^{M} y_{i c} \\log \\left(p_{i c}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中$M$表示类别的数量，$y_{ic}$是符号函数(0或者1), 如果样本$i$的真实类别等于$c$取1，否者取0。$p_{ic}$表示观测样本$i$属于类别$c$的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524882d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat + 1e-9))) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0987ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_multiply_gate(w, x, dz):\n",
    "    \"\"\"\n",
    "    w shape = (75, 128)\n",
    "    x shape = (64, 128)\n",
    "    dz shape = (64, 75)\n",
    "    \"\"\"\n",
    "    dw = np.dot(dz.T, x) # shape = (75, 128)\n",
    "    dx = np.dot(w.T, dz.T) # shape = (128, 64)\n",
    "    return dw, dx\n",
    "\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9844757",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先依据公式创建三个门的权重:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{I}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x i}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h i}+\\boldsymbol{b}_{i}\\right) \\\\\n",
    "\\boldsymbol{F}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x f}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h f}+\\boldsymbol{b}_{f}\\right) \\\\\n",
    "\\boldsymbol{O}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x o}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h o}+\\boldsymbol{b}_{o}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb4378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hi = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "W_hf = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "W_ho = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7779f1a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;之后需要产生**候选记忆单元**：\n",
    "\n",
    "$$\n",
    "\\tilde{\\boldsymbol{C}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x c}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h c}+\\boldsymbol{b}_{c}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;**记忆单元**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{C}_{t}=\\boldsymbol{F}_{t} \\odot \\boldsymbol{C}_{t-1}+\\boldsymbol{I}_{t} \\odot \\tilde{\\boldsymbol{C}}_{t}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;`LSTM`与`RNN、GRU`的区别是里面所含的状态有两个，一个是$C$，另外一个是$H$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "005b2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hc = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3364c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**隐状态**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_{t}=\\boldsymbol{O}_{t} \\odot \\tanh \\left(\\boldsymbol{C}_{t}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$\\tanh$是为了保证值是在`+1`到`-1`之间的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5110087",
   "metadata": {},
   "source": [
    "&emsp;&emsp;之后我们再创建一个处理输入的权重矩阵，本来是有四个的$W_{xi}, W_{xf}, W_{xo}, W_{xc}$，这里我们为了方便起见，统一为一个$U$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2540fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.random.randn(hidden_dim, dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690db3e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;之后还有一个输出层的权重，也将其全部设置为$V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c12dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.random.randn(dimension, hidden_dim) / np.sqrt(hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e565a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, grad=False):\n",
    "    if grad:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward_lstm_recurrent(x, c_state, h_state, U, W_hf, W_hi, W_hc, W_ho, V):\n",
    "    \n",
    "    # 计算输入的处理单元。\n",
    "    mul_u = np.dot(x, U.T)\n",
    "    \n",
    "    # 计算遗忘门\n",
    "    mul_Wf = np.dot(h_state, W_hf.T)\n",
    "    add_Wf = mul_u + mul_Wf\n",
    "    f = sigmoid(add_Wf)\n",
    "    \n",
    "    # 计算输入门\n",
    "    mul_Wi = np.dot(h_state, W_hi.T)\n",
    "    add_Wi = mul_u + mul_Wi\n",
    "    i = sigmoid(add_Wi)\n",
    "    \n",
    "    # 计算候选记忆单元\n",
    "    mul_Wc = np.dot(h_state, W_hc.T)\n",
    "    add_Wc = mul_u + mul_Wc\n",
    "    c_hat = tanh(add_Wc)\n",
    "    \n",
    "    # 记忆单元选择需要从之前的c_state中遗忘多少，从输入门中提取多少候选记忆单元。\n",
    "    C = c_state * f + i * c_hat\n",
    "    \n",
    "    # 输出门\n",
    "    mul_Wo = np.dot(h_state, W_ho.T)\n",
    "    add_Wo = mul_u + mul_Wo\n",
    "    o = sigmoid(add_Wo)\n",
    "    \n",
    "    # 计算隐藏状态。\n",
    "    h = o * tanh(C)\n",
    "    \n",
    "    mul_v = np.dot(h, V.T)\n",
    "    return (mul_u, mul_Wf, add_Wf, mul_Wi, add_Wi, mul_Wc, add_Wc, C, mul_Wo, add_Wo, h, mul_v, i, o, c_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f67404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_recurrent(x, c_state, h_state, U, Wf, Wi, Wc, Wo, V, d_mul_v, saved_graph):\n",
    "    mul_u, mul_Wf, add_Wf, mul_Wi, add_Wi, mul_Wc, add_Wc, C, mul_Wo, add_Wo, h, mul_v, i, o, c_hat = saved_graph\n",
    "    dV, dh = backward_multiply_gate(V, h, d_mul_v)\n",
    "    dC = tanh(C, True) * o * dh.T\n",
    "    do = tanh(C) * dh.T\n",
    "    dadd_Wo = sigmoid(add_Wo, True) * do\n",
    "    dmul_u1, dmul_Wo = backward_add_gate(mul_u, mul_Wo, dadd_Wo)\n",
    "    dWo, dprev_state = backward_multiply_gate(Wo, h_state, dmul_Wo)\n",
    "    dc_hat = dC * i\n",
    "    dadd_Wc = tanh(add_Wc, True) * dc_hat\n",
    "    dmul_u2, dmul_Wc = backward_add_gate(mul_u, mul_Wc, dadd_Wc)\n",
    "    dWc, dprev_state = backward_multiply_gate(Wc, h_state, dmul_Wc)\n",
    "    di = dC * c_hat\n",
    "    dadd_Wi = sigmoid(add_Wi, True) * di\n",
    "    dmul_u3, dmul_Wi = backward_add_gate(mul_u, mul_Wi, dadd_Wi)\n",
    "    dWi, dprev_state = backward_multiply_gate(Wi, h_state, dmul_Wi)\n",
    "    df = dC * c_state\n",
    "    dadd_Wf = sigmoid(add_Wf, True) * df\n",
    "    dmul_u4, dmul_Wf = backward_add_gate(mul_u, mul_Wf, dadd_Wf)\n",
    "    dWf, dprev_state = backward_multiply_gate(Wf, h_state, dmul_Wf)\n",
    "    dU, dx = backward_multiply_gate(U, x, dmul_u4)\n",
    "    return (dU, dWf, dWi, dWc, dWo, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c36d0afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 3.869480435620895, accuracy 0.1328125\n",
      "epoch 100, loss 3.5236123521197826, accuracy 0.17838541666666666\n",
      "epoch 150, loss 3.300833054781156, accuracy 0.20963541666666666\n",
      "epoch 200, loss 3.059171981690467, accuracy 0.2760416666666667\n",
      "epoch 250, loss 2.9879058625218793, accuracy 0.2799479166666667\n",
      "epoch 300, loss 2.7253541986952823, accuracy 0.3567708333333333\n",
      "epoch 350, loss 2.8991301010145087, accuracy 0.3111979166666667\n",
      "epoch 400, loss 2.739520752820132, accuracy 0.3567708333333333\n",
      "epoch 450, loss 2.7594340279988354, accuracy 0.3203125\n",
      "epoch 500, loss 2.550837708695022, accuracy 0.4010416666666667\n",
      "epoch 550, loss 2.564432655698529, accuracy 0.35546875\n",
      "epoch 600, loss 2.636274814861799, accuracy 0.3346354166666667\n",
      "epoch 650, loss 2.451738966036372, accuracy 0.3736979166666667\n",
      "epoch 700, loss 2.4445363968472624, accuracy 0.3489583333333333\n",
      "epoch 750, loss 2.3753177022147187, accuracy 0.4231770833333333\n",
      "epoch 800, loss 2.3726650853133333, accuracy 0.40625\n",
      "epoch 850, loss 2.4342965281173736, accuracy 0.375\n",
      "epoch 900, loss 2.215702911220192, accuracy 0.4453125\n",
      "epoch 950, loss 2.2622451015481855, accuracy 0.4388020833333333\n",
      "epoch 1000, loss 2.2390543474833016, accuracy 0.43359375\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    \n",
    "    prev_c = np.zeros((batch_size, hidden_dim))\n",
    "    prev_h = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "        \n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_lstm_recurrent(batch_x[:, n, :], prev_c, prev_h, U, W_hf, W_hi, W_hc, W_ho, V))\n",
    "        \n",
    "        prev_c = layers[-1][7]\n",
    "        prev_h = layers[-1][10]\n",
    "        \n",
    "        out_logits[:, n, :] = layers[-1][-4]\n",
    "        \n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)), axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs, axis=1) == y)\n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    \n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW_hf = np.zeros(W_hf.shape)\n",
    "    dW_hi = np.zeros(W_hi.shape)\n",
    "    dW_hc = np.zeros(W_hc.shape)\n",
    "    dW_ho = np.zeros(W_ho.shape)\n",
    "    \n",
    "    prev_c = np.zeros((batch_size, hidden_dim))\n",
    "    prev_h = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dU_t, dWf_t, dWi_t, dWc_t, dWo_t, dV_t = backward_recurrent(batch_x[:,n,:], prev_c, prev_h, U, W_hf, W_hi, \n",
    "                                                                    W_hc, W_ho, V, d_mul_v, layers[n])\n",
    "        prev_c = layers[n][7]\n",
    "        prev_h = layers[n][10]\n",
    "        dU += dU_t\n",
    "        dV += dV_t\n",
    "        dW_hf += dWf_t\n",
    "        dW_hi += dWi_t\n",
    "        dW_hc += dWc_t\n",
    "        dW_ho += dWo_t\n",
    "    U -= learning_rate * dU\n",
    "    V -= learning_rate * dV\n",
    "    W_hf -= learning_rate * dW_hf\n",
    "    W_hi -= learning_rate * dW_hi\n",
    "    W_hc -= learning_rate * dW_hc\n",
    "    W_ho -= learning_rate * dW_ho\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('epoch {}, loss {}, accuracy {}'.format(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aadc38",
   "metadata": {},
   "source": [
    "### Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a22ea617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (LSTM): LSTM(75, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=75, bias=True)\n",
      ")\n",
      "epoch 50, loss 2.037531852722168, accuracy 0.4661458333333333\n",
      "epoch 100, loss 1.0574300289154053, accuracy 0.7330729166666666\n",
      "epoch 150, loss 0.7793861031532288, accuracy 0.7682291666666666\n",
      "epoch 200, loss 0.6055523753166199, accuracy 0.83203125\n",
      "epoch 250, loss 0.67572021484375, accuracy 0.7955729166666666\n",
      "epoch 300, loss 0.5834397673606873, accuracy 0.8203125\n",
      "epoch 350, loss 0.5546103119850159, accuracy 0.8177083333333334\n",
      "epoch 400, loss 0.5669826865196228, accuracy 0.828125\n",
      "epoch 450, loss 0.544745147228241, accuracy 0.8359375\n",
      "epoch 500, loss 0.49985525012016296, accuracy 0.8268229166666666\n",
      "epoch 550, loss 0.4533112049102783, accuracy 0.8489583333333334\n",
      "epoch 600, loss 0.4929572343826294, accuracy 0.83203125\n",
      "epoch 650, loss 0.4681812524795532, accuracy 0.8450520833333334\n",
      "epoch 700, loss 0.4950845241546631, accuracy 0.8333333333333334\n",
      "epoch 750, loss 0.5215380787849426, accuracy 0.81640625\n",
      "epoch 800, loss 0.41228094696998596, accuracy 0.84765625\n",
      "epoch 850, loss 0.4233684837818146, accuracy 0.859375\n",
      "epoch 900, loss 0.4076651334762573, accuracy 0.8580729166666666\n",
      "epoch 950, loss 0.3912968635559082, accuracy 0.859375\n",
      "epoch 1000, loss 0.3942139148712158, accuracy 0.8645833333333334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(LSTM, self).__init__()\n",
    "        self.LSTM = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_dim))\n",
    "        # out, (hn, hc) = self.LSTM(x, (h0, c0))\n",
    "        out, (hn, hc) = self.LSTM(x, None)  # 得到所有时间序列的输出。\n",
    "        \n",
    "        # out = self.fc(out[:, -1, :])  # 取最后一个时间步的输出。\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "lstm = LSTM(input_dim = dimension, hidden_dim = hidden_dim, num_layers=1, output_dim = dimension)\n",
    "print(lstm)\n",
    "\n",
    "\n",
    "# 分开定义softmax运算和交叉熵损失函数会造成数值不稳定。\n",
    "# 因此PyTorch提供了一个具有良好数值稳定性且包括softmax运算和交叉熵计算的函数。\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)  # 随机采样，选择batch_id。\n",
    "    # prev_s = np.zeros((batch_size, hidden_dim))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "    \n",
    "    # 从Numpy转成torch之后送入神经网络中去。\n",
    "    output = lstm(torch.from_numpy(batch_x))  # torch.Size([64, 12, 75])\n",
    "    label = torch.argmax(torch.from_numpy(batch_y).view(-1, dimension), dim=1) # shape = 786\n",
    "    \n",
    "    accuracy = np.mean(torch.argmax(output.view(-1, dimension), axis=1).numpy() == label.numpy())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output.view(-1, dimension), label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i + 1) % 50 == 0:\n",
    "        print(\"epoch {}, loss {}, accuracy {}\".format(i+1, loss.item(), accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f4b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
