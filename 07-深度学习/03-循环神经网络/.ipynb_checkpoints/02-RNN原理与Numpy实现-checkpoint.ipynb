{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1d271b",
   "metadata": {},
   "source": [
    "## 循环神经网络 RNN\n",
    "\n",
    "&emsp;&emsp;在潜变量自回归模型中，我们采用潜变量$h_{t}$总结过去的信息。\n",
    "\n",
    "$$\n",
    "p(h_{t} | h_{t-1}, x_{t-1}) \\\\\n",
    "p(x_{t} | h_{t},x_{t-1})\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;接下来就是说如何把这个模型做成`RNN`:\n",
    "\n",
    "1. 针对$p(h_{t} | h_{t-1}, x_{t-1})$这一项, 我们可以建模成：\n",
    "\n",
    "$$\n",
    "h_{t} = \\tanh(W_{hh}h_{t-1} + W_{hx}x_{t-1} + b_{h})\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;上式中，如果去掉$W_{hh}h_{t-1}$这一项的话，就会退化成`MLP`了。\n",
    "\n",
    "2. 针对$p(x_{t} | h_{t},x_{t-1})$这一项，我们的输出可以描述为:\n",
    "\n",
    "$$\n",
    "o_{t} = \\phi(W_{ho}h_{t} + b_{o})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241baa10",
   "metadata": {},
   "source": [
    "## torch.nn.RNN\n",
    "\n",
    "在`PyTorch`中，`RNN`的`API`地址为：[torch.nn.RNN(*args, **kwargs)](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN)\n",
    "\n",
    "其需要的参数有:\n",
    "\n",
    "1. `input_size`: 输入数据$x$的特征大小。\n",
    "2. `hidden_size`: 隐藏状态$h$的特征大小。\n",
    "3. `num_layers`: 默认只有一层，可以通过改变`num_layers`改变层数的大小。\n",
    "4. `nonlinearity`: 非线性激活函数，可以为`tanh`或者`relu`，默认为`tanh`。\n",
    "5. `bias`: 如果设置为`False`的话，就没有`bias`，默认为`True`。\n",
    "6. `batch_first`: 若设置为`True`，则提供的数据维度为`(batch, seq, feature)`，否则为`(seq, batch, feature)`，默认为`False`。\n",
    "7. `dropout`：如果为非`0`的值，则会设置`dropout`，默认为`0`。\n",
    "8. `bidirectional`: 是否为双向。若为双向的结构，则输出的结果大小为 `2 * hidden_size`。\n",
    "\n",
    "\n",
    "实例化之后，输入数据有两部分：`input,h_0`。输入数据维度为：$(L, N, H_{in})$, 如果`batch_first=False`, 则维度为：$(N, L, H_{in})$。`h_0`的数据`shape`为`(D * num_layers, N, H_out)`。\n",
    "\n",
    "其中：$N=batch\\ size, L=sequence\\ length, D = 2 \\ if\\ bidrectional=True,\\  otherwise = 1$, $H_{in}=input\\_size, H_{out}=hidden\\_size$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e016b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "bs, T = 2, 3 # 批大小，输入序列长度\n",
    "input_size, hidden_size = 2, 3  # 输入特征大小，隐含层特征大小\n",
    "\n",
    "input_data = torch.randn(bs, T, input_size)  # 随机初始化输入特征数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a592f39",
   "metadata": {},
   "source": [
    "### 单向，单层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8622164",
   "metadata": {},
   "source": [
    "#### 双向，单层的API调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b234df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4158,  0.2196,  0.3127],\n",
      "         [-0.1908, -0.0525, -0.1845],\n",
      "         [-0.4863,  0.3603,  0.2505]],\n",
      "\n",
      "        [[-0.7959, -0.8847, -0.5323],\n",
      "         [-0.0699,  0.4007,  0.5177],\n",
      "         [-0.0020,  0.5124,  0.2386]]], grad_fn=<TransposeBackward1>)\n",
      "tensor([[[-0.4863,  0.3603,  0.2505],\n",
      "         [-0.0020,  0.5124,  0.2386]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h_prev = torch.zeros(bs, hidden_size)  # 初始化隐含状态\n",
    "single_rnn = nn.RNN(input_size, hidden_size, batch_first=True)  # 输入特征维度为4， 输出特征维度为3。\n",
    "output, hn = single_rnn(input_data, h_prev.unsqueeze(0))  # 不指定h_0的话，则默认会给0。\n",
    "print(output)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359012e2",
   "metadata": {},
   "source": [
    "从上述输出结果可以看到，输出结果为`batch size`为`1`，序列长度为`2`， 每个序列的输出的维度都是`3`，最后一个序列的输出就是`hn`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535e024",
   "metadata": {},
   "source": [
    "#### 双向，单层RNN前向传播理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410dd291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(input_data, weight_ih, bias_ih, weight_hh, bias_hh, h_prev):\n",
    "    bs, T, input_size = input_data.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim)  # 初始化一个输出(状态)矩阵\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input_data[:, t, :].unsqueeze(2)  # 获取当前时刻输入特征， bs * input_size * 1\n",
    "        \n",
    "        w_ih_batch = weight_ih.unsqueeze(0).tile(bs, 1, 1)  # bs * h_dim * input_size\n",
    "        w_hh_batch = weight_hh.unsqueeze(0).tile(bs, 1, 1)  # bs * h_dim * h_dim\n",
    "        \n",
    "        w_times_x = torch.bmm(w_ih_batch, x).squeeze(-1)  # bs * h_dim\n",
    "        w_times_h = torch.bmm(w_hh_batch, h_prev.unsqueeze(2)).squeeze(-1)  # bs * h_dim\n",
    "        \n",
    "        h_prev = torch.tanh(w_times_x + bias_ih + w_times_h + bias_hh)\n",
    "        \n",
    "        h_out[:,t,:] = h_prev\n",
    "    \n",
    "    return h_out, h_prev.unsqueeze(0)  # 单向单层的，但是官方给的是三维的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19ba759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.5119,  0.0075],\n",
      "        [-0.5362, -0.4070],\n",
      "        [-0.5311, -0.1011]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.5507, -0.1207,  0.3047],\n",
      "        [ 0.4018, -0.2243, -0.3673],\n",
      "        [ 0.5684, -0.4863, -0.4629]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([-0.3611, -0.0943,  0.3951], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([-0.4305, -0.0467, -0.4333], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k, v in single_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c8a0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4158,  0.2196,  0.3127],\n",
      "         [-0.1908, -0.0525, -0.1845],\n",
      "         [-0.4863,  0.3603,  0.2505]],\n",
      "\n",
      "        [[-0.7959, -0.8847, -0.5323],\n",
      "         [-0.0699,  0.4007,  0.5177],\n",
      "         [-0.0020,  0.5124,  0.2386]]], grad_fn=<CopySlices>)\n",
      "tensor([[[-0.4863,  0.3603,  0.2505],\n",
      "         [-0.0020,  0.5124,  0.2386]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, hn = rnn_forward(input_data, single_rnn.weight_ih_l0, single_rnn.bias_ih_l0, \\\n",
    "                         single_rnn.weight_hh_l0, single_rnn.bias_hh_l0, h_prev)\n",
    "print(output)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9cdbdc",
   "metadata": {},
   "source": [
    "可以看到输出结果与官方`API`的调用结果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a857c0f",
   "metadata": {},
   "source": [
    "### 双向，单层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b3cc3",
   "metadata": {},
   "source": [
    "#### 双向，单层的API调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b5bb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2831,  0.0730,  0.4316, -0.5670, -0.0961,  0.4662],\n",
      "         [ 0.2739, -0.1995,  0.4555, -0.4329,  0.1318,  0.3973],\n",
      "         [ 0.3073, -0.3371,  0.4671, -0.6645,  0.1620,  0.1460]],\n",
      "\n",
      "        [[ 0.4380,  0.9180, -0.4362, -0.6307, -0.6673,  0.2989],\n",
      "         [ 0.3911, -0.3844,  0.5364, -0.5271,  0.0734,  0.4458],\n",
      "         [-0.1203, -0.4641,  0.5987, -0.5103,  0.2283,  0.3184]]],\n",
      "       grad_fn=<TransposeBackward1>)\n",
      "tensor([[[ 0.3073, -0.3371,  0.4671],\n",
      "         [-0.1203, -0.4641,  0.5987]],\n",
      "\n",
      "        [[-0.5670, -0.0961,  0.4662],\n",
      "         [-0.6307, -0.6673,  0.2989]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bidirectional_rnn = nn.RNN(input_size, hidden_size, batch_first=True, bidirectional=True)  # 输入特征维度为4， 输出特征维度为3， 1层。\n",
    "h_prev = torch.zeros(2, bs, hidden_size)  # 初始化隐含状态\n",
    "bi_output, bi_hn = bidirectional_rnn(input_data, h_prev)  # 不指定h_0的话，则默认会给0。\n",
    "print(bi_output)\n",
    "print(bi_hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08845988",
   "metadata": {},
   "source": [
    "#### 双向，单层RNN前向传播理解\n",
    "\n",
    "双向`RNN`的前向计算过程是什么样的呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "694acbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidirectional_rnn_forward(input_data, weight_ih, bias_ih, weight_hh, bias_hh, h_prev,\\\n",
    "                             weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse):\n",
    "    \n",
    "    bs, T, input_size = input_data.shape\n",
    "    h_dim = weight_ih.shape[0]\n",
    "    h_out = torch.zeros(bs, T, h_dim * 2)  # 初始化一个输出(状态)矩阵, 双向是两倍特征大小。\n",
    "    \n",
    "    forward_output = rnn_forward(input_data, weight_ih, bias_ih, weight_hh, bias_hh, h_prev)[0]\n",
    "    \n",
    "    backward_output = rnn_forward(torch.flip(input_data, [1]), weight_ih_reverse, weight_hh_reverse, \\\n",
    "                                  bias_ih_reverse, bias_hh_reverse, h_prev_reverse)[0]  # backward layer\n",
    "    \n",
    "    h_out[:, :, :h_dim] = forward_output\n",
    "    h_out[:, :, h_dim:] = backward_output\n",
    "\n",
    "    return h_out, h_out[:, -1, :].reshape((bs, 2, h_dim)).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3bfcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 Parameter containing:\n",
      "tensor([[ 0.3819, -0.1383],\n",
      "        [ 0.1533,  0.5730],\n",
      "        [-0.2634, -0.2586]], requires_grad=True)\n",
      "weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.5423,  0.3853,  0.1053],\n",
      "        [-0.1075, -0.3597, -0.1994],\n",
      "        [-0.2672,  0.2382,  0.1060]], requires_grad=True)\n",
      "bias_ih_l0 Parameter containing:\n",
      "tensor([ 0.0654, -0.0144, -0.0434], requires_grad=True)\n",
      "bias_hh_l0 Parameter containing:\n",
      "tensor([0.4866, 0.1902, 0.3265], requires_grad=True)\n",
      "weight_ih_l0_reverse Parameter containing:\n",
      "tensor([[-0.2933,  0.1265],\n",
      "        [-0.0481, -0.2726],\n",
      "        [-0.2160,  0.0169]], requires_grad=True)\n",
      "weight_hh_l0_reverse Parameter containing:\n",
      "tensor([[-0.3859,  0.3072, -0.5566],\n",
      "        [-0.1584,  0.3211, -0.4665],\n",
      "        [-0.2365,  0.0129,  0.4616]], requires_grad=True)\n",
      "bias_ih_l0_reverse Parameter containing:\n",
      "tensor([-0.3966, -0.5015,  0.3502], requires_grad=True)\n",
      "bias_hh_l0_reverse Parameter containing:\n",
      "tensor([-0.4332,  0.4477, -0.2797], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k, v in bidirectional_rnn.named_parameters():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1931ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2831,  0.0730,  0.4316, -0.6645,  0.1620,  0.1460],\n",
      "         [ 0.2739, -0.1995,  0.4555, -0.4329,  0.1318,  0.3973],\n",
      "         [ 0.3073, -0.3371,  0.4671, -0.5670, -0.0961,  0.4662]],\n",
      "\n",
      "        [[ 0.4380,  0.9180, -0.4362, -0.5103,  0.2283,  0.3184],\n",
      "         [ 0.3911, -0.3844,  0.5364, -0.5271,  0.0734,  0.4458],\n",
      "         [-0.1203, -0.4641,  0.5987, -0.6307, -0.6673,  0.2989]]],\n",
      "       grad_fn=<CopySlices>)\n",
      "tensor([[[ 0.3073, -0.3371,  0.4671],\n",
      "         [-0.1203, -0.4641,  0.5987]],\n",
      "\n",
      "        [[-0.5670, -0.0961,  0.4662],\n",
      "         [-0.6307, -0.6673,  0.2989]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output,hn = bidirectional_rnn_forward(input_data, bidirectional_rnn.weight_ih_l0, bidirectional_rnn.bias_ih_l0,\\\n",
    "                         bidirectional_rnn.weight_hh_l0, bidirectional_rnn.bias_hh_l0, h_prev[0], \\\n",
    "                         bidirectional_rnn.weight_ih_l0_reverse, bidirectional_rnn.bias_ih_l0_reverse, \\\n",
    "                         bidirectional_rnn.weight_hh_l0_reverse, bidirectional_rnn.bias_hh_l0_reverse, h_prev[1])\n",
    "print(output)\n",
    "print(hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fed8e7",
   "metadata": {},
   "source": [
    "### RNNCell\n",
    "\n",
    "`RNN`其实就是多次的`RNNCell`的计算。\n",
    "\n",
    "函数原型为:[torch.nn.RNNCell](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html?highlight=rnncell#torch.nn.RNNCell)\n",
    "\n",
    "```python\n",
    "CLASS torch.nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', device=None, dtype=None)\n",
    "```\n",
    "\n",
    "举例为:\n",
    "\n",
    "```python\n",
    "rnn = nn.RNNCell(10, 20)\n",
    "input = torch.randn(6, 3, 10)\n",
    "hx = torch.randn(3, 20)\n",
    "output = []\n",
    "for i in range(6):\n",
    "    hx = rnn(input[i], hx)\n",
    "    output.append(hx)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b175f853",
   "metadata": {},
   "source": [
    "## Vanilla-RNN-代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb6358",
   "metadata": {},
   "source": [
    "### Numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d1c3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()\n",
    "\n",
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read() # 将文件中的所有数据读取进来。\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    \n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_control(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e6cbb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len text:  15294\n",
      "len text_vocab:  75\n",
      "one_hot shape:  (15294, 75)\n"
     ]
    }
   ],
   "source": [
    "text, text_vocab = get_vocab('../consumer.h', lower = False)\n",
    "one_hot = embed_to_control(text, text_vocab)\n",
    "print('len text: ', len(text))\n",
    "print('len text_vocab: ', len(text_vocab))\n",
    "print('one_hot shape: ', one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab9cc1",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`tanh`激活函数为:\n",
    "\n",
    "$$\n",
    "\\tanh x=\\frac{\\sinh x}{\\cosh x}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;导数为:\n",
    "\n",
    "$$\n",
    "(\\tanh x)^{\\prime}=\\operatorname{sech}^{2} x=1-\\tanh ^{2} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57574421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bae637",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{t} = \\tanh(W_{hh}h_{t-1} + W_{hx}x_{t-1} + b_{h})\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_{t} = \\phi(W_{ho}h_{t} + b_{o})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6fdd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_rnn_recurrent(x, prev_state, W_hx, W_hh, W_ho):\n",
    "    mul_hx = np.dot(x, W_hx.T)\n",
    "    \n",
    "    # 处理隐藏状态部分。\n",
    "    mul_hh = np.dot(prev_state, W_hh.T)\n",
    "    add_previous_now = mul_hx + mul_hh\n",
    "    current_state = tanh(add_previous_now)\n",
    "    \n",
    "    # 处理输出部分。\n",
    "    mul_o = np.dot(current_state, W_ho.T)\n",
    "    return (mul_hx, mul_hh, add_previous_now, current_state, mul_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11728e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: np.max(x)取的是二维数组x中的最大值。\n",
    "    \"\"\"\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c168f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;多分类的交叉熵损失如下:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{N} \\sum_{i} L_{i}=-\\frac{1}{N} \\sum_{i} \\sum_{c=1}^{M} y_{i c} \\log \\left(p_{i c}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中$M$表示类别的数量，$y_{ic}$是符号函数(0或者1), 如果样本$i$的真实类别等于$c$取1，否者取0。$p_{ic}$表示观测样本$i$属于类别$c$的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1b94430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat + 1e-9))) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e74b2b0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`RNN`的反向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2ea97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_multiply_gate(w, x, dz):\n",
    "    \"\"\"\n",
    "    w shape = (75, 128)\n",
    "    x shape = (64, 128)\n",
    "    dz shape = (64, 75)\n",
    "    \"\"\"\n",
    "    dw = np.dot(dz.T, x) # shape = (75, 128)\n",
    "    dx = np.dot(w.T, dz.T) # shape = (128, 64)\n",
    "    return dw, dx\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2\n",
    "\n",
    "def backward_rnn_recurrent(x, prev_state, W_hx, W_hh, W_ho, d_mu_o, saved_graph):\n",
    "    mul_hx, mul_hh, add_previous_now, current_state, mul_o = saved_graph\n",
    "    \n",
    "    dW_ho, d_CurrentState = backward_multiply_gate(W_ho, current_state, d_mu_o)\n",
    "    \n",
    "    dadd_previous_now = tanh(add_previous_now, True) * d_CurrentState.T\n",
    "    \n",
    "    dmul_hh, dmul_hx = backward_add_gate(mul_hh, mul_hx, dadd_previous_now)\n",
    "    dW_hh, dprev_state = backward_multiply_gate(W_hh, prev_state, dmul_hh)\n",
    "    dW_hx, dx = backward_multiply_gate(W_hx, x, dmul_hx)\n",
    "    \n",
    "    return (dprev_state, dW_hx, dW_hh, dW_ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c89142ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension is : 75\n",
      "epoch 50, loss 4.241343069364474, accuracy 0.06640625\n",
      "epoch 100, loss 4.032394839844798, accuracy 0.18489583333333334\n",
      "epoch 150, loss 3.6716801635951057, accuracy 0.16536458333333334\n",
      "epoch 200, loss 3.4627890638009937, accuracy 0.23828125\n",
      "epoch 250, loss 3.4195365176120514, accuracy 0.24088541666666666\n",
      "epoch 300, loss 3.167283505103797, accuracy 0.2760416666666667\n",
      "epoch 350, loss 3.0534844597935122, accuracy 0.3203125\n",
      "epoch 400, loss 3.0401859430164486, accuracy 0.296875\n",
      "epoch 450, loss 2.9362047046204367, accuracy 0.35546875\n",
      "epoch 500, loss 2.774710712376892, accuracy 0.35546875\n",
      "epoch 550, loss 2.884420198759688, accuracy 0.3333333333333333\n",
      "epoch 600, loss 2.6950117225465324, accuracy 0.3697916666666667\n",
      "epoch 650, loss 2.7025486548669435, accuracy 0.35546875\n",
      "epoch 700, loss 2.6027955887830725, accuracy 0.3776041666666667\n",
      "epoch 750, loss 2.1376824503047134, accuracy 0.50390625\n",
      "epoch 800, loss 2.56399462748593, accuracy 0.3880208333333333\n",
      "epoch 850, loss 2.3527700177305126, accuracy 0.44140625\n",
      "epoch 900, loss 2.2883131472404985, accuracy 0.44140625\n",
      "epoch 950, loss 2.193940043807322, accuracy 0.4713541666666667\n",
      "epoch 1000, loss 2.2447776495198464, accuracy 0.453125\n"
     ]
    }
   ],
   "source": [
    "epoch = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "sequence_length = int(12)\n",
    "dimension = one_hot.shape[1]\n",
    "print('dimension is :', dimension)\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "hidden_dim = 128\n",
    "\n",
    "W_hx = np.random.randn(hidden_dim, dimension) / np.sqrt(hidden_dim)\n",
    "W_hh = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "W_ho = np.random.randn(dimension, hidden_dim) / np.sqrt(hidden_dim)\n",
    "\n",
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    batch_id = random.sample(possible_batch_id, batch_size)  # 随机采样，选择batch_id。\n",
    "    prev_s = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "    \n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_rnn_recurrent(batch_x[:, n, :], prev_s, W_hx, W_hh, W_ho))\n",
    "        prev_s = layers[-1][3]\n",
    "        out_logits[:, n, :] = layers[-1][-1]\n",
    "    \n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)), axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs, axis=1) == y)\n",
    "    \n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    \n",
    "    # 之后需要开始计算反向更新部分了。\n",
    "    \n",
    "    # 1. 计算梯度:\n",
    "    delta = probs  # 取网络的输出结果。\n",
    "    delta[range(y.shape[0]), y] -= 1  # 将网络输出结果中对应标签的那个概率减去1。\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    dW_hx = np.zeros(W_hx.shape)\n",
    "    dW_hh = np.zeros(W_hh.shape)\n",
    "    dW_ho = np.zeros(W_ho.shape)\n",
    "    prev_state = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        d_mul_o = delta[:, n, :] # shape = (batch_size, dimension)\n",
    "        \n",
    "        dprev_s, dW_hx_t, dW_hh_t, dW_ho_t = backward_rnn_recurrent(batch_x[:,n,:], prev_state, \n",
    "                                                         W_hx, W_hh, W_ho, d_mul_o, layers[n])\n",
    "        prev_state = layers[n][3]\n",
    "        dW_hx += dW_hx_t\n",
    "        dW_hh += dW_hh_t\n",
    "        dW_ho += dW_ho_t\n",
    "    \n",
    "    # 更新\n",
    "    W_hx -= learning_rate * dW_hx \n",
    "    W_hh -= learning_rate * dW_hh\n",
    "    W_ho -= learning_rate * dW_ho\n",
    "    if(i + 1) % 50 == 0:\n",
    "        print(\"epoch {}, loss {}, accuracy {}\".format(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd545d",
   "metadata": {},
   "source": [
    "### Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "634f4762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(75, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=75, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True, nonlinearity='tanh')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_dim))\n",
    "        out, hn = self.rnn(x, None)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "rnn = RNN(input_dim = dimension, hidden_dim = hidden_dim, num_layers=1, output_dim = dimension)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b9b5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 3.3897383213043213, accuracy 0.11067708333333333\n",
      "epoch 100, loss 3.04331374168396, accuracy 0.2526041666666667\n",
      "epoch 150, loss 2.7403066158294678, accuracy 0.3346354166666667\n",
      "epoch 200, loss 2.039881467819214, accuracy 0.5052083333333334\n",
      "epoch 250, loss 2.1290059089660645, accuracy 0.4700520833333333\n",
      "epoch 300, loss 1.5940618515014648, accuracy 0.6067708333333334\n",
      "epoch 350, loss 1.5607830286026, accuracy 0.6197916666666666\n",
      "epoch 400, loss 1.2755593061447144, accuracy 0.6692708333333334\n",
      "epoch 450, loss 1.2091299295425415, accuracy 0.69140625\n",
      "epoch 500, loss 1.3169924020767212, accuracy 0.6731770833333334\n",
      "epoch 550, loss 1.3107393980026245, accuracy 0.66796875\n",
      "epoch 600, loss 1.18193519115448, accuracy 0.7083333333333334\n",
      "epoch 650, loss 1.027571678161621, accuracy 0.7434895833333334\n",
      "epoch 700, loss 0.9772723317146301, accuracy 0.7278645833333334\n",
      "epoch 750, loss 1.1997982263565063, accuracy 0.6888020833333334\n",
      "epoch 800, loss 0.989878237247467, accuracy 0.7395833333333334\n",
      "epoch 850, loss 0.8884389400482178, accuracy 0.7565104166666666\n",
      "epoch 900, loss 0.8054671883583069, accuracy 0.7760416666666666\n",
      "epoch 950, loss 0.9281501173973083, accuracy 0.7421875\n",
      "epoch 1000, loss 0.7821805477142334, accuracy 0.796875\n"
     ]
    }
   ],
   "source": [
    "# 分开定义softmax运算和交叉熵损失函数会造成数值不稳定。\n",
    "# 因此PyTorch提供了一个具有良好数值稳定性且包括softmax运算和交叉熵计算的函数。\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.5\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)  # 随机采样，选择batch_id。\n",
    "    prev_s = np.zeros((batch_size, hidden_dim))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "    \n",
    "    # 从Numpy转成torch之后送入神经网络中去。\n",
    "    output = rnn(torch.from_numpy(batch_x))  # torch.Size([64, 12, 75])\n",
    "    label = torch.argmax(torch.from_numpy(batch_y).view(-1, dimension), dim=1) # shape = 786\n",
    "    \n",
    "    accuracy = np.mean(torch.argmax(output.view(-1, dimension), axis=1).numpy() == label.numpy())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output.view(-1, dimension), label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i + 1) % 50 == 0:\n",
    "        print(\"epoch {}, loss {}, accuracy {}\".format(i+1, loss.item(), accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74296201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdf923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbada5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd32d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
