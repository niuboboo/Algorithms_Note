{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb4682d",
   "metadata": {},
   "source": [
    "## torch.nn.GRU\n",
    "\n",
    "\n",
    "在`PyTorch`中，`GRU`的`API`地址为：[torch.nn.GRU(*args, **kwargs)](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe964b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "&r_{t}=\\sigma\\left(W_{i r} x_{t}+b_{i r}+W_{h r} h_{(t-1)}+b_{h r}\\right) \\\\\n",
    "&n_{t}=\\tanh \\left(W_{i n} x_{t}+b_{i n}+r_{t} *\\left(W_{h n} h_{(t-1)}+b_{h n}\\right)\\right) \\\\\n",
    "&z_{t}=\\sigma\\left(W_{i z} x_{t}+b_{i z}+W_{h z} h_{(t-1)}+b_{h z}\\right) \\\\\n",
    "&h_{t}=\\left(1-z_{t}\\right) * n_{t}+z_{t} * h_{(t-1)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800057eb",
   "metadata": {},
   "source": [
    "`GRU`大概是在`14`年或者`15`年提出来的，`LSTM`在`2000`年左右提出来的。在`GRU`中没有$c_{t}$这个东西的。在`GRU`中有两个门，一个叫做`reset`，一个叫做更新门$z_{t}$。\n",
    "\n",
    "由于没有$c_{t}$，所以在做初始状态的时候，我们只需要提供$h_{0}$作为初始状态。\n",
    "\n",
    "相比于`LSTM`，没有$c_{t}$，也就没有输入门对于全局信息的控制，取而代之的是一个重制门对于上一时刻信息的提取，对于当前时刻的信息则并不用一个门控制，而是直接给到$n_{t}$，也就是:\n",
    "\n",
    "$$\n",
    "n_{t}=\\tanh \\left(W_{i n} x_{t}+b_{i n}+r_{t} *\\left(W_{h n} h_{(t-1)}+b_{h n}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc355a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e05bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "354d2f6e",
   "metadata": {},
   "source": [
    "## Vanilla-GRU-代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b647a3",
   "metadata": {},
   "source": [
    "### Numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433ddf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len text:  15294\n",
      "len text_vocab:  75\n",
      "one_hot shape:  (15294, 75)\n",
      "dimension is : 75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()\n",
    "\n",
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read() # 将文件中的所有数据读取进来。\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    \n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_control(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot\n",
    "\n",
    "\n",
    "text, text_vocab = get_vocab('../consumer.h', lower = False)\n",
    "one_hot = embed_to_control(text, text_vocab)\n",
    "print('len text: ', len(text))\n",
    "print('len text_vocab: ', len(text_vocab))\n",
    "print('one_hot shape: ', one_hot.shape)\n",
    "\n",
    "epoch = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "sequence_length = int(12)\n",
    "dimension = one_hot.shape[1]\n",
    "print('dimension is :', dimension)\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8132a3a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`tanh`激活函数为:\n",
    "\n",
    "$$\n",
    "\\tanh x=\\frac{\\sinh x}{\\cosh x}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;导数为:\n",
    "\n",
    "$$\n",
    "(\\tanh x)^{\\prime}=\\operatorname{sech}^{2} x=1-\\tanh ^{2} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aef321c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee87686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: np.max(x)取的是二维数组x中的最大值。\n",
    "    \"\"\"\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a6731",
   "metadata": {},
   "source": [
    "&emsp;&emsp;多分类的交叉熵损失如下:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{N} \\sum_{i} L_{i}=-\\frac{1}{N} \\sum_{i} \\sum_{c=1}^{M} y_{i c} \\log \\left(p_{i c}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中$M$表示类别的数量，$y_{ic}$是符号函数(0或者1), 如果样本$i$的真实类别等于$c$取1，否者取0。$p_{ic}$表示观测样本$i$属于类别$c$的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1868e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat + 1e-9))) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14bfc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, grad=False):\n",
    "    if grad:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d27852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_multiply_gate(w, x, dz):\n",
    "    \"\"\"\n",
    "    w shape = (75, 128)\n",
    "    x shape = (64, 128)\n",
    "    dz shape = (64, 75)\n",
    "    \"\"\"\n",
    "    dw = np.dot(dz.T, x) # shape = (75, 128)\n",
    "    dx = np.dot(w.T, dz.T) # shape = (128, 64)\n",
    "    return dw, dx\n",
    "\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3741bd2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **更新门**：能关注的机制（更新门）。$Z_{t}$\n",
    "\n",
    "2. **重置门**：能遗忘的机制（重置门）。$R_{t}$\n",
    "\n",
    "$$\n",
    "Z_{t} = \\sigma(X_{t} W_{xz} + H_{t-1} W_{tz} + b_{z}) \\\\\n",
    "R_{t} = \\sigma(X_{t} W_{xr} + H_{t-1} W_{hr} + b_{r})\n",
    "$$\n",
    "\n",
    "3. **候选隐藏状态**:\n",
    "\n",
    "&emsp;&emsp;之后基于这两个门的输出，我们来计算候选隐藏状态:\n",
    "\n",
    "$$\n",
    "\\tilde{\\boldsymbol{H}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\left(\\boldsymbol{R}_{t} \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;如果将其与`RNN`做对比的话，我们可以发现，如果不看$R_{t}$的话，他就和之前的`RNN`是一样的。\n",
    "\n",
    "4. **隐状态**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_{t}=\\mathbf{Z}_{t} \\odot \\boldsymbol{H}_{t-1}+\\left(1-\\boldsymbol{Z}_{t}\\right) \\odot \\tilde{\\boldsymbol{H}}_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7639921",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "U = np.random.randn(hidden_dim, dimension) / np.sqrt(hidden_dim)\n",
    "Wz = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "Wr = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "Wh = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "V = np.random.randn(dimension, hidden_dim) / np.sqrt(dimension)\n",
    "\n",
    "def forward_gru_recurrent(x, h_state, U, Wz, Wr, Wh, V):\n",
    "    mul_u = np.dot(x, U.T)\n",
    "    \n",
    "    # 更新门。\n",
    "    mul_Wz = np.dot(h_state, Wz.T)\n",
    "    add_Wz = mul_u + mul_Wz\n",
    "    z = sigmoid(add_Wz)\n",
    "    \n",
    "    # 重置门。\n",
    "    mul_Wr = np.dot(h_state, Wr.T)\n",
    "    add_Wr = mul_u + mul_Wr\n",
    "    r = sigmoid(add_Wr)\n",
    "    \n",
    "    # 计算候选隐藏状态。\n",
    "    mul_Wh = np.dot(h_state * r, Wh.T)\n",
    "    add_Wh = mul_u + mul_Wh\n",
    "    h_hat = tanh(add_Wh)\n",
    "    \n",
    "    # 隐藏状态。\n",
    "    h = (1 - z) * h_state + z * h_hat\n",
    "    mul_v = np.dot(h, V.T)\n",
    "    return (mul_u, mul_Wz, add_Wz, z, mul_Wr, add_Wr, r, mul_Wh, add_Wh, h_hat, h, mul_v)\n",
    "\n",
    "def backward_multiply_gate(w, x, dz):\n",
    "    dW = np.dot(dz.T, x)\n",
    "    dx = np.dot(w.T, dz.T)\n",
    "    return dW, dx\n",
    "\n",
    "def backward_gru_recurrent(x, h_state, U, Wz, Wr, Wh, V, d_mul_v, saved_graph):\n",
    "    mul_u, mul_Wz, add_Wz, z, mul_Wr, add_Wr, r, mul_Wh, add_Wh, h_hat, h, mul_v = saved_graph\n",
    "    dV, dh = backward_multiply_gate(V, h, d_mul_v)\n",
    "    dh_hat = z * dh.T\n",
    "    dadd_Wh = tanh(add_Wh, True) * dh_hat\n",
    "    dmul_u1, dmul_Wh = backward_add_gate(mul_u, mul_Wh, dadd_Wh)\n",
    "    dWh, dprev_state = backward_multiply_gate(Wh, h_state * r, dmul_Wh)\n",
    "    dr = dprev_state * h_state.T\n",
    "    dadd_Wr = sigmoid(add_Wr, True) * dr.T\n",
    "    dmul_u2, dmul_Wr = backward_add_gate(mul_u, mul_Wr, dadd_Wr)\n",
    "    dWr, dprev_state = backward_multiply_gate(Wr, h_state, dmul_Wr)\n",
    "    dz = -h_state + h_hat\n",
    "    dadd_Wz = sigmoid(add_Wz, True) * dz\n",
    "    dmul_u3, dmul_Wz = backward_add_gate(mul_u, mul_Wz, dadd_Wz)\n",
    "    dWz, dprev_state = backward_multiply_gate(Wz, h_state, dmul_Wz)\n",
    "    dU, dx = backward_multiply_gate(U, x, dmul_u3)\n",
    "    return (dU, dWz, dWr, dWh, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f81df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 4.203867721992362, accuracy 0.203125\n",
      "epoch 100, loss 4.0719420698203965, accuracy 0.171875\n",
      "epoch 150, loss 3.945095020706722, accuracy 0.15364583333333334\n",
      "epoch 200, loss 4.116852843829865, accuracy 0.1640625\n",
      "epoch 250, loss 4.005570576480906, accuracy 0.11588541666666667\n",
      "epoch 300, loss 3.791289822564561, accuracy 0.13020833333333334\n",
      "epoch 350, loss 3.745911286265436, accuracy 0.09244791666666667\n",
      "epoch 400, loss 3.7166766648938983, accuracy 0.08984375\n",
      "epoch 450, loss 3.58060720942242, accuracy 0.1328125\n",
      "epoch 500, loss 3.680578494627021, accuracy 0.09765625\n",
      "epoch 550, loss 3.5597145255912235, accuracy 0.13671875\n",
      "epoch 600, loss 3.556371752580326, accuracy 0.09635416666666667\n",
      "epoch 650, loss 3.567199841630222, accuracy 0.10026041666666667\n",
      "epoch 700, loss 3.572499573873319, accuracy 0.11067708333333333\n",
      "epoch 750, loss 3.501475628971075, accuracy 0.109375\n",
      "epoch 800, loss 3.5580926738055934, accuracy 0.11979166666666667\n",
      "epoch 850, loss 3.582812347534802, accuracy 0.11588541666666667\n",
      "epoch 900, loss 3.4885843983504876, accuracy 0.09635416666666667\n",
      "epoch 950, loss 3.4423414480819368, accuracy 0.125\n",
      "epoch 1000, loss 3.542654146733359, accuracy 0.09635416666666667\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    \n",
    "    prev_h = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:, n, :] = one_hot[id1, :]\n",
    "        batch_y[:, n, :] = one_hot[id2, :]\n",
    "    \n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_gru_recurrent(batch_x[:,n,:], prev_h, U, Wz, Wr, Wh, V))\n",
    "        prev_h = layers[-1][-2]\n",
    "        out_logits[:, n, :] = layers[-1][-1]\n",
    "        \n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)), axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs, axis=1) == y)\n",
    "    \n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    \n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dWz = np.zeros(Wz.shape)\n",
    "    dWr = np.zeros(Wr.shape)\n",
    "    dWh = np.zeros(Wh.shape)\n",
    "    \n",
    "    prev_h = np.zeros((batch_size, hidden_dim))\n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dU_t, dWz_t, dWr_t, dWh_t, dV_t = backward_gru_recurrent(batch_x[:,n,:], prev_h, \n",
    "                                                                    U, Wz, Wr, Wh, V, d_mul_v, layers[n])\n",
    "        prev_h = layers[n][-2]\n",
    "        dU += dU_t\n",
    "        dV += dV_t\n",
    "        dWz += dWz_t\n",
    "        dWr += dWr_t\n",
    "        dWh += dWh_t\n",
    "    U -= learning_rate * dU\n",
    "    V -= learning_rate * dV\n",
    "    Wz -= learning_rate * dWz\n",
    "    Wr -= learning_rate * dWr\n",
    "    Wh -= learning_rate * dWh\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('epoch {}, loss {}, accuracy {}'.format(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef3202",
   "metadata": {},
   "source": [
    "### Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d089cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (GRU): GRU(75, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=75, bias=True)\n",
      ")\n",
      "epoch 50, loss 1.6199532747268677, accuracy 0.5520833333333334\n",
      "epoch 100, loss 0.7474623322486877, accuracy 0.8046875\n",
      "epoch 150, loss 0.6157116293907166, accuracy 0.8059895833333334\n",
      "epoch 200, loss 0.5109125971794128, accuracy 0.8502604166666666\n",
      "epoch 250, loss 0.5903604626655579, accuracy 0.80859375\n",
      "epoch 300, loss 0.45260095596313477, accuracy 0.8580729166666666\n",
      "epoch 350, loss 0.487970232963562, accuracy 0.8489583333333334\n",
      "epoch 400, loss 0.42427536845207214, accuracy 0.8671875\n",
      "epoch 450, loss 0.45133838057518005, accuracy 0.84765625\n",
      "epoch 500, loss 0.45451799035072327, accuracy 0.8268229166666666\n",
      "epoch 550, loss 0.494780570268631, accuracy 0.8307291666666666\n",
      "epoch 600, loss 0.3985040485858917, accuracy 0.859375\n",
      "epoch 650, loss 0.43380674719810486, accuracy 0.8580729166666666\n",
      "epoch 700, loss 0.37696531414985657, accuracy 0.8671875\n",
      "epoch 750, loss 0.488079309463501, accuracy 0.8307291666666666\n",
      "epoch 800, loss 0.4159274399280548, accuracy 0.8606770833333334\n",
      "epoch 850, loss 0.3686436414718628, accuracy 0.8619791666666666\n",
      "epoch 900, loss 0.41990819573402405, accuracy 0.8723958333333334\n",
      "epoch 950, loss 0.38303133845329285, accuracy 0.8619791666666666\n",
      "epoch 1000, loss 0.4105536639690399, accuracy 0.8567708333333334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(GRU, self).__init__()\n",
    "        self.GRU = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_dim))\n",
    "        out, hn = self.GRU(x, None)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "gru = GRU(input_dim = dimension, hidden_dim = hidden_dim, num_layers=1, output_dim = dimension)\n",
    "print(gru)\n",
    "\n",
    "\n",
    "# 分开定义softmax运算和交叉熵损失函数会造成数值不稳定。\n",
    "# 因此PyTorch提供了一个具有良好数值稳定性且包括softmax运算和交叉熵计算的函数。\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)  # 随机采样，选择batch_id。\n",
    "    # prev_s = np.zeros((batch_size, hidden_dim))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "    \n",
    "    # 从Numpy转成torch之后送入神经网络中去。\n",
    "    output = gru(torch.from_numpy(batch_x))  # torch.Size([64, 12, 75])\n",
    "    label = torch.argmax(torch.from_numpy(batch_y).view(-1, dimension), dim=1) # shape = 786\n",
    "    \n",
    "    accuracy = np.mean(torch.argmax(output.view(-1, dimension), axis=1).numpy() == label.numpy())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output.view(-1, dimension), label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i + 1) % 50 == 0:\n",
    "        print(\"epoch {}, loss {}, accuracy {}\".format(i+1, loss.item(), accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48fbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ccd19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05bd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
