{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb4682d",
   "metadata": {},
   "source": [
    "## torch.nn.GRU\n",
    "\n",
    "\n",
    "在`PyTorch`中，`GRU`的`API`地址为：[torch.nn.GRU(*args, **kwargs)](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html?highlight=gru#torch.nn.GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe964b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "&r_{t}=\\sigma\\left(W_{i r} x_{t}+b_{i r}+W_{h r} h_{(t-1)}+b_{h r}\\right) \\\\\n",
    "&n_{t}=\\tanh \\left(W_{i n} x_{t}+b_{i n}+r_{t} *\\left(W_{h n} h_{(t-1)}+b_{h n}\\right)\\right) \\\\\n",
    "&z_{t}=\\sigma\\left(W_{i z} x_{t}+b_{i z}+W_{h z} h_{(t-1)}+b_{h z}\\right) \\\\\n",
    "&h_{t}=\\left(1-z_{t}\\right) * n_{t}+z_{t} * h_{(t-1)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800057eb",
   "metadata": {},
   "source": [
    "`GRU`大概是在`14`年或者`15`年提出来的，`LSTM`在`2000`年左右提出来的。在`GRU`中没有$c_{t}$这个东西的。在`GRU`中有两个门，一个叫做`reset`，一个叫做更新门$z_{t}$。\n",
    "\n",
    "由于没有$c_{t}$，所以在做初始状态的时候，我们只需要提供$h_{0}$作为初始状态。\n",
    "\n",
    "相比于`LSTM`，没有$c_{t}$，也就没有输入门对于全局信息的控制，取而代之的是一个重制门对于上一时刻信息的提取，对于当前时刻的信息则并不用一个门控制，而是直接给到$n_{t}$，也就是:\n",
    "\n",
    "$$\n",
    "n_{t}=\\tanh \\left(W_{i n} x_{t}+b_{i n}+r_{t} *\\left(W_{h n} h_{(t-1)}+b_{h n}\\right)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d812da",
   "metadata": {},
   "source": [
    "在相同输入输出模型下，gru的模型参数大致是lstm的四分之三倍。可以验证一下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604e05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "lstm_layer = nn.LSTM(3, 5)\n",
    "gru_layer = nn.GRU(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f22e71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in lstm_layer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d723c794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in gru_layer.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776cf659",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "input_data = torch.randn(bs, T, i_size)  # 输入序列\n",
    "\n",
    "h0 = torch.randn(bs, h_size)  # 初始值，不需要训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd2aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1355,  0.4056,  0.3317,  0.4589, -0.4083],\n",
      "         [-0.1811,  0.7739,  0.2401,  0.1952, -0.1132],\n",
      "         [-0.1386,  0.8449,  0.1788, -0.0816, -0.0403]],\n",
      "\n",
      "        [[-0.0475, -0.0651,  0.4481,  0.2246,  0.4372],\n",
      "         [ 0.1057, -0.1324,  0.3786,  0.1656,  0.5676],\n",
      "         [ 0.0160,  0.4196,  0.3794, -0.0798,  0.3670]]],\n",
      "       grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "gru_layer = nn.GRU(i_size, h_size, batch_first=True)\n",
    "output, h_final = gru_layer(input_data, h0.unsqueeze(0))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66564ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([15, 4])\n",
      "weight_hh_l0 torch.Size([15, 5])\n",
      "bias_ih_l0 torch.Size([15])\n",
      "bias_hh_l0 torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "for k, v in gru_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddcc4ed",
   "metadata": {},
   "source": [
    "### 实现GRU前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11df4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_forward(input_data, init_state, w_ih, w_hh, b_ih, b_hh):\n",
    "    prev_h = init_state\n",
    "    bs, T, i_size = input_data.shape\n",
    "    h_size = w_ih.shape[0] // 3\n",
    "    \n",
    "    # 对权重扩维，复制成batch size倍。\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)\n",
    "    \n",
    "    output = torch.zeros(bs, T, h_size)  # GRU网络的输出状态序列\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input_data[:, t, :]  # t时刻GRU cell的输入特征向量， [bs, i_size]\n",
    "        \n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)).squeeze(-1)  # [bs, 3*h_size, 1]\n",
    "        \n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)).squeeze(-1) \n",
    "        \n",
    "        # 重置门\n",
    "        r_t = torch.sigmoid(w_times_x[:, :h_size] + w_times_h_prev[:, :h_size] + b_ih[:h_size]+ b_hh[:h_size])\n",
    "        # 更新门\n",
    "        z_t = torch.sigmoid(w_times_x[:, h_size:2*h_size] + w_times_h_prev[:, h_size:2*h_size] \\\n",
    "                            + b_ih[h_size:2*h_size]+ b_hh[h_size:2*h_size])\n",
    "        \n",
    "        # 候选状态\n",
    "        n_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size] + b_ih[2*h_size:3*h_size] + \\\n",
    "                         r_t * (w_times_h_prev[:, 2*h_size:3*h_size] + b_hh[2*h_size:3*h_size]))\n",
    "        \n",
    "        prev_h = (1 - z_t) * n_t + z_t * prev_h  # 增量更新隐含状态\n",
    "        \n",
    "        output[:, t, :] = prev_h\n",
    "        \n",
    "    return output, prev_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e96f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1355,  0.4056,  0.3317,  0.4589, -0.4083],\n",
      "         [-0.1811,  0.7739,  0.2401,  0.1952, -0.1132],\n",
      "         [-0.1386,  0.8449,  0.1788, -0.0816, -0.0403]],\n",
      "\n",
      "        [[-0.0475, -0.0651,  0.4481,  0.2246,  0.4372],\n",
      "         [ 0.1057, -0.1324,  0.3786,  0.1656,  0.5676],\n",
      "         [ 0.0160,  0.4196,  0.3794, -0.0798,  0.3670]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "output, h_final = gru_forward(input_data, h0, gru_layer.weight_ih_l0, gru_layer.weight_hh_l0, \\\n",
    "                             gru_layer.bias_ih_l0, gru_layer.bias_hh_l0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d2f6e",
   "metadata": {},
   "source": [
    "## Vanilla-GRU-代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b647a3",
   "metadata": {},
   "source": [
    "### Numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433ddf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len text:  15294\n",
      "len text_vocab:  75\n",
      "one_hot shape:  (15294, 75)\n",
      "dimension is : 75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()\n",
    "\n",
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read() # 将文件中的所有数据读取进来。\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    \n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_control(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot\n",
    "\n",
    "\n",
    "text, text_vocab = get_vocab('../consumer.h', lower = False)\n",
    "one_hot = embed_to_control(text, text_vocab)\n",
    "print('len text: ', len(text))\n",
    "print('len text_vocab: ', len(text_vocab))\n",
    "print('one_hot shape: ', one_hot.shape)\n",
    "\n",
    "epoch = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "sequence_length = int(12)\n",
    "dimension = one_hot.shape[1]\n",
    "print('dimension is :', dimension)\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8132a3a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`tanh`激活函数为:\n",
    "\n",
    "$$\n",
    "\\tanh x=\\frac{\\sinh x}{\\cosh x}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;导数为:\n",
    "\n",
    "$$\n",
    "(\\tanh x)^{\\prime}=\\operatorname{sech}^{2} x=1-\\tanh ^{2} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aef321c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ee87686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: np.max(x)取的是二维数组x中的最大值。\n",
    "    \"\"\"\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a6731",
   "metadata": {},
   "source": [
    "&emsp;&emsp;多分类的交叉熵损失如下:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{N} \\sum_{i} L_{i}=-\\frac{1}{N} \\sum_{i} \\sum_{c=1}^{M} y_{i c} \\log \\left(p_{i c}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中$M$表示类别的数量，$y_{ic}$是符号函数(0或者1), 如果样本$i$的真实类别等于$c$取1，否者取0。$p_{ic}$表示观测样本$i$属于类别$c$的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1868e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat + 1e-9))) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14bfc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, grad=False):\n",
    "    if grad:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d27852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_multiply_gate(w, x, dz):\n",
    "    \"\"\"\n",
    "    w shape = (75, 128)\n",
    "    x shape = (64, 128)\n",
    "    dz shape = (64, 75)\n",
    "    \"\"\"\n",
    "    dw = np.dot(dz.T, x) # shape = (75, 128)\n",
    "    dx = np.dot(w.T, dz.T) # shape = (128, 64)\n",
    "    return dw, dx\n",
    "\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3741bd2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **更新门**：能关注的机制（更新门）。$Z_{t}$\n",
    "\n",
    "2. **重置门**：能遗忘的机制（重置门）。$R_{t}$\n",
    "\n",
    "$$\n",
    "Z_{t} = \\sigma(X_{t} W_{xz} + H_{t-1} W_{tz} + b_{z}) \\\\\n",
    "R_{t} = \\sigma(X_{t} W_{xr} + H_{t-1} W_{hr} + b_{r})\n",
    "$$\n",
    "\n",
    "3. **候选隐藏状态**:\n",
    "\n",
    "&emsp;&emsp;之后基于这两个门的输出，我们来计算候选隐藏状态:\n",
    "\n",
    "$$\n",
    "\\tilde{\\boldsymbol{H}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x h}+\\left(\\boldsymbol{R}_{t} \\odot \\boldsymbol{H}_{t-1}\\right) \\boldsymbol{W}_{h h}+\\boldsymbol{b}_{h}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;如果将其与`RNN`做对比的话，我们可以发现，如果不看$R_{t}$的话，他就和之前的`RNN`是一样的。\n",
    "\n",
    "4. **隐状态**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_{t}=\\mathbf{Z}_{t} \\odot \\boldsymbol{H}_{t-1}+\\left(1-\\boldsymbol{Z}_{t}\\right) \\odot \\tilde{\\boldsymbol{H}}_{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7639921",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "U = np.random.randn(hidden_dim, dimension) / np.sqrt(hidden_dim)\n",
    "Wz = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "Wr = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "Wh = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "V = np.random.randn(dimension, hidden_dim) / np.sqrt(dimension)\n",
    "\n",
    "def forward_gru_recurrent(x, h_state, U, Wz, Wr, Wh, V):\n",
    "    mul_u = np.dot(x, U.T)\n",
    "    \n",
    "    # 更新门。\n",
    "    mul_Wz = np.dot(h_state, Wz.T)\n",
    "    add_Wz = mul_u + mul_Wz\n",
    "    z = sigmoid(add_Wz)\n",
    "    \n",
    "    # 重置门。\n",
    "    mul_Wr = np.dot(h_state, Wr.T)\n",
    "    add_Wr = mul_u + mul_Wr\n",
    "    r = sigmoid(add_Wr)\n",
    "    \n",
    "    # 计算候选隐藏状态。\n",
    "    mul_Wh = np.dot(h_state * r, Wh.T)\n",
    "    add_Wh = mul_u + mul_Wh\n",
    "    h_hat = tanh(add_Wh)\n",
    "    \n",
    "    # 隐藏状态。\n",
    "    h = (1 - z) * h_state + z * h_hat\n",
    "    mul_v = np.dot(h, V.T)\n",
    "    return (mul_u, mul_Wz, add_Wz, z, mul_Wr, add_Wr, r, mul_Wh, add_Wh, h_hat, h, mul_v)\n",
    "\n",
    "def backward_multiply_gate(w, x, dz):\n",
    "    dW = np.dot(dz.T, x)\n",
    "    dx = np.dot(w.T, dz.T)\n",
    "    return dW, dx\n",
    "\n",
    "def backward_gru_recurrent(x, h_state, U, Wz, Wr, Wh, V, d_mul_v, saved_graph):\n",
    "    mul_u, mul_Wz, add_Wz, z, mul_Wr, add_Wr, r, mul_Wh, add_Wh, h_hat, h, mul_v = saved_graph\n",
    "    dV, dh = backward_multiply_gate(V, h, d_mul_v)\n",
    "    dh_hat = z * dh.T\n",
    "    dadd_Wh = tanh(add_Wh, True) * dh_hat\n",
    "    dmul_u1, dmul_Wh = backward_add_gate(mul_u, mul_Wh, dadd_Wh)\n",
    "    dWh, dprev_state = backward_multiply_gate(Wh, h_state * r, dmul_Wh)\n",
    "    dr = dprev_state * h_state.T\n",
    "    dadd_Wr = sigmoid(add_Wr, True) * dr.T\n",
    "    dmul_u2, dmul_Wr = backward_add_gate(mul_u, mul_Wr, dadd_Wr)\n",
    "    dWr, dprev_state = backward_multiply_gate(Wr, h_state, dmul_Wr)\n",
    "    dz = -h_state + h_hat\n",
    "    dadd_Wz = sigmoid(add_Wz, True) * dz\n",
    "    dmul_u3, dmul_Wz = backward_add_gate(mul_u, mul_Wz, dadd_Wz)\n",
    "    dWz, dprev_state = backward_multiply_gate(Wz, h_state, dmul_Wz)\n",
    "    dU, dx = backward_multiply_gate(U, x, dmul_u3)\n",
    "    return (dU, dWz, dWr, dWh, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f81df28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 4.205812871238337, accuracy 0.17708333333333334\n",
      "epoch 100, loss 4.125757887102406, accuracy 0.14973958333333334\n",
      "epoch 150, loss 3.9376015608682837, accuracy 0.13020833333333334\n",
      "epoch 200, loss 3.964367698691824, accuracy 0.22526041666666666\n",
      "epoch 250, loss 4.11912351339582, accuracy 0.125\n",
      "epoch 300, loss 4.0097548409817465, accuracy 0.09635416666666667\n",
      "epoch 350, loss 3.876725700625436, accuracy 0.10677083333333333\n",
      "epoch 400, loss 3.766448631615121, accuracy 0.109375\n",
      "epoch 450, loss 3.673177671133151, accuracy 0.10546875\n",
      "epoch 500, loss 3.6843513633393843, accuracy 0.10546875\n",
      "epoch 550, loss 3.6535054605590456, accuracy 0.0859375\n",
      "epoch 600, loss 3.5876677972762896, accuracy 0.09895833333333333\n",
      "epoch 650, loss 3.525700681275738, accuracy 0.11979166666666667\n",
      "epoch 700, loss 3.6460132689903233, accuracy 0.1015625\n",
      "epoch 750, loss 3.6322743117080214, accuracy 0.08723958333333333\n",
      "epoch 800, loss 3.58966615628067, accuracy 0.10416666666666667\n",
      "epoch 850, loss 3.6297152951641727, accuracy 0.08854166666666667\n",
      "epoch 900, loss 3.5177781963123747, accuracy 0.08723958333333333\n",
      "epoch 950, loss 3.557674432577809, accuracy 0.11197916666666667\n",
      "epoch 1000, loss 3.4764317402347835, accuracy 0.09244791666666667\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    \n",
    "    prev_h = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:, n, :] = one_hot[id1, :]\n",
    "        batch_y[:, n, :] = one_hot[id2, :]\n",
    "    \n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_gru_recurrent(batch_x[:,n,:], prev_h, U, Wz, Wr, Wh, V))\n",
    "        prev_h = layers[-1][-2]\n",
    "        out_logits[:, n, :] = layers[-1][-1]\n",
    "        \n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)), axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs, axis=1) == y)\n",
    "    \n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    \n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dWz = np.zeros(Wz.shape)\n",
    "    dWr = np.zeros(Wr.shape)\n",
    "    dWh = np.zeros(Wh.shape)\n",
    "    \n",
    "    prev_h = np.zeros((batch_size, hidden_dim))\n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dU_t, dWz_t, dWr_t, dWh_t, dV_t = backward_gru_recurrent(batch_x[:,n,:], prev_h, \n",
    "                                                                    U, Wz, Wr, Wh, V, d_mul_v, layers[n])\n",
    "        prev_h = layers[n][-2]\n",
    "        dU += dU_t\n",
    "        dV += dV_t\n",
    "        dWz += dWz_t\n",
    "        dWr += dWr_t\n",
    "        dWh += dWh_t\n",
    "    U -= learning_rate * dU\n",
    "    V -= learning_rate * dV\n",
    "    Wz -= learning_rate * dWz\n",
    "    Wr -= learning_rate * dWr\n",
    "    Wh -= learning_rate * dWh\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('epoch {}, loss {}, accuracy {}'.format(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef3202",
   "metadata": {},
   "source": [
    "### Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d089cd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (GRU): GRU(75, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=75, bias=True)\n",
      ")\n",
      "epoch 50, loss 1.600624680519104, accuracy 0.5651041666666666\n",
      "epoch 100, loss 0.8518295884132385, accuracy 0.7682291666666666\n",
      "epoch 150, loss 0.6507782340049744, accuracy 0.80078125\n",
      "epoch 200, loss 0.5483970642089844, accuracy 0.8411458333333334\n",
      "epoch 250, loss 0.5464432835578918, accuracy 0.8111979166666666\n",
      "epoch 300, loss 0.4642275869846344, accuracy 0.83984375\n",
      "epoch 350, loss 0.43444767594337463, accuracy 0.8528645833333334\n",
      "epoch 400, loss 0.4160071611404419, accuracy 0.8489583333333334\n",
      "epoch 450, loss 0.5063161253929138, accuracy 0.8268229166666666\n",
      "epoch 500, loss 0.4659353494644165, accuracy 0.8450520833333334\n",
      "epoch 550, loss 0.4283309876918793, accuracy 0.859375\n",
      "epoch 600, loss 0.4190591275691986, accuracy 0.8567708333333334\n",
      "epoch 650, loss 0.4155247211456299, accuracy 0.8671875\n",
      "epoch 700, loss 0.4059120714664459, accuracy 0.8580729166666666\n",
      "epoch 750, loss 0.46249547600746155, accuracy 0.84375\n",
      "epoch 800, loss 0.40460729598999023, accuracy 0.8567708333333334\n",
      "epoch 850, loss 0.3897285759449005, accuracy 0.8619791666666666\n",
      "epoch 900, loss 0.4492295980453491, accuracy 0.8489583333333334\n",
      "epoch 950, loss 0.4489783048629761, accuracy 0.84765625\n",
      "epoch 1000, loss 0.40807315707206726, accuracy 0.8528645833333334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(GRU, self).__init__()\n",
    "        self.GRU = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_dim))\n",
    "        out, hn = self.GRU(x, None)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "gru = GRU(input_dim = dimension, hidden_dim = hidden_dim, num_layers=1, output_dim = dimension)\n",
    "print(gru)\n",
    "\n",
    "\n",
    "# 分开定义softmax运算和交叉熵损失函数会造成数值不稳定。\n",
    "# 因此PyTorch提供了一个具有良好数值稳定性且包括softmax运算和交叉熵计算的函数。\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)  # 随机采样，选择batch_id。\n",
    "    # prev_s = np.zeros((batch_size, hidden_dim))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "    \n",
    "    # 从Numpy转成torch之后送入神经网络中去。\n",
    "    output = gru(torch.from_numpy(batch_x))  # torch.Size([64, 12, 75])\n",
    "    label = torch.argmax(torch.from_numpy(batch_y).view(-1, dimension), dim=1) # shape = 786\n",
    "    \n",
    "    accuracy = np.mean(torch.argmax(output.view(-1, dimension), axis=1).numpy() == label.numpy())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output.view(-1, dimension), label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i + 1) % 50 == 0:\n",
    "        print(\"epoch {}, loss {}, accuracy {}\".format(i+1, loss.item(), accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d48fbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ccd19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05bd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
