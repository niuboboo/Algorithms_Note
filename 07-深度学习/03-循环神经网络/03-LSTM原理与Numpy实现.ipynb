{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804b996c",
   "metadata": {},
   "source": [
    "- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140bef87",
   "metadata": {},
   "source": [
    "## torch.nn.LSTM\n",
    "\n",
    "在`PyTorch`中，`LSTM`的`API`地址为：[torch.nn.LSTM(*args, **kwargs)](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=nn%20lstm#torch.nn.LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4af18c",
   "metadata": {},
   "source": [
    "在`LSTM`经典结构图中，依据最上面一条黑线细胞状态更新历史信息，\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "i_{t} &=\\sigma\\left(W_{i i} x_{t}+b_{i i}+W_{h i} h_{t-1}+b_{h i}\\right) \\\\\n",
    "f_{t} &=\\sigma\\left(W_{i f} x_{t}+b_{i f}+W_{h f} h_{t-1}+b_{h f}\\right) \\\\\n",
    "g_{t} &=\\tanh \\left(W_{i g} x_{t}+b_{i g}+W_{h g} h_{t-1}+b_{h g}\\right) \\\\\n",
    "c_{t} &=f_{t} \\odot c_{t-1}+i_{t} \\odot g_{t} \\\\\n",
    "o_{t} &=\\sigma\\left(W_{i o} x_{t}+b_{i o}+W_{h o} h_{t-1}+b_{h o}\\right) \\\\\n",
    "h_{t} &=o_{t} \\odot \\tanh \\left(c_{t}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd819946",
   "metadata": {},
   "source": [
    "$i_{t}$是输入门，$f_{t}$是遗忘门，$o_{t}$是输出门。$c_{t}$是细胞单元，$g_{t}$是候选细胞状态。\n",
    "\n",
    "`LSTM`需要做的事情，就两件事：更新当前的细胞状态，拿到更新之后的细胞状态决定当前的输出。\n",
    "\n",
    "1. 想要更新当前的细胞状态，就需要一个遗忘门，对上一个时刻的细胞状态进行选择性遗忘，也就是$f_{t} \\odot c_{t-1}$, 还需要一个输入门，获取当前时刻输入信息的过滤，也就是$i_{t} \\odot g_{t}$部分。\n",
    "\n",
    "2. 之后就是设计输出门，与当前时刻细胞状态进行过滤，$o_{t} \\odot \\tanh \\left(c_{t}\\right)$。\n",
    "\n",
    "与`RNN`相比，`LSTM`多了一个需要初始化的值$c_{t}$, 如果初始化的参数是学习得到的，这样的算法也称作`meta-learning`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17703fee",
   "metadata": {},
   "source": [
    "其参数空间有：\n",
    "\n",
    "1. input_size: 输入数据$x$的特征大小。\n",
    "2. hidden_size: 隐藏层输出大小，也就是$h_{t}$和$c_{t}$的大小。\n",
    "3. num_layers: 循环层的大小。\n",
    "4. bias: 若设置为False，则不含偏置项。\n",
    "5. batch_first: 若设置为True，则batch的这个维度在第一个维度。\n",
    "6. dropout：如果为非`0`的值，则会设置`dropout`，默认为`0`。\n",
    "7. `bidirectional`: 是否为双向。若为双向的结构，则输出的结果大小为 `2 * hidden_size`。\n",
    "8. proj_size: 这个参数是LSTM网络的变体，是LSTMP，它的作用是为了减少LSTM的参数和计算量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c0bb4",
   "metadata": {},
   "source": [
    "### 实现LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdd3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "bs, T, i_size, h_size = 2, 3, 4, 5\n",
    "input_data = torch.randn(bs, T, i_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2705a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1963, -0.1278, -0.0995,  0.0151, -0.3178],\n",
      "         [-0.1245, -0.4401, -0.1217, -0.0785,  0.0063],\n",
      "         [-0.2461, -0.1975, -0.1093, -0.0019, -0.0014]],\n",
      "\n",
      "        [[-0.3640, -0.1631,  0.1700,  0.0082, -0.3252],\n",
      "         [-0.2442, -0.1396,  0.1449, -0.0412, -0.1769],\n",
      "         [-0.0059, -0.0577, -0.0728, -0.0343, -0.1941]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c_0 = torch.randn(bs, h_size)  # 初始值，不参与训练。\n",
    "h_0 = torch.randn(bs, h_size)\n",
    "\n",
    "lstm_layer = nn.LSTM(i_size, h_size, batch_first=True)\n",
    "\n",
    "output, (h_n, c_n) = lstm_layer(input_data, (h_0.unsqueeze(0), c_0.unsqueeze(0)))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed0e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 5])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for k, v in lstm_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8356c",
   "metadata": {},
   "source": [
    "其中的参数`weight_ih`和参数`weight_hh`都是四个`w`拼接起来的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745abf05",
   "metadata": {},
   "source": [
    "### LSTM前向传播理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7469ad1",
   "metadata": {},
   "source": [
    "`w_ih`的维度为`[4*h_size, i_size]`, `w_hh`的维度为`[4*h_size, h_size]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "338ed7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(input_data, init_states, w_ih, w_hh, b_ih, b_hh):\n",
    "    h0, c0 = init_states\n",
    "    bs, T, i_size = input_data.shape\n",
    "    h_size = w_ih.shape[0] // 4  \n",
    "    \n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4*h_size, i_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4*h_size, h_size]\n",
    "    \n",
    "    \n",
    "    output_size = h_size\n",
    "    output = torch.zeros(bs, T, output_size)  # 输出序列\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input_data[:, t, :]  # 当前时刻的输入向量, [bs, i_size]\n",
    "        \n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)).squeeze(-1)  # [bs, 4 * h_size]\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)).squeeze(-1)  # [bs, 4 * h_size]\n",
    "        \n",
    "        # 分别计算输入门(i)，遗忘门(f)，cell门(g)，输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size]+w_times_h_prev[:, :h_size]+b_ih[:h_size]+b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size]+w_times_h_prev[:, h_size:2*h_size]+ \\\n",
    "                            b_ih[h_size:2*h_size]+b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size]+w_times_h_prev[:, 2*h_size:3*h_size]+ \\\n",
    "                            b_ih[2*h_size:3*h_size]+b_hh[2*h_size:3*h_size])\n",
    "        \n",
    "        o_t = torch.sigmoid(w_times_x[:, 3*h_size:4*h_size]+w_times_h_prev[:, 3*h_size:4*h_size]+ \\\n",
    "                            b_ih[3*h_size:4*h_size]+b_hh[3*h_size:4*h_size])\n",
    "        \n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c)\n",
    "        \n",
    "        output[:, t, :] = prev_h\n",
    "        \n",
    "    return output, (prev_h, prev_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "250e62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1963, -0.1278, -0.0995,  0.0151, -0.3178],\n",
      "         [-0.1245, -0.4401, -0.1217, -0.0785,  0.0063],\n",
      "         [-0.2461, -0.1975, -0.1093, -0.0019, -0.0014]],\n",
      "\n",
      "        [[-0.3640, -0.1631,  0.1700,  0.0082, -0.3252],\n",
      "         [-0.2442, -0.1396,  0.1449, -0.0412, -0.1769],\n",
      "         [-0.0059, -0.0577, -0.0728, -0.0343, -0.1941]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "output, (h_n, c_n) = lstm_forward(input_data, (h_0, c_0), lstm_layer.weight_ih_l0, lstm_layer.weight_hh_l0, \\\n",
    "             lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fe730",
   "metadata": {},
   "source": [
    "### 实现LSTMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8753412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2407,  0.3258,  0.0930],\n",
      "         [ 0.1654,  0.2884,  0.1637],\n",
      "         [ 0.1339,  0.0925,  0.0966]],\n",
      "\n",
      "        [[ 0.1401,  0.3316, -0.0414],\n",
      "         [ 0.1399,  0.1083,  0.0132],\n",
      "         [ 0.0920,  0.0590,  0.0889]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "proj_size = 3  # 对h进行压缩。但是c不进行压缩\n",
    "\n",
    "c_0 = torch.randn(bs, h_size)  # 初始值，不参与训练。\n",
    "h_0 = torch.randn(bs, proj_size)\n",
    "\n",
    "lstm_layer = nn.LSTM(i_size, h_size, batch_first=True, proj_size=proj_size)\n",
    "\n",
    "output, (h_n, c_n) = lstm_layer(input_data, (h_0.unsqueeze(0), c_0.unsqueeze(0)))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25e2bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_ih_l0 torch.Size([20, 4])\n",
      "weight_hh_l0 torch.Size([20, 3])\n",
      "bias_ih_l0 torch.Size([20])\n",
      "bias_hh_l0 torch.Size([20])\n",
      "weight_hr_l0 torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "for k, v in lstm_layer.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a912dc",
   "metadata": {},
   "source": [
    "相比于`LSTM`，多了一个`weight_hr_l0`，这个参数其实就是对`hidden state`进行压缩的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ccf15",
   "metadata": {},
   "source": [
    "### LSTMP前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13d9b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(input_data, init_states, w_ih, w_hh, b_ih, b_hh, w_hr=None):\n",
    "    h0, c0 = init_states\n",
    "    bs, T, i_size = input_data.shape\n",
    "    h_size = w_ih.shape[0] // 4  \n",
    "    \n",
    "    prev_h = h0\n",
    "    prev_c = c0\n",
    "    batch_w_ih = w_ih.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4*h_size, i_size]\n",
    "    batch_w_hh = w_hh.unsqueeze(0).tile(bs, 1, 1)  # [bs, 4*h_size, h_size]\n",
    "    \n",
    "    if w_hr is not None:\n",
    "        p_size = w_hr.shape[0]\n",
    "        output_size = p_size\n",
    "        batch_w_hr = w_hr.unsqueeze(0).tile(bs, 1, 1)  # [bs, p_size, h_size]\n",
    "    else:\n",
    "        output_size = h_size\n",
    "        \n",
    "        \n",
    "    output = torch.zeros(bs, T, output_size)  # 输出序列\n",
    "    \n",
    "    for t in range(T):\n",
    "        x = input_data[:, t, :]  # 当前时刻的输入向量, [bs, i_size]\n",
    "        \n",
    "        w_times_x = torch.bmm(batch_w_ih, x.unsqueeze(-1)).squeeze(-1)  # [bs, 4 * h_size]\n",
    "        w_times_h_prev = torch.bmm(batch_w_hh, prev_h.unsqueeze(-1)).squeeze(-1)  # [bs, 4 * h_size]\n",
    "        \n",
    "        # 分别计算输入门(i)，遗忘门(f)，cell门(g)，输出门(o)\n",
    "        i_t = torch.sigmoid(w_times_x[:, :h_size]+w_times_h_prev[:, :h_size]+b_ih[:h_size]+b_hh[:h_size])\n",
    "        f_t = torch.sigmoid(w_times_x[:, h_size:2*h_size]+w_times_h_prev[:, h_size:2*h_size]+ \\\n",
    "                            b_ih[h_size:2*h_size]+b_hh[h_size:2*h_size])\n",
    "        g_t = torch.tanh(w_times_x[:, 2*h_size:3*h_size]+w_times_h_prev[:, 2*h_size:3*h_size]+ \\\n",
    "                            b_ih[2*h_size:3*h_size]+b_hh[2*h_size:3*h_size])\n",
    "        \n",
    "        o_t = torch.sigmoid(w_times_x[:, 3*h_size:4*h_size]+w_times_h_prev[:, 3*h_size:4*h_size]+ \\\n",
    "                            b_ih[3*h_size:4*h_size]+b_hh[3*h_size:4*h_size])\n",
    "        \n",
    "        prev_c = f_t * prev_c + i_t * g_t\n",
    "        prev_h = o_t * torch.tanh(prev_c) # [bs, h_size]\n",
    "        \n",
    "        if w_hr is not None:  # 做projection\n",
    "            prev_h = torch.bmm(batch_w_hr, prev_h.unsqueeze(-1))# [bs, p_size, 1]\n",
    "            prev_h = prev_h.squeeze(-1)  # [bs, p_size]\n",
    "        \n",
    "        output[:, t, :] = prev_h\n",
    "        \n",
    "    return output, (prev_h, prev_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa0a96ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2407,  0.3258,  0.0930],\n",
      "         [ 0.1654,  0.2884,  0.1637],\n",
      "         [ 0.1339,  0.0925,  0.0966]],\n",
      "\n",
      "        [[ 0.1401,  0.3316, -0.0414],\n",
      "         [ 0.1399,  0.1083,  0.0132],\n",
      "         [ 0.0920,  0.0590,  0.0889]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "output, (h_n, c_n) = lstm_forward(input_data, (h_0, c_0), lstm_layer.weight_ih_l0, lstm_layer.weight_hh_l0, \\\n",
    "             lstm_layer.bias_ih_l0, lstm_layer.bias_hh_l0, lstm_layer.weight_hr_l0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f0f4e",
   "metadata": {},
   "source": [
    "## Vanilla-LSTM-代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd75135",
   "metadata": {},
   "source": [
    "### Numpy实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832de5ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763da4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len text:  15294\n",
      "len text_vocab:  75\n",
      "one_hot shape:  (15294, 75)\n",
      "dimension is : 75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "sns.set()\n",
    "\n",
    "def get_vocab(file, lower = False):\n",
    "    with open(file, 'r') as fopen:\n",
    "        data = fopen.read() # 将文件中的所有数据读取进来。\n",
    "    if lower:\n",
    "        data = data.lower()\n",
    "    \n",
    "    vocab = list(set(data))\n",
    "    return data, vocab\n",
    "\n",
    "def embed_to_control(data, vocab):\n",
    "    onehot = np.zeros((len(data), len(vocab)), dtype = np.float32)\n",
    "    for i in range(len(data)):\n",
    "        onehot[i, vocab.index(data[i])] = 1.0\n",
    "    return onehot\n",
    "\n",
    "\n",
    "text, text_vocab = get_vocab('../consumer.h', lower = False)\n",
    "one_hot = embed_to_control(text, text_vocab)\n",
    "print('len text: ', len(text))\n",
    "print('len text_vocab: ', len(text_vocab))\n",
    "print('one_hot shape: ', one_hot.shape)\n",
    "\n",
    "epoch = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "sequence_length = int(12)\n",
    "dimension = one_hot.shape[1]\n",
    "print('dimension is :', dimension)\n",
    "possible_batch_id = range(len(text) - sequence_length - 1)\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de877d5b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`tanh`激活函数为:\n",
    "\n",
    "$$\n",
    "\\tanh x=\\frac{\\sinh x}{\\cosh x}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;导数为:\n",
    "\n",
    "$$\n",
    "(\\tanh x)^{\\prime}=\\operatorname{sech}^{2} x=1-\\tanh ^{2} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb74007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x, grad=False):\n",
    "    if grad:\n",
    "        output = np.tanh(x)\n",
    "        return (1.0 - np.square(output))\n",
    "    else:\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8891cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    x: np.max(x)取的是二维数组x中的最大值。\n",
    "    \"\"\"\n",
    "    exp_scores = np.exp(x - np.max(x))\n",
    "    return exp_scores / (np.sum(exp_scores, axis=1, keepdims=True) + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e5444",
   "metadata": {},
   "source": [
    "&emsp;&emsp;多分类的交叉熵损失如下:\n",
    "\n",
    "$$\n",
    "L=\\frac{1}{N} \\sum_{i} L_{i}=-\\frac{1}{N} \\sum_{i} \\sum_{c=1}^{M} y_{i c} \\log \\left(p_{i c}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;其中$M$表示类别的数量，$y_{ic}$是符号函数(0或者1), 如果样本$i$的真实类别等于$c$取1，否者取0。$p_{ic}$表示观测样本$i$属于类别$c$的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524882d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(Y_hat, Y, epsilon=1e-12):\n",
    "    Y_hat = np.clip(Y_hat, epsilon, 1. - epsilon)\n",
    "    N = Y_hat.shape[0]\n",
    "    return -np.sum(np.sum(Y * np.log(Y_hat + 1e-9))) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0987ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_multiply_gate(w, x, dz):\n",
    "    \"\"\"\n",
    "    w shape = (75, 128)\n",
    "    x shape = (64, 128)\n",
    "    dz shape = (64, 75)\n",
    "    \"\"\"\n",
    "    dw = np.dot(dz.T, x) # shape = (75, 128)\n",
    "    dx = np.dot(w.T, dz.T) # shape = (128, 64)\n",
    "    return dw, dx\n",
    "\n",
    "\n",
    "def backward_add_gate(x1, x2, dz):\n",
    "    dx1 = dz * np.ones_like(x1)\n",
    "    dx2 = dz * np.ones_like(x2)\n",
    "    return dx1, dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9844757",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先依据公式创建三个门的权重:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{I}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x i}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h i}+\\boldsymbol{b}_{i}\\right) \\\\\n",
    "\\boldsymbol{F}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x f}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h f}+\\boldsymbol{b}_{f}\\right) \\\\\n",
    "\\boldsymbol{O}_{t} &=\\sigma\\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x o}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h o}+\\boldsymbol{b}_{o}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb4378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hi = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "W_hf = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "W_ho = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7779f1a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;之后需要产生**候选记忆单元**：\n",
    "\n",
    "$$\n",
    "\\tilde{\\boldsymbol{C}}_{t}=\\tanh \\left(\\boldsymbol{X}_{t} \\boldsymbol{W}_{x c}+\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{h c}+\\boldsymbol{b}_{c}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;**记忆单元**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{C}_{t}=\\boldsymbol{F}_{t} \\odot \\boldsymbol{C}_{t-1}+\\boldsymbol{I}_{t} \\odot \\tilde{\\boldsymbol{C}}_{t}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;`LSTM`与`RNN、GRU`的区别是里面所含的状态有两个，一个是$C$，另外一个是$H$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005b2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hc = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3364c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**隐状态**:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_{t}=\\boldsymbol{O}_{t} \\odot \\tanh \\left(\\boldsymbol{C}_{t}\\right)\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$\\tanh$是为了保证值是在`+1`到`-1`之间的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5110087",
   "metadata": {},
   "source": [
    "&emsp;&emsp;之后我们再创建一个处理输入的权重矩阵，本来是有四个的$W_{xi}, W_{xf}, W_{xo}, W_{xc}$，这里我们为了方便起见，统一为一个$U$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2540fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.random.randn(hidden_dim, dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690db3e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;之后还有一个输出层的权重，也将其全部设置为$V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c12dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.random.randn(dimension, hidden_dim) / np.sqrt(hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e565a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, grad=False):\n",
    "    if grad:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward_lstm_recurrent(x, c_state, h_state, U, W_hf, W_hi, W_hc, W_ho, V):\n",
    "    \n",
    "    # 计算输入的处理单元。\n",
    "    mul_u = np.dot(x, U.T)\n",
    "    \n",
    "    # 计算遗忘门\n",
    "    mul_Wf = np.dot(h_state, W_hf.T)\n",
    "    add_Wf = mul_u + mul_Wf\n",
    "    f = sigmoid(add_Wf)\n",
    "    \n",
    "    # 计算输入门\n",
    "    mul_Wi = np.dot(h_state, W_hi.T)\n",
    "    add_Wi = mul_u + mul_Wi\n",
    "    i = sigmoid(add_Wi)\n",
    "    \n",
    "    # 计算候选记忆单元\n",
    "    mul_Wc = np.dot(h_state, W_hc.T)\n",
    "    add_Wc = mul_u + mul_Wc\n",
    "    c_hat = tanh(add_Wc)\n",
    "    \n",
    "    # 记忆单元选择需要从之前的c_state中遗忘多少，从输入门中提取多少候选记忆单元。\n",
    "    C = c_state * f + i * c_hat\n",
    "    \n",
    "    # 输出门\n",
    "    mul_Wo = np.dot(h_state, W_ho.T)\n",
    "    add_Wo = mul_u + mul_Wo\n",
    "    o = sigmoid(add_Wo)\n",
    "    \n",
    "    # 计算隐藏状态。\n",
    "    h = o * tanh(C)\n",
    "    \n",
    "    mul_v = np.dot(h, V.T)\n",
    "    return (mul_u, mul_Wf, add_Wf, mul_Wi, add_Wi, mul_Wc, add_Wc, C, mul_Wo, add_Wo, h, mul_v, i, o, c_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f67404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_recurrent(x, c_state, h_state, U, Wf, Wi, Wc, Wo, V, d_mul_v, saved_graph):\n",
    "    mul_u, mul_Wf, add_Wf, mul_Wi, add_Wi, mul_Wc, add_Wc, C, mul_Wo, add_Wo, h, mul_v, i, o, c_hat = saved_graph\n",
    "    dV, dh = backward_multiply_gate(V, h, d_mul_v)\n",
    "    dC = tanh(C, True) * o * dh.T\n",
    "    do = tanh(C) * dh.T\n",
    "    dadd_Wo = sigmoid(add_Wo, True) * do\n",
    "    dmul_u1, dmul_Wo = backward_add_gate(mul_u, mul_Wo, dadd_Wo)\n",
    "    dWo, dprev_state = backward_multiply_gate(Wo, h_state, dmul_Wo)\n",
    "    dc_hat = dC * i\n",
    "    dadd_Wc = tanh(add_Wc, True) * dc_hat\n",
    "    dmul_u2, dmul_Wc = backward_add_gate(mul_u, mul_Wc, dadd_Wc)\n",
    "    dWc, dprev_state = backward_multiply_gate(Wc, h_state, dmul_Wc)\n",
    "    di = dC * c_hat\n",
    "    dadd_Wi = sigmoid(add_Wi, True) * di\n",
    "    dmul_u3, dmul_Wi = backward_add_gate(mul_u, mul_Wi, dadd_Wi)\n",
    "    dWi, dprev_state = backward_multiply_gate(Wi, h_state, dmul_Wi)\n",
    "    df = dC * c_state\n",
    "    dadd_Wf = sigmoid(add_Wf, True) * df\n",
    "    dmul_u4, dmul_Wf = backward_add_gate(mul_u, mul_Wf, dadd_Wf)\n",
    "    dWf, dprev_state = backward_multiply_gate(Wf, h_state, dmul_Wf)\n",
    "    dU, dx = backward_multiply_gate(U, x, dmul_u4)\n",
    "    return (dU, dWf, dWi, dWc, dWo, dV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c36d0afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 3.921067552998206, accuracy 0.10546875\n",
      "epoch 100, loss 3.575667178766933, accuracy 0.10026041666666667\n",
      "epoch 150, loss 3.3109259011061094, accuracy 0.18359375\n",
      "epoch 200, loss 3.191043920397957, accuracy 0.22916666666666666\n",
      "epoch 250, loss 2.980070036653099, accuracy 0.2994791666666667\n",
      "epoch 300, loss 2.9500002072683067, accuracy 0.30078125\n",
      "epoch 350, loss 2.7624340653346326, accuracy 0.3294270833333333\n",
      "epoch 400, loss 2.778283871115836, accuracy 0.3268229166666667\n",
      "epoch 450, loss 2.788082161722753, accuracy 0.3111979166666667\n",
      "epoch 500, loss 2.563601537836983, accuracy 0.3841145833333333\n",
      "epoch 550, loss 2.584979086319973, accuracy 0.3385416666666667\n",
      "epoch 600, loss 2.520896597699117, accuracy 0.3828125\n",
      "epoch 650, loss 2.707388597092463, accuracy 0.3151041666666667\n",
      "epoch 700, loss 2.3654156105836175, accuracy 0.4075520833333333\n",
      "epoch 750, loss 2.4318962914566575, accuracy 0.41015625\n",
      "epoch 800, loss 2.311684977494303, accuracy 0.41015625\n",
      "epoch 850, loss 2.3953958969078815, accuracy 0.3841145833333333\n",
      "epoch 900, loss 2.3730333583598973, accuracy 0.3919270833333333\n",
      "epoch 950, loss 2.274032824544257, accuracy 0.4466145833333333\n",
      "epoch 1000, loss 2.404089827100547, accuracy 0.40234375\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension))\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)\n",
    "    \n",
    "    prev_c = np.zeros((batch_size, hidden_dim))\n",
    "    prev_h = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "        \n",
    "    layers = []\n",
    "    out_logits = np.zeros((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        layers.append(forward_lstm_recurrent(batch_x[:, n, :], prev_c, prev_h, U, W_hf, W_hi, W_hc, W_ho, V))\n",
    "        \n",
    "        prev_c = layers[-1][7]\n",
    "        prev_h = layers[-1][10]\n",
    "        \n",
    "        out_logits[:, n, :] = layers[-1][-4]\n",
    "        \n",
    "    probs = softmax(out_logits.reshape((-1, dimension)))\n",
    "    y = np.argmax(batch_y.reshape((-1, dimension)), axis=1)\n",
    "    accuracy = np.mean(np.argmax(probs, axis=1) == y)\n",
    "    loss = cross_entropy(probs, batch_y.reshape((-1, dimension)))\n",
    "    \n",
    "    delta = probs\n",
    "    delta[range(y.shape[0]), y] -= 1\n",
    "    delta = delta.reshape((batch_size, sequence_length, dimension))\n",
    "    \n",
    "    dU = np.zeros(U.shape)\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW_hf = np.zeros(W_hf.shape)\n",
    "    dW_hi = np.zeros(W_hi.shape)\n",
    "    dW_hc = np.zeros(W_hc.shape)\n",
    "    dW_ho = np.zeros(W_ho.shape)\n",
    "    \n",
    "    prev_c = np.zeros((batch_size, hidden_dim))\n",
    "    prev_h = np.zeros((batch_size, hidden_dim))\n",
    "    \n",
    "    for n in range(sequence_length):\n",
    "        d_mul_v = delta[:, n, :]\n",
    "        dU_t, dWf_t, dWi_t, dWc_t, dWo_t, dV_t = backward_recurrent(batch_x[:,n,:], prev_c, prev_h, U, W_hf, W_hi, \n",
    "                                                                    W_hc, W_ho, V, d_mul_v, layers[n])\n",
    "        prev_c = layers[n][7]\n",
    "        prev_h = layers[n][10]\n",
    "        dU += dU_t\n",
    "        dV += dV_t\n",
    "        dW_hf += dWf_t\n",
    "        dW_hi += dWi_t\n",
    "        dW_hc += dWc_t\n",
    "        dW_ho += dWo_t\n",
    "    U -= learning_rate * dU\n",
    "    V -= learning_rate * dV\n",
    "    W_hf -= learning_rate * dW_hf\n",
    "    W_hi -= learning_rate * dW_hi\n",
    "    W_hc -= learning_rate * dW_hc\n",
    "    W_ho -= learning_rate * dW_ho\n",
    "    if (i+1) % 50 == 0:\n",
    "        print('epoch {}, loss {}, accuracy {}'.format(i+1, loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aadc38",
   "metadata": {},
   "source": [
    "### Pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a22ea617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (LSTM): LSTM(75, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=75, bias=True)\n",
      ")\n",
      "epoch 50, loss 2.1111416816711426, accuracy 0.4856770833333333\n",
      "epoch 100, loss 1.1410794258117676, accuracy 0.7096354166666666\n",
      "epoch 150, loss 0.8636826872825623, accuracy 0.76953125\n",
      "epoch 200, loss 0.6843307018280029, accuracy 0.8033854166666666\n",
      "epoch 250, loss 0.6464931964874268, accuracy 0.82421875\n",
      "epoch 300, loss 0.5510137677192688, accuracy 0.8346354166666666\n",
      "epoch 350, loss 0.5469765067100525, accuracy 0.8463541666666666\n",
      "epoch 400, loss 0.47095003724098206, accuracy 0.8346354166666666\n",
      "epoch 450, loss 0.44830718636512756, accuracy 0.8489583333333334\n",
      "epoch 500, loss 0.50926673412323, accuracy 0.8372395833333334\n",
      "epoch 550, loss 0.4441101849079132, accuracy 0.8541666666666666\n",
      "epoch 600, loss 0.45717760920524597, accuracy 0.8424479166666666\n",
      "epoch 650, loss 0.4530371427536011, accuracy 0.8359375\n",
      "epoch 700, loss 0.45306655764579773, accuracy 0.8450520833333334\n",
      "epoch 750, loss 0.44778844714164734, accuracy 0.8580729166666666\n",
      "epoch 800, loss 0.4754592478275299, accuracy 0.8463541666666666\n",
      "epoch 850, loss 0.4246915578842163, accuracy 0.8515625\n",
      "epoch 900, loss 0.42124176025390625, accuracy 0.8489583333333334\n",
      "epoch 950, loss 0.3932032287120819, accuracy 0.85546875\n",
      "epoch 1000, loss 0.41456151008605957, accuracy 0.8411458333333334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        super(LSTM, self).__init__()\n",
    "        self.LSTM = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_dim))\n",
    "        # out, (hn, hc) = self.LSTM(x, (h0, c0))\n",
    "        out, (hn, hc) = self.LSTM(x, None)  # 得到所有时间序列的输出。\n",
    "        \n",
    "        # out = self.fc(out[:, -1, :])  # 取最后一个时间步的输出。\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "lstm = LSTM(input_dim = dimension, hidden_dim = hidden_dim, num_layers=1, output_dim = dimension)\n",
    "print(lstm)\n",
    "\n",
    "\n",
    "# 分开定义softmax运算和交叉熵损失函数会造成数值不稳定。\n",
    "# 因此PyTorch提供了一个具有良好数值稳定性且包括softmax运算和交叉熵计算的函数。\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(epoch):\n",
    "    batch_x = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_y = np.zeros((batch_size, sequence_length, dimension), dtype=np.float32)\n",
    "    batch_id = random.sample(possible_batch_id, batch_size)  # 随机采样，选择batch_id。\n",
    "    # prev_s = np.zeros((batch_size, hidden_dim))\n",
    "    for n in range(sequence_length):\n",
    "        id1 = [k + n for k in batch_id]\n",
    "        id2 = [k + n + 1 for k in batch_id]\n",
    "        \n",
    "        batch_x[:, n, :] = one_hot[id1]\n",
    "        batch_y[:, n, :] = one_hot[id2]\n",
    "    \n",
    "    # 从Numpy转成torch之后送入神经网络中去。\n",
    "    output = lstm(torch.from_numpy(batch_x))  # torch.Size([64, 12, 75])\n",
    "    label = torch.argmax(torch.from_numpy(batch_y).view(-1, dimension), dim=1) # shape = 786\n",
    "    \n",
    "    accuracy = np.mean(torch.argmax(output.view(-1, dimension), axis=1).numpy() == label.numpy())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output.view(-1, dimension), label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(i + 1) % 50 == 0:\n",
    "        print(\"epoch {}, loss {}, accuracy {}\".format(i+1, loss.item(), accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f4b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
