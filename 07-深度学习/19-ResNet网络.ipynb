{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55dad70f",
   "metadata": {},
   "source": [
    "## 残差网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d89b35",
   "metadata": {},
   "source": [
    "### 残差块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4422c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e800f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        if use_1x1conv:  # 如果要使用1x1的卷积核的话, 能够把输入通道数变成输出通道数。\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b885d8",
   "metadata": {},
   "source": [
    "### 输入和输出形状一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4094a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 3)\n",
    "X = torch.rand(4, 3, 6, 6)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb94f6b",
   "metadata": {},
   "source": [
    "通常来说，增加输出通道数的同时，我们会减半输出的高和宽。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687c11f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3, 6, use_1x1conv=True, strides=2)\n",
    "blk(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ecb50",
   "metadata": {},
   "source": [
    "## ResNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a656e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                  nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda55606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb4859",
   "metadata": {},
   "source": [
    "之后在ResNet加入所有残差块，这里每个模块使用两个残差块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8ca0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55a5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                   nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                   nn.Flatten(), nn.Linear(512, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde36b4",
   "metadata": {},
   "source": [
    "&emsp;&emsp;可以观察一下ResNet中不同模块的输入形状是如何变化的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63709f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 64, 56, 56])\n",
      "Sequential output shape:\t torch.Size([1, 128, 28, 28])\n",
      "Sequential output shape:\t torch.Size([1, 256, 14, 14])\n",
      "Sequential output shape:\t torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 224, 224))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, \"output shape:\\t\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5bf0af",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07e2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "\n",
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    \n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "        \n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)\n",
    "    \n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "484407dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinyzqh/miniforge3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-wdh4qigd/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d171f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4518989",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = argmax(y_hat, axis=1)\n",
    "    cmp = astype(y_hat, y.dtype) == y\n",
    "    return float(reduce_sum(astype(cmp, y.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03f7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "        if not device: # 如果没有指定device, 就用网络参数中的所给定的那个device。\n",
    "            device = next(iter(net.parameters())).device\n",
    "    \n",
    "    metric = Accumulator(2)\n",
    "    \n",
    "    for X, y in data_iter:\n",
    "        if isinstance(X, list):\n",
    "            X = [x.to(device) for x in X]  # 将数据挪到device上。\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            \n",
    "        y = y.to(device)\n",
    "        \n",
    "        metric.add(accuracy(net(X), y), y.numel())\n",
    "        \n",
    "        return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbfbf563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "    net.apply(init_weights)  # 对net里面的每一个parameter都去run一下init_weights这个函数。\n",
    "    print(\"trainng on {}\".format(device))\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    num_batches = len(train_iter)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_hat = net(X)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            \n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            \n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                print(\"epoch {}, train_loss {}, train_acc {}\".format(epoch + (i + 1) / num_batches, \n",
    "                                                                  train_loss, train_acc))\n",
    "    test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "    print(f'loss {train_loss:.3f}, train acc {train_acc:.3f}, 'f'test acc {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f501a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "234e4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainng on cpu\n",
      "epoch 0.2, train_loss 7.410240459949412, train_acc 0.20462101063829788\n",
      "epoch 0.4, train_loss 4.719084936253568, train_acc 0.22913896276595744\n",
      "epoch 0.6, train_loss 3.597994198190405, train_acc 0.2984264184397163\n",
      "epoch 0.8, train_loss 2.98771570939967, train_acc 0.3544714095744681\n",
      "epoch 1.0, train_loss 2.5912207761128743, train_acc 0.4076666666666667\n",
      "epoch 1.2, train_loss 0.8108842208030376, train_acc 0.6983045212765957\n",
      "epoch 1.4, train_loss 0.7390604621552407, train_acc 0.7234458111702128\n",
      "epoch 1.6, train_loss 0.6951395368322413, train_acc 0.7400820035460993\n",
      "epoch 1.8, train_loss 0.6646418644392744, train_acc 0.7513505651595744\n",
      "epoch 2.0, train_loss 0.6396222532908121, train_acc 0.7609166666666667\n",
      "epoch 2.2, train_loss 0.5163917211776085, train_acc 0.8070977393617021\n",
      "epoch 2.4, train_loss 0.5001508279683742, train_acc 0.8115442154255319\n",
      "epoch 2.6, train_loss 0.4914179598608761, train_acc 0.815021054964539\n",
      "epoch 2.8, train_loss 0.47974256235868373, train_acc 0.8204163896276596\n",
      "epoch 3.0, train_loss 0.47336694145202635, train_acc 0.8234\n",
      "epoch 3.2, train_loss 0.42820750018383597, train_acc 0.8420046542553191\n",
      "epoch 3.4, train_loss 0.4253735824468288, train_acc 0.8415890957446809\n",
      "epoch 3.6, train_loss 0.4153395415620601, train_acc 0.8449412677304965\n",
      "epoch 3.8, train_loss 0.4105095733353432, train_acc 0.8462849069148937\n",
      "epoch 4.0, train_loss 0.4062767333348592, train_acc 0.84795\n",
      "epoch 4.2, train_loss 0.3666878019241577, train_acc 0.863031914893617\n",
      "epoch 4.4, train_loss 0.36911246846330925, train_acc 0.8621176861702128\n",
      "epoch 4.6, train_loss 0.37073186378107004, train_acc 0.8617021276595744\n",
      "epoch 4.8, train_loss 0.37183365954997694, train_acc 0.8612242353723404\n",
      "epoch 5.0, train_loss 0.3676894569396973, train_acc 0.8628333333333333\n",
      "epoch 5.2, train_loss 0.35546450094973786, train_acc 0.8666057180851063\n",
      "epoch 5.4, train_loss 0.3447724905736903, train_acc 0.8705535239361702\n",
      "epoch 5.6, train_loss 0.34116401031930393, train_acc 0.8723958333333334\n",
      "epoch 5.8, train_loss 0.34124654967417106, train_acc 0.8721534242021277\n",
      "epoch 6.0, train_loss 0.3389557561556498, train_acc 0.8729666666666667\n",
      "epoch 6.2, train_loss 0.3200662608476395, train_acc 0.8814827127659575\n",
      "epoch 6.4, train_loss 0.363003098742759, train_acc 0.8660654920212766\n",
      "epoch 6.6, train_loss 0.3817505509929454, train_acc 0.8584607712765957\n",
      "epoch 6.8, train_loss 0.3808493134664728, train_acc 0.8587308843085106\n",
      "epoch 7.0, train_loss 0.37355895137786865, train_acc 0.8608\n",
      "epoch 7.2, train_loss 0.3280102569372096, train_acc 0.8741688829787234\n",
      "epoch 7.4, train_loss 0.32748880649500706, train_acc 0.8744182180851063\n",
      "epoch 7.6, train_loss 0.32480671092973534, train_acc 0.8766068262411347\n",
      "epoch 7.8, train_loss 0.3234797151006283, train_acc 0.8775556848404256\n",
      "epoch 8.0, train_loss 0.32256396236419677, train_acc 0.8784333333333333\n",
      "epoch 8.2, train_loss 0.2983757733030522, train_acc 0.8887965425531915\n",
      "epoch 8.4, train_loss 0.2984834868223109, train_acc 0.8878407579787234\n",
      "epoch 8.6, train_loss 0.29639593020398564, train_acc 0.8885749113475178\n",
      "epoch 8.8, train_loss 0.2986763301999011, train_acc 0.8876122007978723\n",
      "epoch 9.0, train_loss 0.2961099327723185, train_acc 0.8886166666666667\n",
      "epoch 9.2, train_loss 0.27558809930973865, train_acc 0.898936170212766\n",
      "epoch 9.4, train_loss 0.2785692016812081, train_acc 0.895279255319149\n",
      "epoch 9.6, train_loss 0.281725027354051, train_acc 0.8937555407801419\n",
      "epoch 9.8, train_loss 0.28007949603364823, train_acc 0.8939702460106383\n",
      "epoch 10.0, train_loss 0.2797756888707479, train_acc 0.89445\n",
      "loss 0.280, train acc 0.894, test acc 0.875\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.9, 10\n",
    "train(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c68c7",
   "metadata": {},
   "source": [
    "## ResNet为什么能训练出1000层的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ab2cb",
   "metadata": {},
   "source": [
    "&emsp;&emsp;解决梯度消失的一个办法就是将乘法变加法。\n",
    "\n",
    "1. 一般的全连接神经网络可以表示为$y = f(x)$, 对其求梯度下降法我们可以得到$\\frac{\\partial y}{\\partial w}$, $w = w - \\eta \\frac{\\partial y}{\\partial w}$。\n",
    "\n",
    "2. 如果是更深的网络的话，可以描述为$y^{\\prime} = g(f(x))$, $\\frac{y^{\\prime}}{\\partial w} = \\frac{\\partial y^{\\prime}}{\\partial y} \\frac{\\partial y}{\\partial w} = \\frac{\\partial g(y)}{\\partial y} \\frac{\\partial y}{\\partial w}$。其中$\\frac{\\partial g(y)}{\\partial y}$与预测效果相关，如果预测地比较好，那么loss就会比较小，以致于梯度是比较小的一个数，以致于用乘法的话就会使得梯度消失。\n",
    "\n",
    "3. 如果是残差网络的话，$y^{\\prime \\prime} = f(x) + g(f(x))$, 对其求导数之后，我们可以得到$\\frac{\\partial y^{\\prime \\prime}}{\\partial w} = \\frac{\\partial y}{\\partial w} + \\frac{y^{\\prime}}{\\partial w}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677e1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe000f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
