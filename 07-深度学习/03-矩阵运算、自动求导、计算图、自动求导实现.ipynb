{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f782c4",
   "metadata": {},
   "source": [
    "## 矩阵运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7692e4",
   "metadata": {},
   "source": [
    "### 标量导数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9ac97",
   "metadata": {},
   "source": [
    "$$\n",
    "y = uv\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;求导之后为:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{du}{dx} v + \\frac{dv}{dx} u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073340e",
   "metadata": {},
   "source": [
    "### 亚导数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151fe94a",
   "metadata": {},
   "source": [
    "&emsp;&emsp;将导数扩展到不可微的函数，比如$y=|x|$。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial|x|}{\\partial x}= \\begin{cases}1 & \\text { if } x>0 \\\\ -1 & \\text { if } x<0 \\\\ a & \\text { if } x=0, \\quad a \\in[-1,1]\\end{cases}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;在$x=0$的时候，可以在`-1，1`之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b083b1",
   "metadata": {},
   "source": [
    "### 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17618bf",
   "metadata": {},
   "source": [
    "- $y$是一个标量，$x$是一个向量的时候:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}=\\left[\\begin{array}{c}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{n}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\mathbf{x}}=\\left[\\frac{\\partial y}{\\partial x_{1}}, \\frac{\\partial y}{\\partial x_{2}}, \\ldots, \\frac{\\partial y}{\\partial x_{n}}\\right]\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;标量关于列向量的导数会变成一个行向量。**梯度方向与等高线正交，且方向指向值变大的方向**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc440e97",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**样例**\n",
    "\n",
    "1. $y = ||\\mathbf{x}||^{2}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\mathbf{x}} = 2 \\mathbf{x}^{T}\n",
    "$$\n",
    "\n",
    "2. $y = <u, v>$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\mathbf{x}} = \\mathbf{u}^{T} \\frac{\\partial v}{\\partial x} + \\mathbf{v}^{T} \\frac{\\partial u}{\\partial x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf37416",
   "metadata": {},
   "source": [
    "- $y$是一个向量，$x$是一个标量的时候:\n",
    "\n",
    "&emsp;&emsp;当上面的函数是一个向量，下面的函数是一个标量的话，假设$y$是一个列向量\n",
    "\n",
    "$$\n",
    "\\mathbf{y}=\\left[\\begin{array}{c}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{m}\n",
    "\\end{array}\\right] \\quad \\frac{\\partial \\mathbf{y}}{\\partial x}=\\left[\\begin{array}{c}\n",
    "\\frac{\\partial y_{1}}{\\partial x} \\\\\n",
    "\\frac{\\partial y_{2}}{\\partial x} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial y_{m}}{\\partial x}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;列向量关于标量的导数，他也是一个列向量。这个被称之为分子布局符号，反过来的版本叫做分母布局符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861bf5a6",
   "metadata": {},
   "source": [
    "- $y$是一个向量，$x$是一个向量的时候:\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\mathbf{x}=\\left[\\begin{array}{c}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{n}\n",
    "\\end{array}\\right] \\quad \\mathbf{y}=\\left[\\begin{array}{c}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{m}\n",
    "\\end{array}\\right] \\\\\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\\left[\\begin{array}{c}\n",
    "\\frac{\\partial y_{1}}{\\partial \\mathbf{x}} \\\\\n",
    "\\frac{\\partial y_{2}}{\\partial \\mathbf{x}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial y_{m}}{\\partial \\mathbf{x}}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}}, \\frac{\\partial y_{1}}{\\partial x_{2}}, \\ldots, \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\\n",
    "\\frac{\\partial y_{2}}{\\partial x_{1}}, \\frac{\\partial y_{2}}{\\partial x_{2}}, \\ldots, \\frac{\\partial y_{2}}{\\partial x_{n}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial y_{m}}{\\partial x_{1}}, \\frac{\\partial y_{m}}{\\partial x_{2}}, \\ldots, \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{array}\\right]\n",
    "\\end{gathered}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345c39c",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**样例**\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccc}\n",
    "\\mathbf{y} & \\mathbf{a} & \\mathbf{x} & \\mathbf{A x} & \\mathbf{x}^{T} \\mathbf{A} \\\\\n",
    "\\hline \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} & \\mathbf{0} & \\mathbf{I} & \\mathbf{A} & \\mathbf{A}^{T}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l|lll}\n",
    "\\mathbf{y} & \\ {a \\mathbf{u}} & \\mathbf{A u} & \\mathbf{u}+\\mathbf{v} \\\\\n",
    "\\hline \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} & a \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} & \\mathbf{A} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} & \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}+\\frac{\\partial \\mathbf{v}}{\\partial \\mathbf{x}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;$\\mathbf{x} \\in \\mathbb{R}^{n}, \\quad \\mathbf{y} \\in \\mathbb{R}^{m}, \\quad \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\in \\mathbb{R}^{m \\times n}$\n",
    "$a, \\mathbf{a}$ and $\\mathbf{A}$ are not functions of $\\mathbf{x}$\n",
    "0 and I are matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18458c6",
   "metadata": {},
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3668be79",
   "metadata": {},
   "source": [
    "- 标量链式法则:\n",
    "\n",
    "$$\n",
    "y=f(u), u=g(x) \\quad \\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial x}\n",
    "$$\n",
    "\n",
    "- 扩展到向量\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial y}{\\partial \\mathbf{x}}=\\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial \\mathbf{x}} \\quad \\frac{\\partial y}{\\partial \\mathbf{x}}=\\frac{\\partial y}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}} \\quad \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} k=\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{u}} \\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "&emsp;&emsp;对应的下标为维度为:\n",
    "\n",
    "$$\n",
    "(1, n) \\rightarrow \\ (1,)(1, n) \\quad \\ (1, n) \\rightarrow \\  (1, k) \\ \\ (k, n) \\quad(m, n) \\rightarrow \\  (m, k)(k, n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa8a7f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;自动求导计算一个函数在指定值上的导数，他有别于符号求导，和数值求导: $\\frac{\\partial f(x)}{\\partial x}=\\lim _{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e3ba9",
   "metadata": {},
   "source": [
    "## 计算图\n",
    "\n",
    "计算图等价于链式法则求导的过程。将代码分解成操作子，一步一步展开，再将计算表示成一个无环的图：\n",
    "\n",
    "<img src=\"../images/com_graph.jpeg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa8ceb",
   "metadata": {},
   "source": [
    "自动求导有两种模式：\n",
    "\n",
    "1. 正向累积\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial u_{n}}\\left(\\frac{\\partial u_{n}}{\\partial u_{n-1}}\\left(\\ldots\\left(\\frac{\\partial u_{2}}{\\partial u_{1}} \\frac{\\partial u_{1}}{\\partial x}\\right)\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "2. 反向累积、又称反向传递\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x}=\\left(\\left(\\left(\\frac{\\partial y}{\\partial u_{n}} \\frac{\\partial u_{n}}{\\partial u_{n-1}}\\right) \\ldots\\right) \\frac{\\partial u_{2}}{\\partial u_{1}}\\right) \\frac{\\partial u_{1}}{\\partial x}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f535c",
   "metadata": {},
   "source": [
    "## 自动求导的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2123fcd3",
   "metadata": {},
   "source": [
    "&emsp;&emsp;假设我们想对函数$y=2 \\mathbf{x}^{T} \\mathbf{x}$关于列向量$\\mathbf{x}$求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac078c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a2aaf",
   "metadata": {},
   "source": [
    "在计算$y$关于$\\mathbf{x}$的梯度之前，我们需要一个地方来存储梯度，就需要通过x.requires_grad_来告诉计算机，需要将梯度存在某个地方。之后通过x.grad就可以访问它的梯度了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5478ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True) # 等价于 `x = torch.arange(4.0, requires_grad=True)`\n",
    "x.grad # 默认值是None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4598fbe",
   "metadata": {},
   "source": [
    "现在让我们计算$y$，就是$x$与$x$内积的计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ec69a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e4745",
   "metadata": {},
   "source": [
    "通过调用反向传播函数来自动计算$y$关于$x$每个分量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e398008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dcdf2",
   "metadata": {},
   "source": [
    "我们知道梯度会等于4 * x，我们可以来验证这一点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54ec8bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e814ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a6190",
   "metadata": {},
   "source": [
    "在默认情况下，pytorch会将所有的梯度累积起来，所以在计算下一个的时候，我们需要将之前的值清除。现在让我们计算$x$的另一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f11fb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97636e03",
   "metadata": {},
   "source": [
    "假设y不是一个标量，会怎么样？假设x是一个向量，y也是一个向量。理论上它的backward因该是一个矩阵，但是在深度学习中我们很少对向量的函数来求导，大部分的时候我们是对一个标量来求导。\n",
    "\n",
    "深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9de921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caefeb15",
   "metadata": {},
   "source": [
    "将某些计算移动到记录的计算图之外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68532c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach() # 把y当作一个常数u。\n",
    "z = u * x # z关于x来讲，就是一个常数乘以x。\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6faf2dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc90fe30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5dfe8",
   "metadata": {},
   "source": [
    "即使构建函数的计算图需要通过Python控制流(例如，条件、循环或任意函数调用)，我们仍然可以计算得到的变量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e48016e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()\n",
    "\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b72f8",
   "metadata": {},
   "source": [
    "torch在每次计算的时候会将计算图存下来，之后反向传播的时候就倒着计算一遍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3223b0",
   "metadata": {},
   "source": [
    "## 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9a9cd",
   "metadata": {},
   "source": [
    "1. pytorch为什么会默认梯度累加？\n",
    "\n",
    "方便内存不够的时候，将批量做小。\n",
    "\n",
    "2. 为什么深度学习中一般对标量求导，而不是对矩阵或者向量，如果我的loss是包含向量或者矩阵，那求导之前是不是要把他们变成标量？\n",
    "\n",
    "\n",
    "如果是向量的话，对矩阵的loss就会变成一个矩阵，矩阵再往下走一层就会变成一个四维矩阵。神经网络一深，就会变成一个特别特别大的张量。\n",
    "\n",
    "3. 多个loss分别反向传播的时候，是不是需要累积梯度？\n",
    "\n",
    "是这样的，这也是pytorch中为什么是默认累积梯度。\n",
    "\n",
    "4. 为什么获取.grad前需要backward?\n",
    "\n",
    "因为计算梯度是很贵的，默认不会计算梯度。\n",
    "\n",
    "5. 求导的过程一般来说是不是都是有向图，也就是可以用树状结构来表示，有没有其他环状的图结构？\n",
    "\n",
    "有的RNN就是一个环状的图。\n",
    "\n",
    "6. pytorch框架设计上可以实现矢量的求导吗？\n",
    "\n",
    "是可以的，高阶求导，比如二阶求导。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
