{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c40d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c087de70",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01d2e5f9",
   "metadata": {},
   "source": [
    "## Patch Embedding\n",
    "\n",
    "将一幅图片转化成一个`patch embedding`。\n",
    "\n",
    "主要有`naive`的实现版本和`conv2d`的实现版本。\n",
    "\n",
    "首先是将图像切分成一个个`patch`，与`vit`一样。每个`patch`就像`NLP`中的每个`token`一样。在论文中，`patch`的大小取$4 \\times 4$, 每个`patch`中都存在三个通道，因此每个`patch`的像素点都是$4 \\times 4 \\times 3 = 48$个像素点。这是原始像素点，想要得到`patch embedding`的话，我们还需要将其通过一个线性层映射到长度为$C$的向量上。\n",
    "\n",
    "<img src=\"../../images/07-swintransformer.jpg\" width=\"80%\">\n",
    "\n",
    "\n",
    "每一个`block`是不会对像素做改变的。在运用`patch merging`的时候才会使得像素发生改变。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d52299",
   "metadata": {},
   "source": [
    "1. 方法一：基于unfold API来模仿卷积思路来实现对图像分块，设置kernel size = patch size。得到格式为[bs, num_patch, patch_size]的向量。将张量与[bs, patch_size, model_dim_C]的权重矩阵相乘，得到[bs, num_patch, model_dim_C]的patch embedding。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adc53a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41581989",
   "metadata": {},
   "source": [
    "输入图像大小都是按照卷积操作的图像大小给定的，维度为:`[bs * channel * h * w]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665f48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2emb_naive(image, patch_size, weight):\n",
    "    patch = F.unfold(image, kernel_size=(patch_size, patch_size),\n",
    "                    stride=(patch_size, patch_size)).transpose(-1, -2)  # [bs, num_patch, patch_depth]\n",
    "    patch_embedding = patch @ weight\n",
    "    return patch_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071130c",
   "metadata": {},
   "source": [
    "2. 方法二：就是直接通过卷积操作来实现，省去了与权重相乘的过程，卷积输出的通道数就是编码之后的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a577c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2emb_conv(image, kernel, stride):\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride) # [bs, oc, oh, ow]\n",
    "    bs, oc, oh, ow = conv_output.shape\n",
    "    patch_embedding = conv_output.reshape((bs, oc, oh * ow)).transpose(-1, -2)\n",
    "    return patch_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e608ec",
   "metadata": {},
   "source": [
    "## SwinTransformer Block\n",
    "\n",
    "`SwinTransformer`中的`swin`是`shift`和`window`的结合。`SwinTransformer Block`主要包括两部分，一个是`Window Multi-head Self-attention`和`Shifted Windows Multi-head Self-attention`。\n",
    "\n",
    "把`patch`做成在不同的`window`内，在`window`内做`self-attention`。每一个`window`内的`self-attention`复杂度和`window`内的`patch`数目成平方关系。而`window`数目与图片大小成线性关系，因此算法整体的复杂度也是会与图片大小成线性关系。`window`与`window`之间的连接是通过`shift window`来去实现的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e20775",
   "metadata": {},
   "source": [
    "### Window Multi-head Self-attention\n",
    "\n",
    "&emsp;&emsp;这里就是构建多头的注意力机制，并计算其复杂度。\n",
    "\n",
    "1. 将输入经过`mlp`映射到$q，k，v$。输入$x$的维度为$L \\times C$，其中$L$为序列长度，$C$为特征大小。映射矩阵的维度为$C \\times C$，所以这两个矩阵相乘的复杂度为$LC^{2}$。有三个映射，所以其复杂度为$3LC^{2}$。将$q，k，v$拆分乘多头的形式，也就是维度$C$拆分成$\\frac{C}{n}$, 但是这里的多头计算互不影响，也就是头与头之间并不会做`attention`的计算，因此可以与`batch size`的那一维度进行统一的看待。\n",
    "2. 之后我们就需要计算`attention`计算过程的复杂度，首先是$q k^{T}$, 其中$q$的矩阵维度为$L \\times C$, $k^{T}$的矩阵维度为$C \\times L$, 因此这两个矩阵相乘的复杂度为$L^{2}C$。\n",
    "3. 之后将计算得到的概率与$v$相乘，也就是$L \\times L$的矩阵与$L \\times C$的矩阵相乘，同样计算其复杂度为$L^{2}C$。\n",
    "4. 得到最终的`[bs, L, C]`的数据与`mlp`相乘，也就是$L \\times C$的矩阵与$C \\times C$的矩阵相乘，复杂度为$LC^{2}$。\n",
    "\n",
    "&emsp;&emsp;此时，我们可以得出总体的复杂度为:\n",
    "\n",
    "$$\n",
    "4LC^{2} + 2L^{2}C\n",
    "$$\n",
    "\n",
    "可以看出，重头是在做`attention`部分，复杂度与序列长度成平方关系。\n",
    "\n",
    "在计算$q k^{T}$时，我们需要考虑掩码，让两两无效的位置之间能量为负无穷，经过`softmax`之后，概率就会变成`0`。掩码是在`shift window MHSA`中会用到，在`window MHSA`中不会用到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a125ed5",
   "metadata": {},
   "source": [
    "在涉及模型，参数的定义的时候，我们需要写成`class`的形式，并且`module`需要继承自`nn.Module`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2799525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_head):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        \n",
    "        self.proj_linear_layer = nn.Linear(model_dim, 3*model_dim)\n",
    "        self.final_linear_layer = nn.Linear(model_dim, model_dim)\n",
    "    \n",
    "    def forward(self, input, additive_mask=None):\n",
    "        bs, seqlen, model_dim = input.shape\n",
    "        num_head = self.num_head\n",
    "        \n",
    "        head_dim = model_dim // self.num_head\n",
    "        proj_output = self.proj_linear_layer(input) # [bs, seqlen, 3 * model_dim]\n",
    "        q, k, v = proj_output.chunk(3, dim=-1)  # [bs, seqlen, model_dim]\n",
    "        \n",
    "        q = q.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)\n",
    "        q = q.reshape(bs * num_head, seqlen, head_dim)\n",
    "        \n",
    "        k = k.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)\n",
    "        k = k.reshape(bs * num_head, seqlen, head_dim)\n",
    "        \n",
    "        v = v.reshape(bs, seqlen, num_head, head_dim).transpose(1, 2)\n",
    "        v = v.reshape(bs * num_head, seqlen, head_dim)\n",
    "        \n",
    "        if additive_mask is None:\n",
    "            attn_prob = F.softmax(torch.bmm(q, k.transpose(-2, -1))/ math.sqrt(head_dim), dim=-1)\n",
    "        else:\n",
    "            additive_mask = additive_mask.tile((num_head, 1, 1))\n",
    "            attn_prob = F.softmax(torch.bmm(q, k.transpose(-2, -1))/ math.sqrt(head_dim) + additive_mask, dim=-1)\n",
    "            \n",
    "        output = torch.bmm(attn_prob, v)  # [bs * num_head, seqlen, head_dim]\n",
    "        output = output.reshape(bs, num_head, seqlen, head_dim).transpose(1, 2) # [bs, seqlen, num_head, head_dim]\n",
    "        output = output.reshape(bs, seqlen, model_dim)\n",
    "        \n",
    "        output = self.final_linear_layer(output)\n",
    "        return attn_prob, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6c858",
   "metadata": {},
   "source": [
    "之后，我们就需要去构建带window的MHSA，并计算其复杂度\n",
    "1. 之前像素做成的patch进一步划分成一个个更大的window。首先需要将三维的patch embedding转换成图片的形式，也就是将[bs, num_patch, model_dim]转成[bs, height, width]的形式。高度乘以宽度需要是patch的总数。采用unfold这个API将patch划分成window。\n",
    "\n",
    "2. 划分成window之后，我们就可以在window内部去划分MHSA。window数目可以与bs统一对待，因为window数据之间并不做attention。\n",
    "\n",
    "假设窗的边长是$W$，那么窗内元素的数目就是$W^{2}$，那么计算窗内的总体复杂度是$4W^{2}C^{2} + 2W^{4}C$。假设patch的总数目是$L$, 那么窗的数目就是$L/W^{2}$。因此W-MHSA的复杂度为$4LC^{2} + 2LW^{2}C$。\n",
    "\n",
    "复杂度对比：\n",
    "\n",
    "`MHSA`：$4LC^{2} + 2L^{2}C$。\n",
    "\n",
    "`W-MHSA`：$4LC^{2} + 2LW^{2}C$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cb83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_multi_head_self_attention(patch_embedding, mhsa, window_size=4, num_head=2):\n",
    "    num_patch_in_window = window_size * window_size\n",
    "    bs, num_patch, patch_depth = patch_embedding.shape\n",
    "    image_height = image_width = int(math.sqrt(num_patch))\n",
    "    \n",
    "    patch_embedding = patch_embedding.transpose(-1, -2)\n",
    "    patch = patch_embedding.reshape(bs, patch_depth, image_height, image_width)\n",
    "    window = F.unfold(patch, kernel_size=(window_size, window_size), \n",
    "                      stride=(window_size, window_size)).transpose(-1, -2)\n",
    "    \n",
    "    bs, num_window, patch_depth_times_num_patch_in_window = window.shape\n",
    "    window = window.reshape(bs * num_window, patch_depth, num_patch_in_window).transpose(-2, -1)\n",
    "    \n",
    "    attn_prob, output = mhsa(window)  # [bs * num_window, num_patch_in_window, patch_depth]\n",
    "    \n",
    "    output = output.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040b0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedef6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b835ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83a8f50d",
   "metadata": {},
   "source": [
    "### Shifted Windows Multi-head Self-attention\n",
    "\n",
    "1. 首先将上一步W-MHSA的结果转换成图片的形式，假设已经做了新的window的划分，这一步叫做shift window。为了保持window数目不变，从而有高效计算，需要将图片的patch往左和往上各自滑动半个窗口大小的步长，保持patch所属window类别不变。之后再将图片patch还原成window的数据格式。\n",
    "\n",
    "2. 由于cycle shift后，每个window虽然形状不规则，但部分window中存在原本不属于同一个窗口的patch，所以需要生成mask。\n",
    "\n",
    "3. 如何生成`mask`？构建`mask`时，首先需要去构建一个`shift-window`的`patch`所属的`window`类别矩阵，之后对该矩阵往上，往左滑动半个窗口大小的步长，之后通过`unfold`操作得到`[bs, num_window, num_patch_in_window]`形状的类别矩阵。之后将其扩充维度到`[bs, num_window, num_patch_in_window, 1]`这个维度。将该矩阵与其转置矩阵进行作差，得到同类关系矩阵（为0的位置上的patch属于同类，否则属于不同类）。对同类关系矩阵中非零位置用负无穷去填充，对于0的位置用0去填充，这样就构建好了MHSA所需的mask。此mask的形状为[bs, num_window, num_patch_in_window, num_patch_in_window]。\n",
    "\n",
    "4. 将window转换成三维的格式，[bs*num_window, num_patch_in_window, patch_depth]。\n",
    "5. 将三维格式的特征连同mask一起送入HSA中计算得到注意力的输出。\n",
    "6. 将注意力的输出转化成图片patch的格式，[bs, num_window, num_patch_in_window, patch_depth]。\n",
    "7. 未了恢复位置，需要将图片的patch往右和往下各自滑动半个窗口大小的步长，至此，SW-MHSA计算完毕。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e388b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数，将transformer block的结果转换成图片的格式。\n",
    "def window2image(msa_output):\n",
    "    bs, num_window, num_patch_in_window, patch_depth = msa_output.shape\n",
    "    window_size = int(math.sqrt(num_patch_in_window))\n",
    "    \n",
    "    image_height = int(math.sqrt(num_window)) * window_size\n",
    "    image_width = image_height\n",
    "    \n",
    "    msa_output = msa_output.reshape(bs, int(math.sqrt(num_window)),\n",
    "                                        int(math.sqrt(num_window)),\n",
    "                                        window_size,\n",
    "                                        window_size,\n",
    "                                        patch_depth)\n",
    "    \n",
    "    msa_output = msa_output.transpose(2, 3) \n",
    "    image = msa_output.reshape(bs, image_height*image_width, patch_depth)\n",
    "    \n",
    "    image = image.transpose(-1, -2).reshape(bs, patch_depth, image_height, image_width) # 跟卷积格式一致\n",
    "    return image\n",
    "\n",
    "# 辅助函数，高效计算swmsa\n",
    "def shift_window(w_msa_output, window_size, shift_size, generate_mask=False):\n",
    "    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "    \n",
    "    w_msa_output = window2image(w_msa_output)  # [bs, depth, h, w]\n",
    "    bs, patch_depth, image_height, image_width = w_msa_output.shape\n",
    "    \n",
    "    rolled_w_msa_input = torch.roll(w_msa_output, shifts=(shift_size, shift_size), dims=(2, 3))\n",
    "    \n",
    "    shift_w_msa_input = rolled_w_msa_input.reshape(bs, patch_depth,\n",
    "                                                   int(math.sqrt(num_window)),\n",
    "                                                   window_size,\n",
    "                                                   int(math.sqrt(num_window)),\n",
    "                                                   window_size\n",
    "                                                  )\n",
    "    \n",
    "    shift_w_msa_input = shift_w_msa_input.transpose(3, 4)\n",
    "    shift_w_msa_input = shift_w_msa_input.reshape(bs, patch_depth, num_window*num_patch_in_window)\n",
    "    shift_w_msa_input = shift_w_msa_input.transpose(-1, -2)\n",
    "    shift_window = shift_w_msa_input.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    \n",
    "    if generate_mask:\n",
    "        additive_mask = build_mask_for_shifted_wmsa(bs, image_height, image_width, window_size)\n",
    "    else:\n",
    "        additive_mask = None\n",
    "    \n",
    "    return shift_window, additive_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58e487",
   "metadata": {},
   "source": [
    "生成mask矩阵之前，首先需要生成类别矩阵index_matrix。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d641c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建shift window multi-head attention mask\n",
    "def build_mask_for_shifted_wmsa(batch_size, image_height, image_width, window_size):\n",
    "    index_matrix = torch.zeros(image_height, image_width)\n",
    "    \n",
    "    for i in range(image_height):\n",
    "        for j in range(image_width):\n",
    "            row_times = (i + window_size//2) // window_size\n",
    "            col_times = (j + window_size//2) // window_size\n",
    "            index_matrix[i, j] = row_times * (image_height // window_size) + col_times + 1\n",
    "    \n",
    "    rolled_index_matrix = torch.roll(index_matrix, shifts=(-window_size//2, -window_size//2), dims=(0, 1))\n",
    "    rolled_index_matrix = rolled_index_matrix.unsqueeze(0).unsqueeze(0)  # [bs, ch, h, w]\n",
    "    \n",
    "    c = F.unfold(rolled_index_matrix, kernel_size=(window_size, window_size),\n",
    "                 stride=(window_size, window_size)).transpose(-1, -2)\n",
    "    \n",
    "    c = c.tile(batch_size, 1, 1) # [bs, num_window, num_patch_in_window]\n",
    "    \n",
    "    bs, num_window, num_patch_in_window = c.shape\n",
    "    c1 = c.unsqueeze(-1) # [bs, num_window, num_patch_in_window, 1]\n",
    "    c2 = (c1 - c1.transpose(-1, -2)) == 0 # [bs, num_window, num_patch_in_window, num_patch_in_window]\n",
    "    valid_mask = c2.to(torch.float32)\n",
    "    \n",
    "    additive_mask = (1 - valid_mask)*(-1e-9) # [bs, num_window, num_patch_in_window, num_patch_in_window]\n",
    "    \n",
    "    additive_mask = additive_mask.reshape(bs*num_window, num_patch_in_window, num_patch_in_window)\n",
    "    \n",
    "    return additive_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b76cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "def shift_window_multi_head_self_attention(w_msa_output, mhsa, window_size=4, num_head=2):\n",
    "    \"\"\"\n",
    "    w_msa_output: 上一步shift window的输出。\n",
    "    mhsa: 新的实例化的mhsa的对象。\n",
    "    \"\"\"\n",
    "    bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "    \n",
    "    shifted_w_msa_input, additive_mask = shift_window(w_msa_output, window_size, \n",
    "                                                      shift_size=-window_size//2, generate_mask=True)\n",
    "    shifted_w_msa_input = shifted_w_msa_input.reshape(bs*num_window, num_patch_in_window, patch_depth)\n",
    "    \n",
    "    attn_prob, output = mhsa(shifted_w_msa_input, additive_mask=additive_mask)\n",
    "    output = output.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "    \n",
    "    output, _ = shift_window(output, window_size, shift_size=window_size//2, generate_mask=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9284cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc84473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c48fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23c81dc9",
   "metadata": {},
   "source": [
    "## Patch Merging\n",
    "\n",
    "`Patch Merging`主要实现的是像素的降低，将周围的`patch`浓缩成一个`patch`。\n",
    "\n",
    "`Patch Merging`是将`window`格式的特征转换成图片`patch`格式。利用`unfold`操作，按照`merge_size * merge_size`的大小得到新的`patch`，形状为`[bs, num_patch_new, merge_size * merge_size * patch_depth_old]`。\n",
    "\n",
    "使用全连接层对`depth`进行`0.5`倍的降维，输出`patch embedding`的形状格式`[bs, num_patch, patch_depth]`。\n",
    "\n",
    "举例说明：以`merge_size = 2`为例，经过`PatchMerging`后，数目减少为之前的`1/4`，但是`depth`增大为原来的两倍，而不是`4`倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c44492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, model_dim, merge_size, output_depth_scale=0.5):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.merge_size = merge_size\n",
    "        self.proj_layer = nn.Linear(\n",
    "            model_dim * merge_size * merge_size,\n",
    "            int(model_dim * merge_size * merge_size * output_depth_scale)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        bs, num_window, num_patch_in_window, patch_depth = input.shape\n",
    "        \n",
    "        window_size = int(math.sqrt(num_patch_in_window))\n",
    "        \n",
    "        input = window2image(input) # [bs, patch_depth, image_h, image_w]\n",
    "        \n",
    "        merge_window = F.unfold(input, kernel_size=(self.merge_size, self.merge_size),\n",
    "                               stride=(self.merge_size, self.merge_size)).transpose(-1, -2)\n",
    "        \n",
    "        merge_window = self.proj_layer(merge_window) # [bs, num_patch, new_patch_depth]\n",
    "        \n",
    "        return merge_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92816b",
   "metadata": {},
   "source": [
    "## 如何构建swin transformer block\n",
    "\n",
    "每个block包含LayerNorm，W-MHSA，MLP，SW-MHSA、残差连接等模块。\n",
    "输入是patch embedding格式\n",
    "每个MLP包含两层，分别是4*model_dim和model_dim大小。\n",
    "输出的是window的数据格式，[bs, num_window, num_patch_in_window, patch_depth]。\n",
    "需要注意残差连接对数据形状的要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea416c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    def __init__(self, model_dim, window_size, num_head):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm3 = nn.LayerNorm(model_dim)\n",
    "        self.layer_norm4 = nn.LayerNorm(model_dim)\n",
    "        \n",
    "        self.wsma_mlp1 = nn.Linear(model_dim, 4*model_dim)\n",
    "        self.wsma_mlp2 = nn.Linear(4*model_dim, model_dim)\n",
    "        \n",
    "        self.swsma_mlp1 = nn.Linear(model_dim, 4*model_dim)\n",
    "        self.swsma_mlp2 = nn.Linear(4*model_dim, model_dim)\n",
    "        \n",
    "        self.mhsa1 = MultiHeadSelfAttention(model_dim, num_head)\n",
    "        self.mhsa2 = MultiHeadSelfAttention(model_dim, num_head)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        bs, num_patch, patch_depth = input.shape\n",
    "        input1 = self.layer_norm1(input)\n",
    "        w_msa_output = window_multi_head_self_attention(input, self.mhsa1, window_size=4, num_head=2)\n",
    "        bs, num_window, num_patch_in_window, patch_depth = w_msa_output.shape\n",
    "        w_msa_output = input + w_msa_output.reshape(bs, num_patch, patch_depth)\n",
    "        output1 = self.wsma_mlp2(self.wsma_mlp1(self.layer_norm2(w_msa_output)))\n",
    "        \n",
    "        input2 = self.layer_norm3(output1)\n",
    "        input2 = input2.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "        sw_msa_output = shift_window_multi_head_self_attention(input2, self.mhsa2, window_size=4, num_head=2)\n",
    "        sw_msa_output = output1 + sw_msa_output.reshape(bs, num_patch, patch_depth)\n",
    "        output2 = self.swsma_mlp2(self.swsma_mlp1(self.layer_norm4(sw_msa_output)))\n",
    "        output2 += sw_msa_output\n",
    "        \n",
    "        output2 = output2.reshape(bs, num_window, num_patch_in_window, patch_depth)\n",
    "        \n",
    "        return output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0b237",
   "metadata": {},
   "source": [
    "## 如何构建swin transformermodel\n",
    "\n",
    "输入是图片，首先对图片进行分块得到patch embedding。\n",
    "\n",
    "经过第一个stage，进行patch merging，再进行第二个stage。以此类推\n",
    "\n",
    "对最后一个block的输出转换成patch embedding的格式，[bs, num_patch, patch_depth]。\n",
    "\n",
    "对patch embeding在时间维度进行平均池化，并映射到分类层，得到分类的logits。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96576562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerModel(nn.Module):\n",
    "    def __init__(self, input_image_channel=3, patch_size=4, model_dim_C=8, num_class=10,\n",
    "                window_size = 4, num_head = 2, merge_size = 2):\n",
    "        super(SwinTransformerModel, self).__init__()\n",
    "        patch_depth = patch_size * patch_size * input_image_channel\n",
    "        self.patch_size = patch_size\n",
    "        self.model_dim_C = model_dim_C\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.patch_embedding_weight = nn.Parameter(torch.randn(patch_depth, model_dim_C))\n",
    "        \n",
    "        self.block1 = SwinTransformerBlock(model_dim_C, window_size, num_head)\n",
    "        self.block2 = SwinTransformerBlock(model_dim_C*2, window_size, num_head)\n",
    "        self.block3 = SwinTransformerBlock(model_dim_C*4, window_size, num_head)\n",
    "        self.block4 = SwinTransformerBlock(model_dim_C*8, window_size, num_head)\n",
    "        \n",
    "        self.patch_merging1 = PatchMerging(model_dim_C, merge_size)\n",
    "        self.patch_merging2 = PatchMerging(model_dim_C*2, merge_size)\n",
    "        self.patch_merging3 = PatchMerging(model_dim_C*4, merge_size)\n",
    "        \n",
    "        self.final_layer = nn.Linear(model_dim_C*8, num_classes)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        patch_embedding_naive = image2emb_naive(image, self.patch_size, self.patch_embedding_weight)\n",
    "        \n",
    "        # block1\n",
    "        patch_embedding = patch_embedding_naive\n",
    "        print(patch_embedding.shape)\n",
    "        \n",
    "        sw_msa_output = self.block1(patch_embedding)\n",
    "        print(\"block 1 output\", sw_msa_output.shape)\n",
    "        \n",
    "        merged_patch1 = self.patch_merging1(sw_msa_output)\n",
    "        sw_msa_output_1 = self.block2(merged_patch1)\n",
    "        print(\"block 2 output\", sw_msa_output_1.shape)\n",
    "        \n",
    "        merged_patch2 = self.patch_merging2(sw_msa_output_1)\n",
    "        sw_msa_output_2 = self.block3(merged_patch2)\n",
    "        print(\"block 3 output\", sw_msa_output_2.shape)\n",
    "        \n",
    "        merged_patch3 = self.patch_merging3(sw_msa_output_2)\n",
    "        sw_msa_output_3 = self.block4(merged_patch3)\n",
    "        print(\"block 4 output\", sw_msa_output_3.shape)\n",
    "        \n",
    "        bs, num_window, num_patch_in_window, patch_depth = sw_msa_output_3.shape\n",
    "        sw_msa_output_3 = sw_msa_output_3.reshape(bs, -1, patch_depth)\n",
    "        \n",
    "        pool_output = torch.mean(sw_msa_output_3, dim=1)\n",
    "        logits = self.final_layer(pool_output)\n",
    "        print(\"logits\", logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3301e9c",
   "metadata": {},
   "source": [
    "## 模型测试\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67f2158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4096, 8])\n",
      "block 1 output torch.Size([4, 256, 16, 8])\n",
      "block 2 output torch.Size([4, 64, 16, 16])\n",
      "block 3 output torch.Size([4, 16, 16, 32])\n",
      "block 4 output torch.Size([4, 4, 16, 64])\n",
      "logits torch.Size([4, 10])\n",
      "tensor([[ 0.1698, -0.9871, -0.3158, -0.0033,  0.7429, -0.2816, -0.0267, -0.0669,\n",
      "         -0.1649, -0.4092],\n",
      "        [ 0.1386, -0.9790, -0.3111, -0.0200,  0.7560, -0.2866, -0.0131, -0.0499,\n",
      "         -0.1631, -0.4140],\n",
      "        [ 0.1590, -0.9869, -0.3212, -0.0055,  0.7552, -0.2835, -0.0131, -0.0642,\n",
      "         -0.1543, -0.4135],\n",
      "        [ 0.1698, -0.9873, -0.3231, -0.0243,  0.7357, -0.2724, -0.0145, -0.0723,\n",
      "         -0.1595, -0.4056]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    bs, ic, image_h, image_w = 4, 3, 256, 256\n",
    "    patch_size = 4\n",
    "    model_dim_C = 8  # 一开始patch embedding的大小\n",
    "    max_num_token = 16\n",
    "    num_classes = 10\n",
    "    window_size = 4\n",
    "    num_head = 2\n",
    "    merge_size = 2\n",
    "    \n",
    "    patch_depth = patch_size * patch_size * ic\n",
    "    image = torch.randn(bs, ic, image_h, image_w)\n",
    "    model = SwinTransformerModel(ic, patch_size, model_dim_C, num_classes, window_size, num_head, merge_size)\n",
    "    \n",
    "    logits = model(image)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8179b29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5fed72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
