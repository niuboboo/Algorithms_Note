{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0e584c",
   "metadata": {},
   "source": [
    "vision transformer只用了transformer的encoder部分，并且是做成一个分类任务。\n",
    "\n",
    "将transformer直接应用到像素图片上有以下问题：\n",
    "\n",
    "1. 计算量特别大。\n",
    "2. 单个像素点不像单个单词那样包含的信息量那么大。\n",
    "\n",
    "vision transformer将图像分成很多个块，将一个个块当作token送入transformer中去。对于这个块有两种方式处理：\n",
    "\n",
    "1. 将图片切割成多个块(image2patch)，然后将这个快经过一个仿射变换得到embedding(patch2embedding)。\n",
    "2. 也可以理解成图片得到patch的过程是卷积操作，并且kernel size等于stride。然后将卷积过后的结果拉直，得到token embedding。\n",
    "\n",
    "为了去做分类任务，vision transformer借鉴了bert中的class token这样的占位符，可以理解为这个class token就是去做query的作用，收集一些这个模型能够做好分类任务的信息。也就是在序列的开头增加一个可训练的embedding。\n",
    "\n",
    "在vit中同样使用了position embedding。\n",
    "\n",
    "\n",
    "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "\n",
    "但是需要将vit这个模型在大量的数据集上做预训练。图片大小可能大一点小一点，但是patch的大小一致，反映在序列上就是序列长短的不一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db4f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c05c25",
   "metadata": {},
   "source": [
    "## convert image to embedding vector sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae18b95",
   "metadata": {},
   "source": [
    "第一步是需要将`image`变成一个`embedding`，但是有两种方式实现，一种是直接切割，另一种是通过卷积的方式实现。\n",
    "\n",
    "在切割的时候，我们可以通过`torch.nn.functional.unfold`来去拿到卷积的区域。函数原型为:\n",
    "\n",
    "\n",
    "```python\n",
    "torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c04a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2emb_naive(image, patch_size, weight):\n",
    "    # image shape: (batch_size, channel, h, w)\n",
    "    patch = F.unfold(image, kernel_size=patch_size, stride=patch_size).transpose(-1, -2)\n",
    "    patch_embedding = patch @ weight\n",
    "    return patch_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a4829",
   "metadata": {},
   "source": [
    "设置一些超参数:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef90fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, ic, image_h, image_w = 1, 3, 8, 8\n",
    "patch_size = 4\n",
    "model_dim = 8\n",
    "patch_depth = patch_size * patch_size * ic  # 图像分割完之后每个patch包含的像素点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eccfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_token = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b85739",
   "metadata": {},
   "source": [
    "除此之外，我们还需要一个weight，也就是patch到embeddin的乘法矩阵，使得最终的输出维度为model_dim。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6c5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn(patch_depth, model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2cddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "image = torch.randn(bs, ic, image_h, image_w)\n",
    "patch_embedding_naive = image2emb_naive(image, patch_size, weight)\n",
    "print(patch_embedding_naive.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e95b1",
   "metadata": {},
   "source": [
    "之后，我们再通过卷积来去实现裁剪的过程:\n",
    "\n",
    "通道数为embedding的size，oh * ow为sequence的长度:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef03143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2emb_conv(image, kernel, stride):\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride) # bs * oc * oh * ow\n",
    "    bs, oc, ow, oh = conv_output.shape\n",
    "    patch_embedding = conv_output.reshape((bs, oc, oh * ow)).transpose(-1, -2)\n",
    "    return patch_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8abe4",
   "metadata": {},
   "source": [
    "weight中的model dim是输出通道数目，patch_depth是卷积核的面积乘以输入通道数，所以我们可以将kernel reshape成oc * ic * kh * kw。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce9ec013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  6.5134,   5.7577,  -4.5906,  -0.2947,  -1.0477,   3.9927,  -3.1848,\n",
      "            2.3162],\n",
      "         [ -3.3703,  -5.5934,   9.0800,  10.9870,  -4.4067,   1.5479, -19.9679,\n",
      "            1.2167],\n",
      "         [  4.2536,   2.1038,  -3.6262,  -3.4400,   9.4593,  -0.5640,  -1.9320,\n",
      "            3.2651],\n",
      "         [ 19.6710,  -8.8445,  -2.8704, -12.5182,  -5.3701,   0.6626,  -3.4929,\n",
      "            1.8841]]])\n",
      "tensor([[[  6.5134,   5.7577,  -4.5906,  -0.2947,  -1.0477,   3.9927,  -3.1848,\n",
      "            2.3162],\n",
      "         [ -3.3703,  -5.5934,   9.0800,  10.9870,  -4.4067,   1.5479, -19.9679,\n",
      "            1.2167],\n",
      "         [  4.2536,   2.1038,  -3.6262,  -3.4400,   9.4593,  -0.5640,  -1.9320,\n",
      "            3.2651],\n",
      "         [ 19.6710,  -8.8445,  -2.8704, -12.5182,  -5.3701,   0.6626,  -3.4929,\n",
      "            1.8841]]])\n"
     ]
    }
   ],
   "source": [
    "kernel = weight.transpose(0, 1).reshape((-1, ic, patch_size, patch_size))\n",
    "patch_embedding_conv = image2emb_conv(image, kernel, patch_size)\n",
    "\n",
    "print(patch_embedding_naive)\n",
    "print(patch_embedding_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6798b9",
   "metadata": {},
   "source": [
    "## class token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cd9dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8])\n",
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "cls_token_embedding = torch.randn(bs, 1, model_dim, requires_grad=True)\n",
    "\n",
    "token_embedding = torch.cat([cls_token_embedding, patch_embedding_conv], dim=1)\n",
    "\n",
    "print(cls_token_embedding.size())\n",
    "print(token_embedding.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94118130",
   "metadata": {},
   "source": [
    "## add position embeding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98d8943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8])\n"
     ]
    }
   ],
   "source": [
    "position_embedding_table = torch.randn(max_num_token, model_dim, requires_grad=True)\n",
    "print(position_embedding_table.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7976ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "torch.Size([1, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "seq_len = token_embedding.shape[1]\n",
    "position_embedding = torch.tile(position_embedding_table[:seq_len], [token_embedding.shape[0], 1, 1])\n",
    "print(seq_len)\n",
    "print(position_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d530ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding += position_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ced9c2",
   "metadata": {},
   "source": [
    "## pass embedding to transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4ffb114",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "encoder_output = transformer_encoder(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a7b70",
   "metadata": {},
   "source": [
    "## do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eeecc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "label = torch.randint(10, (bs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f396416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_output = encoder_output[:, 0, :]\n",
    "\n",
    "linear_layer = nn.Linear(model_dim, num_classes)\n",
    "logits = linear_layer(cls_token_output)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = loss_fn(logits, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38cadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708843e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
