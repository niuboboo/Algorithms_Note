{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb3ea98",
   "metadata": {},
   "source": [
    "## LeNet\n",
    "\n",
    "\n",
    "&emsp;&emsp;LeNet(LeNet-5)由两个部分组成: 卷积编码器和全连接层密集块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2903909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 1, 28, 28) # 添加通道数。\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    Reshape(), nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(2, stride=2),\n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "    nn.Linear(120, 84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830e74b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape Out Put Shape:  torch.Size([1, 1, 28, 28])\n",
      "Conv2d Out Put Shape:  torch.Size([1, 6, 28, 28])\n",
      "Sigmoid Out Put Shape:  torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d Out Put Shape:  torch.Size([1, 6, 14, 14])\n",
      "Conv2d Out Put Shape:  torch.Size([1, 16, 10, 10])\n",
      "Sigmoid Out Put Shape:  torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d Out Put Shape:  torch.Size([1, 16, 5, 5])\n",
      "Flatten Out Put Shape:  torch.Size([1, 400])\n",
      "Linear Out Put Shape:  torch.Size([1, 120])\n",
      "Sigmoid Out Put Shape:  torch.Size([1, 120])\n",
      "Linear Out Put Shape:  torch.Size([1, 84])\n",
      "Sigmoid Out Put Shape:  torch.Size([1, 84])\n",
      "Linear Out Put Shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, \"Out Put Shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b22c6e",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3d9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "\n",
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
    "    return 4\n",
    "\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    \n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "        \n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)\n",
    "    \n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbe1714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinyzqh/miniforge3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-wdh4qigd/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f6a7b5",
   "metadata": {},
   "source": [
    "## 使用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4e660",
   "metadata": {},
   "source": [
    "&emsp;&emsp;定义一个累加器:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318a4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169f4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "\n",
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = argmax(y_hat, axis=1)\n",
    "    cmp = astype(y_hat, y.dtype) == y\n",
    "    return float(reduce_sum(astype(cmp, y.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f41d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"使用GPU计算模型在数据集上的精度\"\"\"\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "        if not device: # 如果没有指定device, 就用网络参数中的所给定的那个device。\n",
    "            device = next(iter(net.parameters())).device\n",
    "    \n",
    "    metric = Accumulator(2)\n",
    "    \n",
    "    for X, y in data_iter:\n",
    "        if isinstance(X, list):\n",
    "            X = [x.to(device) for x in X]  # 将数据挪到device上。\n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            \n",
    "        y = y.to(device)\n",
    "        \n",
    "        metric.add(accuracy(net(X), y), y.numel())\n",
    "        \n",
    "        return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "224e9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    \n",
    "    net.apply(init_weights)  # 对net里面的每一个parameter都去run一下init_weights这个函数。\n",
    "    print(\"trainng on {}\".format(device))\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    num_batches = len(train_iter)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_hat = net(X)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            metric.add(l * X.shape[0], accuracy(y_hat, y), X.shape[0])\n",
    "            \n",
    "            train_loss = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            \n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                print(\"epoch {}, train_loss {}, train_acc {}\".format(epoch + (i + 1) / num_batches, \n",
    "                                                                  train_loss, train_acc))\n",
    "    test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "    print(f'loss {train_loss:.3f}, train acc {train_acc:.3f}, 'f'test acc {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d25d61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fadfb672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainng on cpu\n",
      "epoch 0.2, train_loss 2.3627252680190067, train_acc 0.09898603723404255\n",
      "epoch 0.4, train_loss 2.3387740799721253, train_acc 0.09832114361702128\n",
      "epoch 0.6, train_loss 2.3296520321081715, train_acc 0.09801640070921985\n",
      "epoch 0.8, train_loss 2.3235291458190757, train_acc 0.10114694148936171\n",
      "epoch 1.0, train_loss 2.3051708170572915, train_acc 0.11141666666666666\n",
      "epoch 1.2, train_loss 1.6401129529831258, train_acc 0.35555186170212766\n",
      "epoch 1.4, train_loss 1.4330693825762322, train_acc 0.4287317154255319\n",
      "epoch 1.6, train_loss 1.3167020391065178, train_acc 0.4711048315602837\n",
      "epoch 1.8, train_loss 1.2337023615837097, train_acc 0.5032205784574468\n",
      "epoch 2.0, train_loss 1.1681793467203776, train_acc 0.5301333333333333\n",
      "epoch 2.2, train_loss 0.9025096728446635, train_acc 0.6457779255319149\n",
      "epoch 2.4, train_loss 0.862120769759442, train_acc 0.6630236037234043\n",
      "epoch 2.6, train_loss 0.8354263094300074, train_acc 0.6742298315602837\n",
      "epoch 2.8, train_loss 0.8140013544483388, train_acc 0.6815990691489362\n",
      "epoch 3.0, train_loss 0.7989158318837484, train_acc 0.6875666666666667\n",
      "epoch 3.2, train_loss 0.7179301165519877, train_acc 0.7224900265957447\n",
      "epoch 3.4, train_loss 0.6960618381804609, train_acc 0.7290558510638298\n",
      "epoch 3.6, train_loss 0.692115620518407, train_acc 0.7306349734042553\n",
      "epoch 3.8, train_loss 0.6819257346239496, train_acc 0.7341672207446809\n",
      "epoch 4.0, train_loss 0.6796586231867472, train_acc 0.7343833333333334\n",
      "epoch 4.2, train_loss 0.6323444982792469, train_acc 0.7549867021276596\n",
      "epoch 4.4, train_loss 0.6287116357620727, train_acc 0.7568151595744681\n",
      "epoch 4.6, train_loss 0.6283673443270068, train_acc 0.7549589982269503\n",
      "epoch 4.8, train_loss 0.6219341823078216, train_acc 0.7569398271276596\n",
      "epoch 5.0, train_loss 0.6180505514780681, train_acc 0.75945\n",
      "epoch 5.2, train_loss 0.6105670421681506, train_acc 0.7578125\n",
      "epoch 5.4, train_loss 0.5958116251103421, train_acc 0.7665392287234043\n",
      "epoch 5.6, train_loss 0.589247612031639, train_acc 0.770279255319149\n",
      "epoch 5.8, train_loss 0.5879139664008263, train_acc 0.770736369680851\n",
      "epoch 6.0, train_loss 0.5800630573908488, train_acc 0.7746666666666666\n",
      "epoch 6.2, train_loss 0.5503419783521206, train_acc 0.7887300531914894\n",
      "epoch 6.4, train_loss 0.546766330904149, train_acc 0.7897273936170213\n",
      "epoch 6.6, train_loss 0.5419544061870439, train_acc 0.7910848847517731\n",
      "epoch 6.8, train_loss 0.5387292309327328, train_acc 0.792740192819149\n",
      "epoch 7.0, train_loss 0.5358851783752442, train_acc 0.7938666666666667\n",
      "epoch 7.2, train_loss 0.5186089879654824, train_acc 0.7997007978723404\n",
      "epoch 7.4, train_loss 0.5110434582258793, train_acc 0.8035654920212766\n",
      "epoch 7.6, train_loss 0.5181129845321601, train_acc 0.8012245124113475\n",
      "epoch 7.8, train_loss 0.5131584600882327, train_acc 0.8030252659574468\n",
      "epoch 8.0, train_loss 0.5065084416071574, train_acc 0.8057\n",
      "epoch 8.2, train_loss 0.49716045755021115, train_acc 0.8137466755319149\n",
      "epoch 8.4, train_loss 0.49561698512828095, train_acc 0.8117935505319149\n",
      "epoch 8.6, train_loss 0.4869563987491824, train_acc 0.8162677304964538\n",
      "epoch 8.8, train_loss 0.4849243203693248, train_acc 0.8171750332446809\n",
      "epoch 9.0, train_loss 0.4825895076751709, train_acc 0.8168666666666666\n",
      "epoch 9.2, train_loss 0.47227022432266397, train_acc 0.8201462765957447\n",
      "epoch 9.4, train_loss 0.4702231677922797, train_acc 0.821766954787234\n",
      "epoch 9.6, train_loss 0.4673274518750238, train_acc 0.8236092641843972\n",
      "epoch 9.8, train_loss 0.46844726325349606, train_acc 0.823720079787234\n",
      "epoch 10.0, train_loss 0.4626006804148356, train_acc 0.8260666666666666\n",
      "loss 0.463, train acc 0.826, test acc 0.828\n"
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.9, 10\n",
    "train(net, train_iter, test_iter, num_epochs, lr, try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c34e790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb720b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4fc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d283f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bf9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa50bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
