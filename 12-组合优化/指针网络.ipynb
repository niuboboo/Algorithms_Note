{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a321625",
   "metadata": {},
   "source": [
    "https://github.com/ccjjxx99/PointerNetworks-pytorch-tsp\n",
    "\n",
    "https://github.com/aurelienbibaut/Actor_CriticPointer_Network-TSP\n",
    "\n",
    "https://github.com/zifeiyu0531/PointerNetwork-RL-TSP\n",
    "\n",
    "https://colab.research.google.com/drive/1lobspU9b7dTO_HuoX-3nibZspTwfa5aX?usp=sharing#scrollTo=UMntU4jUu_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887188d0",
   "metadata": {},
   "source": [
    "## 指针网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a738fb",
   "metadata": {},
   "source": [
    "`RNN`的输入和输出长度是被限制成一样的了，`seq2seq`的方式通过`RNN`编码和另外一个`RNN`解码的方式能够解除掉这个限制，典型的就是注意力机制`Transformer`网络。但是还是存在一个问题：不能够解决输出字典长度不固定的问题，比如英语单词字典就定`n=8000`个单词，那么`RNN`一个时间步的输出就是一个长度为`8000`的向量，是定死的，这类问题对于处理文本类的问题是没有问题的，但是对于一些特定问题，比如组合优化问题\n",
    "\n",
    "如果这个长度也是改变的那怎么办呢？指针网络提出了一种能够解决上述问题的网络结构：\n",
    "\n",
    "<img src=\"../images/pointer_network.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d59d02",
   "metadata": {},
   "source": [
    "https://github.com/ast0414/pointer-networks-pytorch：解决整数排序问题\n",
    " I expand upon earlier work by solving the planar convex hull problem and replace the LSTM-based encoder and decoder with transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf0570",
   "metadata": {},
   "source": [
    "`pointer network`的核心公示如下：\n",
    "\n",
    "$$\n",
    "u_{j}^{i} = v^{T}tanh(W_{1}e_{j} + W_{2}d_{i}) j \\in (1, \\cdots, n)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(C_{i}|C_{1},\\cdots,C_{i-1}, P) = softmax(u^{i})\n",
    "$$\n",
    "\n",
    "它由两个可学习的权重$W_{1}$和$W_{2}$与编码、解码网络的输出相乘。之后再经过一个非线性变换与另一个学习权重$v$相乘。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eec7b8",
   "metadata": {},
   "source": [
    "## 导入相关的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61a2d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3574d3",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "在指针网络求解`TSP`问题中，我们需要对给定的数据进行编码，之后传给`Decoder`去解码得到输出。可以看到下面代码的`encoder`就只采用了一个`LSTM`结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e2ff213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, feature_size, embedding_dim, hidden_dim, n_layers, dropout, bidir):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Linear(feature_size, embedding_dim)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim // 2 if bidir else hidden_dim\n",
    "        self.n_layers = n_layers * 2 if bidir else n_layers\n",
    "        self.bidir = bidir\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, n_layers, dropout=dropout, bidirectional=bidir, batch_first=True)\n",
    "        \n",
    "        self.h0 = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "        self.c0 = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "    def forward(self, input_datas, hidden):\n",
    "        embedded_inputs = self.embedding_layer(input_datas)\n",
    "        outputs, hidden = self.lstm(embedded_inputs, hidden)  # [8, 5, 512], [4, 8, 256], [4, 8, 256]\n",
    "\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "\n",
    "        h0 = self.h0.unsqueeze(0).unsqueeze(0).repeat(self.n_layers, batch_size, self.hidden_dim)  # [4, 8, 256]\n",
    "        c0 = self.h0.unsqueeze(0).unsqueeze(0).repeat(self.n_layers, batch_size, self.hidden_dim)  # [4, 8, 256]\n",
    "\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38671fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(feature_size = 2, embedding_dim = 128, hidden_dim = 512, n_layers = 2, dropout = 0, bidir=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bd39f",
   "metadata": {},
   "source": [
    "假设`batch size = 8`， 序列长度为`5`， `TSP`问题的两个特征点为横纵坐标，是一个两维的变量。需要先编码到指定维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "682c3745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs shape torch.Size([8, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.randn(8, 5, 2)\n",
    "h0, c0 = encoder.init_hidden(batch_size = 8)\n",
    "outputs, hidden = encoder(input_data, (h0, c0))\n",
    "print(\"outputs shape {}\".format(outputs.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a003ed",
   "metadata": {},
   "source": [
    "可以看到，对于`Encoder`部分主要处理的就是将给定的输入序列进行一个编码，这里若采用`Transformer`进行编码的话也是可以的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999eb67b",
   "metadata": {},
   "source": [
    "## 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b81044",
   "metadata": {},
   "source": [
    "`Attention`的输入是由`decoder`上一个时刻的输出和`decoder`上一时刻的隐藏状态变换得到的`input`。和`Encoder`编码之后的结果共同作用得到："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dbafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.context_linear = nn.Conv1d(input_dim, hidden_dim, 1, 1)\n",
    "        self.V = nn.Parameter(torch.FloatTensor(hidden_dim), requires_grad=True)\n",
    "        self._inf = nn.Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        # Initialize vector V\n",
    "        nn.init.uniform(self.V, -1, 1)\n",
    "\n",
    "    def forward(self, input, context, mask):\n",
    "        \n",
    "        # (batch, hidden_dim, seq_len)\n",
    "        inp = self.input_linear(input).unsqueeze(2).expand(-1, -1, context.size(1))\n",
    "\n",
    "        # (batch, hidden_dim, seq_len)\n",
    "        context = context.permute(0, 2, 1)\n",
    "        ctx = self.context_linear(context)\n",
    "\n",
    "        # (batch, 1, hidden_dim)\n",
    "        V = self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        att = torch.bmm(V, self.tanh(inp + ctx)).squeeze(1)\n",
    "        if len(att[mask]) > 0:\n",
    "            att[mask] = self.inf[mask]\n",
    "        alpha = self.softmax(att)\n",
    "\n",
    "        hidden_state = torch.bmm(ctx, alpha.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return hidden_state, alpha\n",
    "\n",
    "    def init_inf(self, mask_size):\n",
    "        self.inf = self._inf.unsqueeze(1).expand(*mask_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b33f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask size torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "input_length, batch_size = 5, 8\n",
    "attention_layer = Attention(input_dim = 512, hidden_dim = 512)\n",
    "mask = nn.Parameter(torch.ones(1), requires_grad=False).repeat(input_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "attention_layer.init_inf(mask.size())\n",
    "print(\"mask size {}\".format(mask.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82291ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66afd08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.randn(8, 512)\n",
    "context = torch.randn(8, 5, 512)\n",
    "mask_input = torch.eq(mask, 0)\n",
    "print(mask_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf20f931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_state size torch.Size([8, 512])\n",
      "output size torch.Size([8, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "hidden_state, output = attention_layer(input_data, context, mask_input)\n",
    "print(\"hidden_state size {}\".format(hidden_state.size()))\n",
    "print(\"output size {}\".format(output.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2d31b",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8716a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_to_hidden = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "        self.hidden_out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.att = Attention(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Used for propagating .cuda() command\n",
    "        self.mask = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.runner = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "    def forward(self, embedded_inputs, decoder_input, decoder_hidden, context):\n",
    "\n",
    "        batch_size, input_length, _ = embedded_inputs.size()  # [8, 5, 128]\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        mask = self.mask.repeat(input_length).unsqueeze(0).repeat(batch_size, 1)  # [8, 5]\n",
    "        self.att.init_inf(mask.size())\n",
    "\n",
    "        # Generating arang(input_length), broadcasted across batch_size\n",
    "        runner = self.runner.repeat(input_length)\n",
    "        for i in range(input_length):\n",
    "            runner.data[i] = i\n",
    "        runner = runner.unsqueeze(0).expand(batch_size, -1).long()\n",
    "\n",
    "        outputs = []\n",
    "        pointers = []\n",
    "\n",
    "        def step(decoder_input, decoder_hidden):\n",
    "\n",
    "            # Regular LSTM\n",
    "            h, c = decoder_hidden  # []\n",
    "\n",
    "            gates = self.input_to_hidden(decoder_input) + self.hidden_to_hidden(h)  # [8, 2048]\n",
    "            input, forget, cell, out = gates.chunk(4, 1)\n",
    "\n",
    "            input = F.sigmoid(input)  # [8, 512]\n",
    "            forget = F.sigmoid(forget)  # [8, 512]\n",
    "            cell = F.tanh(cell)  # [8, 512]\n",
    "            out = F.sigmoid(out)  # [8, 512]\n",
    "\n",
    "            c_t = (forget * c) + (input * cell)  # [8, 512]\n",
    "            h_t = out * F.tanh(c_t)  # [8, 512]\n",
    "\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = F.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "\n",
    "            return hidden_t, c_t, output\n",
    "\n",
    "        # Recurrence loop\n",
    "        for _ in range(input_length):\n",
    "            h_t, c_t, outs = step(decoder_input, decoder_hidden)\n",
    "            decoder_hidden = (h_t, c_t)  # 更新隐藏状态。\n",
    "\n",
    "            # Masking selected inputs\n",
    "            masked_outs = outs * mask\n",
    "\n",
    "            # Get maximum probabilities and indices\n",
    "            max_probs, indices = masked_outs.max(1)\n",
    "            one_hot_pointers = (runner == indices.unsqueeze(1).expand(-1, outs.size()[1])).float()  # 被选中的编码\n",
    "\n",
    "            # Update mask to ignore seen indices\n",
    "            mask = mask * (1 - one_hot_pointers)  #被选中的经过mask之后下一轮就选不到了。\n",
    "\n",
    "            # Get embedded inputs by max indices\n",
    "            embedding_mask = one_hot_pointers.unsqueeze(2).expand(-1, -1, self.embedding_dim).byte()\n",
    "            decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n",
    "\n",
    "            outputs.append(outs.unsqueeze(0))\n",
    "            pointers.append(indices.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs).permute(1, 0, 2)\n",
    "        pointers = torch.cat(pointers, 1)\n",
    "\n",
    "        return (outputs, pointers), decoder_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3017fc8",
   "metadata": {},
   "source": [
    "## Pointer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0c8098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNet(nn.Module):\n",
    "    def __init__(self, feature_size, embedding_dim, hidden_dim, lstm_layers, dropout, bidir=False):\n",
    "\n",
    "        super(PointerNet, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.bidir = bidir\n",
    "        self.embedding = nn.Linear(2, embedding_dim)\n",
    "        self.encoder = Encoder(feature_size, embedding_dim, hidden_dim, lstm_layers, dropout, bidir)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_dim)\n",
    "        self.decoder_input0 = nn.Parameter(torch.FloatTensor(embedding_dim), requires_grad=False)\n",
    "\n",
    "        # Initialize decoder_input0\n",
    "        nn.init.uniform(self.decoder_input0, -1, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        batch_size, input_length, _ = inputs.size()  # [8, 5, 2]\n",
    "\n",
    "        embedded_inputs = self.embedding(inputs)  # [8, 5, 128]\n",
    "        encoder_hidden0 = self.encoder.init_hidden(batch_size)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = self.encoder(inputs, encoder_hidden0)  # [8, 5, 512] ([4, 8, 256], [4, 8, 256])\n",
    "\n",
    "        if self.bidir:\n",
    "            decoder_hidden_init = (torch.cat((encoder_hidden[0][-2:][0], encoder_hidden[0][-2:][1]), dim=-1),\n",
    "                                   torch.cat((encoder_hidden[1][-2:][0], encoder_hidden[1][-2:][1]), dim=-1))\n",
    "            # decoder_hidden0 = (torch.cat(encoder_hidden[0][-2:], dim=-1), torch.cat(encoder_hidden[1][-2:], dim=-1))\n",
    "        else:\n",
    "            decoder_hidden_init = (encoder_hidden[0][-1], encoder_hidden[1][-1])\n",
    "        decoder_input_init = self.decoder_input0.unsqueeze(0).expand(batch_size, -1)  # [8, 128]\n",
    "        (outputs, pointers), decoder_hidden = self.decoder(embedded_inputs, decoder_input_init, decoder_hidden_init, encoder_outputs)\n",
    "\n",
    "        return outputs, pointers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50270760",
   "metadata": {},
   "source": [
    "## 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4403bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsp_opt(points):\n",
    "    \"\"\"\n",
    "    Dynamic programing solution for TSP - O(2^n*n^2)\n",
    "    https://gist.github.com/mlalevic/6222750\n",
    "    :param points: List of (x, y) points\n",
    "    :return: Optimal solution\n",
    "    \"\"\"\n",
    "\n",
    "    def length(x_coord, y_coord):\n",
    "        return np.linalg.norm(np.asarray(x_coord) - np.asarray(y_coord))\n",
    "\n",
    "    # Calculate all lengths\n",
    "    all_distances = [[length(x, y) for y in points] for x in points]\n",
    "    # Initial value - just distance from 0 to every other point + keep the track of edges\n",
    "    A = {(frozenset([0, idx+1]), idx+1): (dist, [0, idx+1]) for idx, dist in enumerate(all_distances[0][1:])}\n",
    "    cnt = len(points)\n",
    "    for m in range(2, cnt):\n",
    "        B = {}\n",
    "        for S in [frozenset(C) | {0} for C in itertools.combinations(range(1, cnt), m)]:\n",
    "            for j in S - {0}:\n",
    "                # This will use 0th index of tuple for ordering, the same as if key=itemgetter(0) used\n",
    "                B[(S, j)] = min([(A[(S-{j}, k)][0] + all_distances[k][j], A[(S-{j}, k)][1] + [j])\n",
    "                                 for k in S if k != 0 and k != j])\n",
    "        A = B\n",
    "    res = min([(A[d][0] + all_distances[0][d[1]], A[d][1]) for d in iter(A)])\n",
    "    return np.asarray(res[1])\n",
    "\n",
    "\n",
    "class TSPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Random TSP dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_size, seq_len, solver=tsp_opt, solve=True):\n",
    "        self.data_size = data_size\n",
    "        self.seq_len = seq_len\n",
    "        self.solve = solve\n",
    "        self.solver = solver\n",
    "        self.data = self._generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.from_numpy(self.data['Points_List'][idx]).float()\n",
    "        solution = torch.from_numpy(self.data['Solutions'][idx]).long() if self.solve else None\n",
    "\n",
    "        sample = {'Points':tensor, 'Solution':solution}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def _generate_data(self):\n",
    "        \"\"\"\n",
    "        :return: Set of points_list ans their One-Hot vector solutions\n",
    "        \"\"\"\n",
    "        points_list = []\n",
    "        solutions = []\n",
    "        data_iter = tqdm(range(self.data_size), unit='data')\n",
    "        for i, _ in enumerate(data_iter):\n",
    "            data_iter.set_description('Data points %i/%i' % (i+1, self.data_size))\n",
    "            points_list.append(np.random.random((self.seq_len, 2)))\n",
    "        solutions_iter = tqdm(points_list, unit='solve')\n",
    "        if self.solve:\n",
    "            for i, points in enumerate(solutions_iter):\n",
    "                solutions_iter.set_description('Solved %i/%i' % (i+1, len(points_list)))\n",
    "                solutions.append(self.solver(points))\n",
    "        else:\n",
    "            solutions = None\n",
    "\n",
    "        return {'Points_List':points_list, 'Solutions':solutions}\n",
    "\n",
    "    def _to1hotvec(self, points):\n",
    "        \"\"\"\n",
    "        :param points: List of integers representing the points indexes\n",
    "        :return: Matrix of One-Hot vectors\n",
    "        \"\"\"\n",
    "        vec = np.zeros((len(points), self.seq_len))\n",
    "        for i, v in enumerate(vec):\n",
    "            v[points[i]] = 1\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac3753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  del sys.path[0]\n",
      "Data points 9726/1000000:   1%|          | 9717/1000000 [00:18<33:04, 498.90data/s] IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 14351/1000000:   1%|▏         | 14329/1000000 [00:22<09:28, 1732.74data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 19148/1000000:   2%|▏         | 19115/1000000 [00:24<10:05, 1619.16data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 25003/1000000:   2%|▏         | 24882/1000000 [00:27<09:14, 1758.58data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 29646/1000000:   3%|▎         | 29409/1000000 [00:30<07:47, 2077.51data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 33791/1000000:   3%|▎         | 33751/1000000 [00:32<09:34, 1681.40data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 323296/1000000:  32%|███▏      | 323204/1000000 [04:47<10:14, 1100.53data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 328873/1000000:  33%|███▎      | 328836/1000000 [04:52<09:48, 1141.43data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 335272/1000000:  34%|███▎      | 335180/1000000 [04:57<10:25, 1062.42data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 341430/1000000:  34%|███▍      | 341374/1000000 [05:02<07:23, 1483.79data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 347891/1000000:  35%|███▍      | 347730/1000000 [05:06<07:15, 1496.79data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 353794/1000000:  35%|███▌      | 353655/1000000 [05:09<07:05, 1518.29data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 359608/1000000:  36%|███▌      | 359459/1000000 [05:13<07:03, 1513.27data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 365291/1000000:  37%|███▋      | 365259/1000000 [05:16<06:57, 1519.30data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 371002/1000000:  37%|███▋      | 370994/1000000 [05:20<06:46, 1546.26data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 376764/1000000:  38%|███▊      | 376733/1000000 [05:23<06:37, 1568.03data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 381959/1000000:  38%|███▊      | 381903/1000000 [05:26<07:04, 1456.39data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 389161/1000000:  39%|███▉      | 389068/1000000 [05:31<06:01, 1690.42data/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Data points 1000000/1000000: 100%|██████████| 1000000/1000000 [14:04<00:00, 1183.44data/s]\n",
      "Solved 1000000/1000000: 100%|██████████| 1000000/1000000 [1:16:43<00:00, 217.23solve/s]\n",
      "  0%|          | 0/3907 [00:00<?, ?Batch/s]C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\torch\\nn\\functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\torch\\nn\\functional.py:1795: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "C:\\Users\\tinyzqh\\.conda\\envs\\inspir\\lib\\site-packages\\torch\\autograd\\__init__.py:156: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen/native/IndexingUtils.h:30.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "100%|█████████▉| 3901/3907 [36:23<00:03,  1.79Batch/s, loss=1.1597148180007935]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ad36adb71039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[0mtarget_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m             \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\inspir\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-0b6f1dc07fe1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mencoder_hidden0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_hidden0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [8, 5, 512] ([4, 8, 256], [4, 8, 256])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbidir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\inspir\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5a148e471470>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_datas, hidden)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_datas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0membedded_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_datas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# [8, 5, 512], [4, 8, 256], [4, 8, 256]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\inspir\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\inspir\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 692\u001b[1;33m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     pass\n",
    "    parser = argparse.ArgumentParser(description=\"Pytorch implementation of Pointer-Net\")\n",
    "\n",
    "#     Data\n",
    "    parser.add_argument('--train_size', default=1000000, type=int, help='Training data size')\n",
    "    parser.add_argument('--val_size', default=10000, type=int, help='Validation data size')\n",
    "    parser.add_argument('--test_size', default=10000, type=int, help='Test data size')\n",
    "    parser.add_argument('--batch_size', default=256, type=int, help='Batch size')\n",
    "    # Train\n",
    "    parser.add_argument('--epoch_nums', default=50000, type=int, help='Number of epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='Learning rate')\n",
    "    # GPU\n",
    "    parser.add_argument('--gpu', default=True, action='store_true', help='Enable gpu')\n",
    "    # TSP\n",
    "    parser.add_argument('--point_nums', type=int, default=5, help='Number of points in TSP')\n",
    "    # Network\n",
    "    parser.add_argument('--embedding_size', type=int, default=128, help='Embedding size')\n",
    "    parser.add_argument('--hidden_size', type=int, default=512, help='Number of hidden units')\n",
    "    parser.add_argument('--num_layers', type=int, default=2, help='Number of LSTM layers')\n",
    "    parser.add_argument('--dropout', type=float, default=0., help='Dropout value')\n",
    "    parser.add_argument('--bidir', default=True, action='store_true', help='Bidirectional')\n",
    "    parser.add_argument('--feature_size', type=int, default=2, help='feature size of input')\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    if args.gpu and torch.cuda.is_available():\n",
    "        USE_CUDA = True\n",
    "#         print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "    else:\n",
    "        USE_CUDA = False\n",
    "\n",
    "    model = PointerNet(args.feature_size, args.embedding_size, args.hidden_size, args.num_layers, args.dropout, args.bidir)\n",
    "\n",
    "    dataset = TSPDataset(args.train_size, args.point_nums)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    if USE_CUDA:\n",
    "        model.cuda()\n",
    "        net = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    CCE = torch.nn.CrossEntropyLoss()\n",
    "    model_optim = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(args.epoch_nums):\n",
    "        batch_loss = []\n",
    "        iterator = tqdm(dataloader, unit='Batch')\n",
    "\n",
    "        for i_batch, sample_batched in enumerate(iterator):\n",
    "#             iterator.set_description('Batch %i/%i' % (epoch+1, args.point_nums))\n",
    "\n",
    "            train_batch = Variable(sample_batched['Points'])\n",
    "            target_batch = Variable(sample_batched['Solution'])\n",
    "\n",
    "            if USE_CUDA:\n",
    "                train_batch = train_batch.cuda()\n",
    "                target_batch = target_batch.cuda()\n",
    "\n",
    "            o, p = model(train_batch)\n",
    "            o = o.contiguous().view(-1, o.size()[-1])\n",
    "\n",
    "            target_batch = target_batch.view(-1)\n",
    "\n",
    "            loss = CCE(o, target_batch)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "            model_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optim.step()\n",
    "\n",
    "            iterator.set_postfix(loss='{}'.format(loss.item()))\n",
    "\n",
    "        iterator.set_postfix(loss=np.average(batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61be56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee8066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616e5dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
