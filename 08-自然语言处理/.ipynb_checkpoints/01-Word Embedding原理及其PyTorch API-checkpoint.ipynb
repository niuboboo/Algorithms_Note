{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0beb14f",
   "metadata": {},
   "source": [
    "在聊`word embedding`之前，我们需要先了解一下**语言建模**。\n",
    "\n",
    "语言建模是基于已有的人类组织的文本语料，来去无监督学习如何组织一句话，并还能得到单词的语义表征。\n",
    "\n",
    "在`NLP`中的语言建模主要有以下几种：\n",
    "\n",
    "1. 统计模型`N-gram`：最早是通过统计单词出现的词频，然后再结合贝叶斯公式去对一个文本进行分类判断。\n",
    "\n",
    "2. 之后结合神经网络，出现了无监督学习方式的`NNLM`。\n",
    "\n",
    "3. 之后就再出现了大规模的无监督学习，`word2vec`，`BERT`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994dd85",
   "metadata": {},
   "source": [
    "## N-gram模型\n",
    "\n",
    "`N-gram`模型是基于统计的模型，其简单，但是泛化能力差，无法得到单词的语义信息。`N-gram`模型的定义是`n`个相邻字符构成的序列。`n`如果等于`1`的话，我们称之为`unigram`，两个单词构成的序列我们称之为`bigram`，三个单词构成的序列称之为`trigram`。\n",
    "\n",
    "`N-gram`模型也有基于单词的`N-gram`模型和基于字母的`N-gram`模型，基于单词的`N-gram`模型特征维度随着语料词汇增大和`n`增大而指数增大（`curse of dimensionality`，维度灾难）。基于单词的`N-gram`模型特征维度只随着`n`增大而增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce077909",
   "metadata": {},
   "source": [
    "但是其实我们是想拿到单词的语义表征，表示成稀疏式的one-hot embedding只能反映单词的位置信息，并不能反映两两单词之间的关系。稀疏式的one-hot embedding的向量长度与单词表数目相同，向量太长也不利于计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ade971",
   "metadata": {},
   "source": [
    "## 基于神经网络的语言模型(NNLM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f8dfc7",
   "metadata": {},
   "source": [
    "## Word2vec模型\n",
    "\n",
    "改进1：抛弃隐含层，并提出`CBOW`和`Skip-gram`。\n",
    "\n",
    "`Continuous Bag-of-Words`（CBOW）不同于`NNLM`，`CBOW`考虑了前后上下文，使用周围单词预测中间单词。\n",
    "\n",
    "$$\n",
    "J_{\\theta} = \\frac{1}{T} \\sum_{t=1}^{T}log p (w_{t}|w_{t-n}, \\cdots, w_{t-1}, w_{t+1}, \\cdots, w_{t+n})\n",
    "$$\n",
    "\n",
    "`Skip-gram`与`Continuous Bag-of-Words`相反，使用中间单词预测周围单词：\n",
    "\n",
    "$$\n",
    "J_{\\theta} = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-n \\leq j  \\leq n, \\neq 0} log p(w_{t+j} | w_{t})\n",
    "$$\n",
    "\n",
    "改进2: 优化Softmax\n",
    "\n",
    "Softmax计算量跟k呈线性关系：\n",
    "\n",
    "$$\n",
    "\\sigma(\\vec{z})_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}}\n",
    "$$\n",
    "\n",
    "提出Hierarchical Softmax。\n",
    "\n",
    "改进3: 引入负采样\n",
    "\n",
    "Continuous Bag of Words（CBOW）。输入是前n个单词和后n个单词。目标使得预测中间单词的概率最大，同时使得负样本单词的概率最小。\n",
    "\n",
    "$$\n",
    "g(w)=\\prod_{u \\in\\{w\\} \\cup N E G(w)} p(u \\mid \\operatorname{Context}(w))\n",
    "$$\n",
    "\n",
    "Skip-gram输入是中间单词，目标是使得上下文单词概率最大，负样本单词的概率最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e74e8",
   "metadata": {},
   "source": [
    "## PyTorch中的Word Embedding\n",
    "\n",
    "- `PyTorch`中对于`Word Embedding`的介绍如下[Word Embedding](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)\n",
    "\n",
    "下面是对PyTorch官网的核心知识点总结:\n",
    "\n",
    "embedding编码是将单词编码到一个指定的维度，本质是查表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39286e",
   "metadata": {},
   "source": [
    "### Embedding核心介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d46f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e4bce70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ed77fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519],\n",
      "        [-0.1661, -1.5228,  0.3817, -1.0276, -0.5631]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
    "print(embeds.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89ae1c",
   "metadata": {},
   "source": [
    "我们将\"hello\"这个单词获取其index，再查表即可得到embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf80818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867c137",
   "metadata": {},
   "source": [
    "### N-Gram语言模型\n",
    "\n",
    "回顾一下`N-Gram`语言模型：给定一个$w$序列，期望计算:\n",
    "\n",
    "$$\n",
    "P\\left(w_{i} \\mid w_{i-1}, w_{i-2}, \\ldots, w_{i-n+1}\\right)\n",
    "$$\n",
    "\n",
    "其中$w_{i}$是整个序列的第$i$个单词。\n",
    "\n",
    "接下来我们举个例子:\n",
    "\n",
    "假设`CONTEXT_SIZE = 2`，编码之后的`EMBEDDING_DIM=10`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c06dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE-1, -1, -1)],\n",
    "        test_sentence[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
    "]\n",
    "# 打印出ngrams的前3个\n",
    "print(ngrams[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc18bc",
   "metadata": {},
   "source": [
    "之后就是构建word到index的字典，方便之后对字典进行查询，首先需要判断总的词表数目为多少，采用set函数即可得到所有不同词的集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e67913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1faea006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55952b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8371896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[521.7259202003479, 519.2380111217499, 516.765364408493, 514.3083226680756, 511.8656668663025, 509.43723130226135, 507.0212330818176, 504.61711144447327, 502.22495698928833, 499.84271144866943]\n",
      "tensor([-0.0365,  0.1829, -1.2690, -0.5939,  0.4525,  0.3140, -0.6911, -0.2820,\n",
      "         0.0993,  0.4963], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[word_to_ix[\"beauty\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079b5a6",
   "metadata": {},
   "source": [
    "之后官网还提供了一些处理词袋模型的预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee345f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
