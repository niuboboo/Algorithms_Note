{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4352a1e",
   "metadata": {},
   "source": [
    "`Dataset`类主要是从磁盘中加载数据，然后对每个样本进行特征和标签的预处理。然后通过`__getitem__`方法去返回一个样本。\n",
    "\n",
    "`DataLoader`的源码地址为：[https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146f876",
   "metadata": {},
   "source": [
    "## `__init__`方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ff999",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self, dataset: Dataset[T_co], batch_size: Optional[int] = 1,\n",
    "                 shuffle: bool = False, sampler: Union[Sampler, Iterable, None] = None,\n",
    "                 batch_sampler: Union[Sampler[Sequence], Iterable[Sequence], None] = None,\n",
    "                 num_workers: int = 0, collate_fn: Optional[_collate_fn_t] = None,\n",
    "                 pin_memory: bool = False, drop_last: bool = False,\n",
    "                 timeout: float = 0, worker_init_fn: Optional[_worker_init_fn_t] = None,\n",
    "                 multiprocessing_context=None, generator=None,\n",
    "                 *, prefetch_factor: int = 2,\n",
    "                 persistent_workers: bool = False):\n",
    "        # dataset: 实例化的dataset对象。\n",
    "        # batch_size: 我们在训练的时候，通常不会用这个默认大小为1。\n",
    "        # shuffle: 每次训练结束将数据集打乱，只需要设置其值为True即可。\n",
    "        # sampler: 可以以自定义的方式来取样本，比如将长度比较接近的样本放到一个minibatch中。\n",
    "        # batch_sampler: 也是自定义的采样方式。\n",
    "        # num_workers: 主要是采用多进程的方式来读取数据，为0的话就是以主进程来读取数据。\n",
    "        # collate_fn: 聚集函数，通常对一个batch进行后处理，比如对一个batch中数据的最大长度进行padding。\n",
    "        # pin_memory: 将Tensor拷贝一份到GPU中去。\n",
    "        # drop_last:  总样本数目不是batch size的一个整数倍的话，设置为True的时候就会将最后一个小批次丢掉。\n",
    "        # sampler: 如何对数据进行采样，可以用它默认的Samper，或者自己实现一个sampler都可以。\n",
    "        torch._C._log_api_usage_once(\"python.data_loader\")\n",
    "\n",
    "        if num_workers < 0:\n",
    "            raise ValueError('num_workers option should be non-negative; '\n",
    "                             'use num_workers=0 to disable multiprocessing.')\n",
    "\n",
    "        if timeout < 0:\n",
    "            raise ValueError('timeout option should be non-negative')\n",
    "\n",
    "        if num_workers == 0 and prefetch_factor != 2:\n",
    "            raise ValueError('prefetch_factor option could only be specified in multiprocessing.'\n",
    "                             'let num_workers > 0 to enable multiprocessing.')\n",
    "        assert prefetch_factor > 0\n",
    "\n",
    "        if persistent_workers and num_workers == 0:\n",
    "            raise ValueError('persistent_workers option needs num_workers > 0')\n",
    "        \n",
    "        # 对成员变量进行设置\n",
    "        self.dataset = dataset\n",
    "        self.num_workers = num_workers\n",
    "        self.prefetch_factor = prefetch_factor\n",
    "        self.pin_memory = pin_memory\n",
    "        self.timeout = timeout\n",
    "        self.worker_init_fn = worker_init_fn\n",
    "        self.multiprocessing_context = multiprocessing_context\n",
    "\n",
    "        # Arg-check dataset related before checking samplers because we want to\n",
    "        # tell users that iterable-style datasets are incompatible with custom\n",
    "        # samplers first, so that they don't learn that this combo doesn't work\n",
    "        # after spending time fixing the custom sampler errors.\n",
    "        if isinstance(dataset, IterableDataset):  # 一般项目都是map类型的，所以直接走else逻辑。\n",
    "            self._dataset_kind = _DatasetKind.Iterable\n",
    "            # NOTE [ Custom Samplers and IterableDataset ]\n",
    "            #\n",
    "            # `IterableDataset` does not support custom `batch_sampler` or\n",
    "            # `sampler` since the key is irrelevant (unless we support\n",
    "            # generator-style dataset one day...).\n",
    "            #\n",
    "            # For `sampler`, we always create a dummy sampler. This is an\n",
    "            # infinite sampler even when the dataset may have an implemented\n",
    "            # finite `__len__` because in multi-process data loading, naive\n",
    "            # settings will return duplicated data (which may be desired), and\n",
    "            # thus using a sampler with length matching that of dataset will\n",
    "            # cause data lost (you may have duplicates of the first couple\n",
    "            # batches, but never see anything afterwards). Therefore,\n",
    "            # `Iterabledataset` always uses an infinite sampler, an instance of\n",
    "            # `_InfiniteConstantSampler` defined above.\n",
    "            #\n",
    "            # A custom `batch_sampler` essentially only controls the batch size.\n",
    "            # However, it is unclear how useful it would be since an iterable-style\n",
    "            # dataset can handle that within itself. Moreover, it is pointless\n",
    "            # in multi-process data loading as the assignment order of batches\n",
    "            # to workers is an implementation detail so users can not control\n",
    "            # how to batchify each worker's iterable. Thus, we disable this\n",
    "            # option. If this turns out to be useful in future, we can re-enable\n",
    "            # this, and support custom samplers that specify the assignments to\n",
    "            # specific workers.\n",
    "            if isinstance(dataset, IterDataPipe):\n",
    "                torch.utils.data.graph_settings.apply_shuffle_settings(dataset, shuffle=shuffle)\n",
    "            elif shuffle is not False:\n",
    "                raise ValueError(\n",
    "                    \"DataLoader with IterableDataset: expected unspecified \"\n",
    "                    \"shuffle option, but got shuffle={}\".format(shuffle))\n",
    "\n",
    "            if sampler is not None:\n",
    "                # See NOTE [ Custom Samplers and IterableDataset ]\n",
    "                raise ValueError(\n",
    "                    \"DataLoader with IterableDataset: expected unspecified \"\n",
    "                    \"sampler option, but got sampler={}\".format(sampler))\n",
    "            elif batch_sampler is not None:\n",
    "                # See NOTE [ Custom Samplers and IterableDataset ]\n",
    "                raise ValueError(\n",
    "                    \"DataLoader with IterableDataset: expected unspecified \"\n",
    "                    \"batch_sampler option, but got batch_sampler={}\".format(batch_sampler))\n",
    "        else: \n",
    "            self._dataset_kind = _DatasetKind.Map\n",
    "\n",
    "\n",
    "\n",
    "        if sampler is not None and shuffle: #设置了一个自定义的sampler，且shuffle=True的话，那么这个逻辑是没有意义的。\n",
    "            raise ValueError('sampler option is mutually exclusive with '\n",
    "                             'shuffle')\n",
    "\n",
    "        if batch_sampler is not None: #设置batch_sampler的话，batch_size、shuffle、sampler和drop_last就都不能设置。\n",
    "            # auto_collation with custom batch_sampler\n",
    "            if batch_size != 1 or shuffle or sampler is not None or drop_last:\n",
    "                raise ValueError('batch_sampler option is mutually exclusive '\n",
    "                                 'with batch_size, shuffle, sampler, and '\n",
    "                                 'drop_last')\n",
    "            batch_size = None\n",
    "            drop_last = False\n",
    "        elif batch_size is None:\n",
    "            # no auto_collation\n",
    "            if drop_last:\n",
    "                raise ValueError('batch_size=None option disables auto-batching '\n",
    "                                 'and is mutually exclusive with drop_last')\n",
    "\n",
    "        if sampler is None:  # give default samplers\n",
    "            if self._dataset_kind == _DatasetKind.Iterable:\n",
    "                # See NOTE [ Custom Samplers and IterableDataset ]\n",
    "                sampler = _InfiniteConstantSampler()\n",
    "            else:  # map-style  # 如果没有设置sampler，且设置了shuffle的话，就采用RandomSampler来采样。\n",
    "                if shuffle:\n",
    "                    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
    "                else:\n",
    "                    sampler = SequentialSampler(dataset)  # type: ignore[arg-type]\n",
    "\n",
    "        if batch_size is not None and batch_sampler is None:  # 走默认的batch_sampler类。\n",
    "            # auto_collation without custom batch_sampler\n",
    "            batch_sampler = BatchSampler(sampler, batch_size, drop_last)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.sampler = sampler\n",
    "        self.batch_sampler = batch_sampler\n",
    "        self.generator = generator\n",
    "\n",
    "        if collate_fn is None:\n",
    "            if self._auto_collation:\n",
    "                collate_fn = _utils.collate.default_collate\n",
    "            else:\n",
    "                collate_fn = _utils.collate.default_convert\n",
    "\n",
    "        self.collate_fn = collate_fn\n",
    "        self.persistent_workers = persistent_workers\n",
    "\n",
    "        self.__initialized = True\n",
    "        self._IterableDataset_len_called = None  # See NOTE [ IterableDataset and __len__ ]\n",
    "\n",
    "        self._iterator = None\n",
    "\n",
    "        self.check_worker_number_rationality()\n",
    "\n",
    "        torch.set_vital('Dataloader', 'enabled', 'True')  # type: ignore[attr-defined]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70c86a",
   "metadata": {},
   "source": [
    "## `__sampler__`类\n",
    "\n",
    "以下部分源码链接为：[https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/sampler.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7d14d",
   "metadata": {},
   "source": [
    "### RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287a4b5",
   "metadata": {},
   "source": [
    "```python\n",
    "class RandomSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements randomly. If without replacement, then sample from a shuffled dataset.\n",
    "    If with replacement, then user can specify :attr:`num_samples` to draw.\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        replacement (bool): samples are drawn on-demand with replacement if ``True``, default=``False``\n",
    "        num_samples (int): number of samples to draw, default=`len(dataset)`. This argument\n",
    "            is supposed to be specified only when `replacement` is ``True``.\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "    data_source: Sized\n",
    "    replacement: bool\n",
    "\n",
    "    def __init__(self, data_source: Sized, replacement: bool = False,\n",
    "                 num_samples: Optional[int] = None, generator=None) -> None:\n",
    "        self.data_source = data_source\n",
    "        self.replacement = replacement\n",
    "        self._num_samples = num_samples\n",
    "        self.generator = generator\n",
    "\n",
    "        if not isinstance(self.replacement, bool):\n",
    "            raise TypeError(\"replacement should be a boolean value, but got \"\n",
    "                            \"replacement={}\".format(self.replacement))\n",
    "\n",
    "        if self._num_samples is not None and not replacement:\n",
    "            raise ValueError(\"With replacement=False, num_samples should not be specified, \"\n",
    "                             \"since a random permute will be performed.\")\n",
    "\n",
    "        if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n",
    "            raise ValueError(\"num_samples should be a positive integer \"\n",
    "                             \"value, but got num_samples={}\".format(self.num_samples))\n",
    "\n",
    "    @property\n",
    "    def num_samples(self) -> int:\n",
    "        # dataset size might change at runtime\n",
    "        if self._num_samples is None:\n",
    "            return len(self.data_source)\n",
    "        return self._num_samples\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        n = len(self.data_source)  # 获取数据集的大小。\n",
    "        if self.generator is None:  # 如果传入的generator是None的话，就会随机产生一个种子。\n",
    "            seed = int(torch.empty((), dtype=torch.int64).random_().item())\n",
    "            generator = torch.Generator()\n",
    "            generator.manual_seed(seed)\n",
    "        else:\n",
    "            generator = self.generator\n",
    "\n",
    "        if self.replacement:\n",
    "            for _ in range(self.num_samples // 32):\n",
    "                yield from torch.randint(high=n, size=(32,), dtype=torch.int64, generator=generator).tolist()\n",
    "            yield from torch.randint(high=n, size=(self.num_samples % 32,), dtype=torch.int64, generator=generator).tolist()\n",
    "        else:\n",
    "            yield from torch.randperm(n, generator=generator).tolist()  # 返回一个0～n-1数字的随机组合。\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2b22a",
   "metadata": {},
   "source": [
    "### SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1327a5",
   "metadata": {},
   "source": [
    "```python\n",
    "class SequentialSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements sequentially, always in the same order.\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "    \"\"\"\n",
    "    data_source: Sized\n",
    "\n",
    "    def __init__(self, data_source: Sized) -> None:\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        return iter(range(len(self.data_source)))  # 直接返回数据的序列标签。\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_source)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02aaa15",
   "metadata": {},
   "source": [
    "### BatchSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c32fe",
   "metadata": {},
   "source": [
    "```python\n",
    "class BatchSampler(Sampler[List[int]]):\n",
    "    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n",
    "    Args:\n",
    "        sampler (Sampler or Iterable): Base sampler. Can be any iterable object\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
    "            its size would be less than ``batch_size``\n",
    "    Example:\n",
    "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
    "        >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler: Union[Sampler[int], Iterable[int]], batch_size: int, drop_last: bool) -> None:\n",
    "        # Since collections.abc.Iterable does not check for `__getitem__`, which\n",
    "        # is one way for an object to be an iterable, we don't do an `isinstance`\n",
    "        # check here.\n",
    "        if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \\\n",
    "                batch_size <= 0:\n",
    "            raise ValueError(\"batch_size should be a positive integer value, \"\n",
    "                             \"but got batch_size={}\".format(batch_size))\n",
    "        if not isinstance(drop_last, bool):\n",
    "            raise ValueError(\"drop_last should be a boolean value, but got \"\n",
    "                             \"drop_last={}\".format(drop_last))\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        batch = []\n",
    "        for idx in self.sampler:  # 从sampler中去元素的index。\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if len(batch) > 0 and not self.drop_last:  # 如果drop_last为False的话，也将其返回。\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # Can only be called if self.sampler has __len__ implemented\n",
    "        # We cannot enforce this condition, so we turn off typechecking for the\n",
    "        # implementation below.\n",
    "        # Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size  # type: ignore[arg-type]\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size  # type: ignore[arg-type]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af85ee",
   "metadata": {},
   "source": [
    "## collate_fn\n",
    "\n",
    "源码位于: [https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001cffc5",
   "metadata": {},
   "source": [
    "```python\n",
    "def default_collate(batch):\n",
    "    r\"\"\"\n",
    "        Function that takes in a batch of data and puts the elements within the batch\n",
    "        into a tensor with an additional outer dimension - batch size. The exact output type can be\n",
    "        a :class:`torch.Tensor`, a `Sequence` of :class:`torch.Tensor`, a\n",
    "        Collection of :class:`torch.Tensor`, or left unchanged, depending on the input type.\n",
    "        This is used as the default function for collation when\n",
    "        `batch_size` or `batch_sampler` is defined in :class:`~torch.utils.data.DataLoader`.\n",
    "        Here is the general input type (based on the type of the element within the batch) to output type mapping:\n",
    "        * :class:`torch.Tensor` -> :class:`torch.Tensor` (with an added outer dimension batch size)\n",
    "        * NumPy Arrays -> :class:`torch.Tensor`\n",
    "        * `float` -> :class:`torch.Tensor`\n",
    "        * `int` -> :class:`torch.Tensor`\n",
    "        * `str` -> `str` (unchanged)\n",
    "        * `bytes` -> `bytes` (unchanged)\n",
    "        * `Mapping[K, V_i]` -> `Mapping[K, default_collate([V_1, V_2, ...])]`\n",
    "        * `NamedTuple[V1_i, V2_i, ...]` -> `NamedTuple[default_collate([V1_1, V1_2, ...]), default_collate([V2_1, V2_2, ...]), ...]`\n",
    "        * `Sequence[V1_i, V2_i, ...]` -> `Sequence[default_collate([V1_1, V1_2, ...]), default_collate([V2_1, V2_2, ...]), ...]`\n",
    "        Args:\n",
    "            batch: a single batch to be collated\n",
    "        Examples:\n",
    "            >>> # Example with a batch of `int`s:\n",
    "            >>> default_collate([0, 1, 2, 3])\n",
    "            tensor([0, 1, 2, 3])\n",
    "            >>> # Example with a batch of `str`s:\n",
    "            >>> default_collate(['a', 'b', 'c'])\n",
    "            ['a', 'b', 'c']\n",
    "            >>> # Example with `Map` inside the batch:\n",
    "            >>> default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}]\n",
    "            {'A': tensor([  0, 100]), 'B': tensor([  1, 100])}\n",
    "            >>> # Example with `NamedTuple` inside the batch:\n",
    "            >>> Point = namedtuple('Point', ['x', 'y'])\n",
    "            >>> default_collate([Point(0, 0), Point(1, 1)])\n",
    "            Point(x=tensor([0, 1]), y=tensor([0, 1]))\n",
    "            >>> # Example with `Tuple` inside the batch:\n",
    "            >>> default_collate([(0, 1), (2, 3)])\n",
    "            [tensor([0, 2]), tensor([1, 3])]\n",
    "            >>> # Example with `List` inside the batch:\n",
    "            >>> default_collate([[0, 1], [2, 3]])\n",
    "            [tensor([0, 2]), tensor([1, 3])]\n",
    "    \"\"\"\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        out = None\n",
    "        if torch.utils.data.get_worker_info() is not None:\n",
    "            # If we're in a background process, concatenate directly into a\n",
    "            # shared memory tensor to avoid an extra copy\n",
    "            numel = sum(x.numel() for x in batch)\n",
    "            storage = elem.storage()._new_shared(numel)\n",
    "            out = elem.new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
    "\n",
    "            return default_collate([torch.as_tensor(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(elem, collections.abc.Mapping):\n",
    "        try:\n",
    "            return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\n",
    "        except TypeError:\n",
    "            # The mapping type may not support `__init__(iterable)`.\n",
    "            return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
    "        return elem_type(*(default_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, collections.abc.Sequence):\n",
    "        # check to make sure that the elements in batch have consistent size\n",
    "        it = iter(batch)\n",
    "        elem_size = len(next(it))\n",
    "        if not all(len(elem) == elem_size for elem in it):\n",
    "            raise RuntimeError('each element in list of batch should be of equal size')\n",
    "        transposed = list(zip(*batch))  # It may be accessed twice, so we use a list.\n",
    "\n",
    "        if isinstance(elem, tuple):\n",
    "            return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n",
    "        else:\n",
    "            try:\n",
    "                return elem_type([default_collate(samples) for samples in transposed])\n",
    "            except TypeError:\n",
    "                # The sequence type may not support `__init__(iterable)` (e.g., `range`).\n",
    "                return [default_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822b0cc",
   "metadata": {},
   "source": [
    "默认的`collate_fn`就什么都没有干，如果需要自己写一个`collate_fn`的话，我们传入的输入也是一个`batch`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113a5ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d76c563",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed4fdb30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "755baf85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "197c7eb4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e0f0020",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
