{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c591ee8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14eed9ed",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "有时候，我们从磁盘中读取特征和标签之后，无法将这些特征和标签直接喂到神经网络之中。比如磁盘读入的数据是一些图片，但是我们可能对这个图片的大小要有一定的要求。或者是将标签变成`One-Hot`的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2943b20",
   "metadata": {},
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4b1fbb",
   "metadata": {},
   "source": [
    "`torch.nn`命名空间下会提供所有的构建网络的模块。每个模块都将是`nn.Module`的子类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7755b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f3b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a0836",
   "metadata": {},
   "source": [
    "### 定义网络类\n",
    "\n",
    "- [https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n",
    "\n",
    "所有的网络都是要去继承`nn.Module`的父类。初始化整个网络是在`__init__`中。每个`nn.Module`的子类需要去在`forward`中去实现输入数据的过网络操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e735a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421ce79a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681f951",
   "metadata": {},
   "source": [
    "如果想要实现`keras`中的`model.summary()`功能的话，我们可以参考以下教程进行安装：\n",
    "\n",
    "1. [pytorch-summary](https://github.com/sksq96/pytorch-summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4c6b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class tensor([5])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(\"Predicted class {}\".format(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f7265",
   "metadata": {},
   "source": [
    "## Flatten\n",
    "\n",
    "- [https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
    "\n",
    "`Flatten`是将数据的维度压缩成末尾维度为`-1`的数据。\n",
    "\n",
    "函数原型为：\n",
    "\n",
    "```python\n",
    "torch.nn.Flatten(start_dim=1, end_dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca91643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 288])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "input_data = torch.randn(32, 1, 5, 5)\n",
    "m1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 5, 1, 1),\n",
    "    nn.Flatten()\n",
    ")\n",
    "output = m1(input_data)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396589ec",
   "metadata": {},
   "source": [
    "## Linear\n",
    "\n",
    "- [https://pytorch.org/docs/stable/generated/torch.nn.Linear.html](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "\n",
    "函数原型为：\n",
    "\n",
    "```python\n",
    "torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "我们可以对数据进行一个线性变换：$y = x A^{T} + b$。函数原型中的`in_features`表示输入数据的特征，`out_features`表示输出数据的特征维度，`bias`表示是否添加偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d87b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "m2 = nn.Linear(20, 30)\n",
    "input_data = torch.rand(128, 20)\n",
    "output = m2(input_data)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e06203",
   "metadata": {},
   "source": [
    "可以通过查看模型的属性变量来查看权重和偏置。其默认初始值是：$\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$, 其中$k = \\frac{1}{in\\_features}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06674e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7150396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.bias.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c27ee9",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "- [https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "\n",
    "神经网络的构成部分里面还有一些非线形函数。`ReLU`的形式可以表示为:\n",
    "\n",
    "$$\n",
    "\\operatorname{ReLU}(x)=(x)^{+}=\\max (0, x)\n",
    "$$\n",
    "\n",
    "\n",
    "其函数原型为：\n",
    "\n",
    "```python\n",
    "torch.nn.ReLU(inplace=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1574a",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "- [https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "\n",
    "`nn.Sequential`是一个有序的容器。数据通过这些容器的顺序就是刚开始定义时候的序列。\n",
    "\n",
    "使用`Sequential`可以创建一个小的`model`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af53a52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(20, 64, 5),\n",
    "    nn.ReLU()\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6e236f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('conv1', nn.Conv2d(1, 20, 5)),\n",
    "    ('relu', nn.ReLU()),\n",
    "    ('conv2', nn.Conv2d(20, 64, 5)),\n",
    "    ('relu', nn.ReLU())\n",
    "]))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238251f",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "- [https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n",
    "\n",
    "函数原型为：\n",
    "\n",
    "```python\n",
    "torch.nn.Softmax(dim=None)\n",
    "```\n",
    "\n",
    "它的实例化参数只有一个就是`dim`，一般传入到`softmax`中的张量都不止一个维度，可能会有两个维度，或者三个维度，我们通常只会对其中的某个维度做`softmax`，其公式可以表示为：\n",
    "\n",
    "$$\n",
    "\\operatorname{Softmax}\\left(x_{i}\\right)=\\frac{\\exp \\left(x_{i}\\right)}{\\sum_{j} \\exp \\left(x_{j}\\right)}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9d65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcca8102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5826, 0.2536, 0.1639],\n",
       "        [0.7336, 0.1555, 0.1109]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "m = nn.Softmax(dim=1)\n",
    "\n",
    "output = m(input_data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d075af1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6128, 0.7647, 0.7466],\n",
       "        [0.3872, 0.2353, 0.2534]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "m = nn.Softmax(dim=0)\n",
    "\n",
    "output = m(input_data)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28e2c6",
   "metadata": {},
   "source": [
    "## 获取模型参数\n",
    "\n",
    "可以通过调用`nn.Module`这个父类的`named_parameters()`方法来返回一个元祖，第一个是`name`，第二个是参数`param`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4837d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a55dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer linear_relu_stack.0.weight | Size torch.Size([512, 784]) | Values tensor([[ 0.0293,  0.0166,  0.0002,  ...,  0.0328, -0.0073,  0.0030],\n",
      "        [ 0.0250,  0.0135,  0.0311,  ..., -0.0034,  0.0084,  0.0155]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "Layer linear_relu_stack.0.bias | Size torch.Size([512]) | Values tensor([0.0306, 0.0263], grad_fn=<SliceBackward>)\n",
      "Layer linear_relu_stack.2.weight | Size torch.Size([512, 512]) | Values tensor([[ 0.0401, -0.0128,  0.0366,  ..., -0.0175, -0.0305,  0.0165],\n",
      "        [ 0.0092, -0.0319,  0.0109,  ...,  0.0228, -0.0025, -0.0107]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "Layer linear_relu_stack.2.bias | Size torch.Size([512]) | Values tensor([-0.0333, -0.0229], grad_fn=<SliceBackward>)\n",
      "Layer linear_relu_stack.4.weight | Size torch.Size([10, 512]) | Values tensor([[ 0.0005, -0.0079,  0.0131,  ...,  0.0030, -0.0120, -0.0071],\n",
      "        [-0.0024, -0.0185,  0.0096,  ...,  0.0348, -0.0007,  0.0168]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "Layer linear_relu_stack.4.bias | Size torch.Size([10]) | Values tensor([ 0.0033, -0.0119], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(\"Layer {} | Size {} | Values {}\".format(name, param.size(), param[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6183abd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
