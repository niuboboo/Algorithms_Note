{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f0b269",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb2311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750876e3",
   "metadata": {},
   "source": [
    "- 原始论文：http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n",
    "\n",
    "考虑一个带有$L$层隐藏层的神经网络，$l \\in \\{1, \\cdots, L\\}$。$z^{(l)}$定义为$l$层网络的输入向量，$y^{(l)}$定义为$l$层网络的输出向量，$w^{(l)}$定义为$l$层网络的权重，$b^{(l)}$定义为$l$层网络的偏置。标准的前馈神经网络可以描述为：\n",
    "\n",
    "其中$f$为激活函数，比如$f(x)=1/(1+exp(-x))$。基于dropout，前馈网络可以表示为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_{i}^{(l+1)} &=\\mathbf{w}_{i}^{(l+1)} \\mathbf{y}^{l}+b_{i}^{(l+1)} \\\\\n",
    "y_{i}^{(l+1)} &=f\\left(z_{i}^{(l+1)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "添加完dropout之后的话，前馈网络可以表示为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_{j}^{(l)} & \\sim \\operatorname{Bernoulli}(p) \\\\\n",
    "\\widetilde{\\mathbf{y}}^{(l)} &=\\mathbf{r}^{(l)} * \\mathbf{y}^{(l)}, \\\\\n",
    "z_{i}^{(l+1)} &=\\mathbf{w}_{i}^{(l+1)} \\widetilde{\\mathbf{y}}^{l}+b_{i}^{(l+1)} \\\\\n",
    "y_{i}^{(l+1)} &=f\\left(z_{i}^{(l+1)}\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c8e75",
   "metadata": {},
   "source": [
    "但是dropout实现由两种方式，一种是上面的`vanilla`版本，但是在做`inference`的时候，数据会进行$1-p$倍的缩小。`inverted dropout`在丢弃完之后，进行$1/(1-p)$倍的放大。反向传播的时候也是同步放大，但是在做`inference`的时候就不需要额外的处理了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e01361",
   "metadata": {},
   "source": [
    "### 1. Numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdf194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, level):\n",
    "    \"\"\"\n",
    "    # dropout函数的实现，函数中，x是本层网络的激活值。Level就是dropout就是每个神经元要被丢弃的概率。\n",
    "    \"\"\"\n",
    "    if level < 0. or level >= 1: #level是概率值，必须在0~1之间\n",
    "        raise ValueError('Dropout level must be in interval [0, 1[.')\n",
    "    retain_prob = 1. - level\n",
    " \n",
    "    # 我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样\n",
    "    # 硬币 正面的概率为p，n表示每个神经元试验的次数\n",
    "    # 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。\n",
    "    #即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了\n",
    "    random_tensor = np.random.binomial(n=1, p=retain_prob, size=x.shape) \n",
    "    print(\"random_tensor: \", random_tensor)\n",
    " \n",
    "    x *= random_tensor\n",
    "    print(\"x: \", x)\n",
    "    x /= retain_prob\n",
    " \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962b0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_tensor:  [0 0 1 0 1 0 0 0 1 1]\n",
      "x:  [ 0.  0.  3.  0.  5.  0.  0.  0.  9. 10.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.      ,  0.      ,  5.      ,  0.      ,  8.333333,  0.      ,\n",
       "        0.      ,  0.      , 14.999999, 16.666666], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  \n",
    "x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)\n",
    "dropout(x,0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378f40c",
   "metadata": {},
   "source": [
    "### 2. Torch实现\n",
    "\n",
    "参数`p`表示不保留节点的比例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafebf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4935,  1.3364,  0.4463, -0.8671,  0.6874],\n",
       "        [-0.2558, -0.9374, -0.9058,  0.9340,  0.5590],\n",
       "        [ 0.0684, -0.2466,  0.6334,  1.1733,  1.7996],\n",
       "        [-1.4010,  1.0581, -0.4400, -2.4728,  0.8179]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.5)\n",
    "data = torch.randn(4, 5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a99c9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000],\n",
       "        [-0.0000, -1.8748, -0.0000,  1.8680,  0.0000],\n",
       "        [ 0.0000, -0.0000,  1.2668,  2.3465,  3.5991],\n",
       "        [-0.0000,  0.0000, -0.0000, -4.9457,  0.0000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = m(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3d39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8cf5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
