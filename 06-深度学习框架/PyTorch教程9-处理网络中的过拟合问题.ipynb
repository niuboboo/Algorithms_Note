{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f0b269",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89f6be",
   "metadata": {},
   "source": [
    "`Dropout`就是随机丢掉网络输入种的特征，丢掉太多或者太少都不好，所以`dropout rate`一般设置为`0.3～0.5`。在`PyTorch`中有两种方法实现`dropout`，一种是以类的方式实现的：\n",
    "\n",
    "```python\n",
    "class Dropout(_DropoutNd):\n",
    "```\n",
    "\n",
    "其官网文档[torch.nn.Dropout(p=0.5, inplace=False)](https://pytorch.org/docs/master/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n",
    "\n",
    "另一种是以函数的方式实现的：\n",
    "\n",
    "```python\n",
    "def dropout(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) \n",
    "```\n",
    "\n",
    "其官网文档[torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)](https://pytorch.org/docs/master/generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ebbc8",
   "metadata": {},
   "source": [
    "### 原理\n",
    "\n",
    "\n",
    "- 原始论文：http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n",
    "\n",
    "考虑一个带有$L$层隐藏层的神经网络，$l \\in \\{1, \\cdots, L\\}$。$z^{(l)}$定义为$l$层网络的输入向量，$y^{(l)}$定义为$l$层网络的输出向量，$w^{(l)}$定义为$l$层网络的权重，$b^{(l)}$定义为$l$层网络的偏置。标准的前馈神经网络可以描述为：\n",
    "\n",
    "其中$f$为激活函数，比如$f(x)=1/(1+exp(-x))$。基于dropout，前馈网络可以表示为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_{i}^{(l+1)} &=\\mathbf{w}_{i}^{(l+1)} \\mathbf{y}^{l}+b_{i}^{(l+1)} \\\\\n",
    "y_{i}^{(l+1)} &=f\\left(z_{i}^{(l+1)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "添加完dropout之后的话，前馈网络可以表示为：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_{j}^{(l)} & \\sim \\operatorname{Bernoulli}(p) \\\\\n",
    "\\widetilde{\\mathbf{y}}^{(l)} &=\\mathbf{r}^{(l)} * \\mathbf{y}^{(l)}, \\\\\n",
    "z_{i}^{(l+1)} &=\\mathbf{w}_{i}^{(l+1)} \\widetilde{\\mathbf{y}}^{l}+b_{i}^{(l+1)} \\\\\n",
    "y_{i}^{(l+1)} &=f\\left(z_{i}^{(l+1)}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "但是dropout实现由两种方式，一种是上面的`vanilla`版本，但是在做`inference`的时候，数据会进行$1-p$倍的缩小。`inverted dropout`在丢弃完之后，进行$1/(1-p)$倍的放大。反向传播的时候也是同步放大，但是在做`inference`的时候就不需要额外的处理了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92ba95",
   "metadata": {},
   "source": [
    "## 类方法实现的Dropout\n",
    "\n",
    "类方法实现的源码中，其`Dropout`类是继承自`_DropoutNd`类, 而`_DropoutNd`类是继承`Module`类。\n",
    "\n",
    "由于此种方式实现的`Dropout`是一个类，所以用的时候，我们首先需要实例化这个对象，然后将这个对象作用到一个输入上。实例化这个`Dropout`对象的时候，我们需要传入一个参数$p$，表示的是以概率$p$，从`Bernoulli`分布中采样，来作用到给定的输入元素中，也就是随机将输入元素以概率$p$屏蔽掉，也就是输入中不被`mask`为`0`的概率为$1-p$。\n",
    "\n",
    "官方文档中还给了一篇论文：[Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580.pdf)\n",
    "\n",
    "因为最终只有$1-p$的概率的输入被保留下来了，所以在训练时，输出会被缩放到$\\frac{1}{1-p}$倍，这种处理方式在做测试(推理)时就不需要额外的处理了。\n",
    "\n",
    "`Python`示例如下：\n",
    "\n",
    "```python\n",
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(20, 16)\n",
    "output = m(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285d2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6e6ac",
   "metadata": {},
   "source": [
    "随机生成一个四行五列的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b94cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1388,  0.7845,  0.0998,  0.5277,  1.3779],\n",
       "        [ 0.6497, -0.5809, -0.4241,  2.1075, -0.1612],\n",
       "        [-0.2707,  0.2061, -1.6815, -0.4468, -0.2063],\n",
       "        [-2.2523, -0.7526,  1.7570, -0.3444, -0.9290]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.randn(4, 5)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da6f7b",
   "metadata": {},
   "source": [
    "将模型设置为训练模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3da8981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.5, inplace=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.5)  # 参数`p`表示不保留节点的比例。\n",
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73136031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2776,  1.5690,  0.0000,  0.0000,  2.7557],\n",
       "        [ 1.2994, -1.1618, -0.8482,  4.2151, -0.0000],\n",
       "        [-0.0000,  0.4122, -0.0000, -0.0000, -0.0000],\n",
       "        [-4.5046, -1.5052,  0.0000, -0.6887, -0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = m(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c59095",
   "metadata": {},
   "source": [
    "将模型设置为测试模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e955da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1388,  0.7845,  0.0998,  0.5277,  1.3779],\n",
       "        [ 0.6497, -0.5809, -0.4241,  2.1075, -0.1612],\n",
       "        [-0.2707,  0.2061, -1.6815, -0.4468, -0.2063],\n",
       "        [-2.2523, -0.7526,  1.7570, -0.3444, -0.9290]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.eval()\n",
    "output = m(data)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf2ac9",
   "metadata": {},
   "source": [
    "从上述输出结果可以看出，训练和测试阶段的不同之处了。训练时会对数据进行一个缩放，测试时是全部输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4977eff",
   "metadata": {},
   "source": [
    "## 函数方法实现\n",
    "\n",
    "在函数方法实现中，需要传入参数`input`，置`0`的概率`p`，`training`表示是否为测试。但是在类方法实现中，并不需要传入`training`这个参数，是因为它继承自`module`类，设置模型为`eval`时，就会将其内部属性`self.training`设置为`False`。\n",
    "\n",
    "官方网址：[torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)](https://pytorch.org/docs/master/generated/torch.nn.functional.dropout.html#torch.nn.functional.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e01361",
   "metadata": {},
   "source": [
    "## Numpy实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bdf194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, p):\n",
    "    \"\"\"\n",
    "    # dropout函数的实现，函数中，x是本层网络的激活值。p就是dropout就是每个神经元要被丢弃的概率。\n",
    "    \"\"\"\n",
    "    if p < 0. or p >= 1: #p是概率值，必须在0~1之间\n",
    "        raise ValueError('Dropout level must be in interval [0, 1[.')\n",
    "    retain_prob = 1. - p\n",
    " \n",
    "    # 我们通过binomial函数，生成与x一样的维数向量。binomial函数就像抛硬币一样，我们可以把每个神经元当做抛硬币一样\n",
    "    # 硬币 正面的概率为p，n表示每个神经元试验的次数\n",
    "    # 因为我们每个神经元只需要抛一次就可以了所以n=1，size参数是我们有多少个硬币。\n",
    "    #即将生成一个0、1分布的向量，0表示这个神经元被屏蔽，不工作了，也就是dropout了\n",
    "    random_tensor = np.random.binomial(n=1, p=retain_prob, size=x.shape) \n",
    "    print(\"random_tensor: \", random_tensor)\n",
    " \n",
    "    x *= random_tensor\n",
    "    print(\"x: \", x)\n",
    "    x /= retain_prob\n",
    " \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962b0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_tensor:  [1 1 0 1 1 1 1 0 1 0]\n",
      "x:  [1. 2. 0. 4. 5. 6. 7. 0. 9. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.6666666,  3.3333333,  0.       ,  6.6666665,  8.333333 ,\n",
       "       10.       , 11.666666 ,  0.       , 14.999999 ,  0.       ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对dropout的测试，大家可以跑一下上面的函数，了解一个输入x向量，经过dropout的结果  \n",
    "x=np.asarray([1,2,3,4,5,6,7,8,9,10],dtype=np.float32)\n",
    "dropout(x,0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc5b52",
   "metadata": {},
   "source": [
    "而如果在整个网络计算过程中，其大体流程如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8cf5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c922ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rate, x, w1, b1, w2, b2):\n",
    "    layer1 = np.maximum(0, np.dot(w1, x)+b1)\n",
    "    mask1 = np.random.binomial(1, 1-rate, layer1.shape)\n",
    "    layer1 = layer1 * mask1\n",
    "    \n",
    "    layer2 = np.maximum(0, np.dot(w2, layer2)+b2)\n",
    "    mask2 = np.random.binomial(1, 1-rate, layer2.shape)\n",
    "    layer2 = layer2 * mask2\n",
    "    \n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "554a5d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(rate, x, w1, b1, w2, b2):\n",
    "    layer1 = np.maximum(0, np.dot(w1, x) + b1)\n",
    "    layer1 = layer1 * (1 - rate)\n",
    "    \n",
    "    layer2 = np.maximum(0, np.dot(w2, layer1) + b2)\n",
    "    layer2 = layer2 * (1 - rate)\n",
    "    \n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19f323",
   "metadata": {},
   "source": [
    "上述计算方法是在训练和测试阶段两部分都做了缩放，为了方便模型之后部署，减少推理的计算时间，还有一种方法是全部在训练过程中做缩放，而测试部分不动："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb285d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_train(rate, x, w1, b1, w2, b2):\n",
    "    layer1 = np.maximum(0, np.dot(w1, x)+b1)\n",
    "    mask1 = np.random.binomial(1, 1-rate, layer1.shape)\n",
    "    layer1 = layer1 * mask1\n",
    "    layer1 = layer1 / (1 - rate)\n",
    "    \n",
    "    layer2 = np.maximum(0, np.dot(w2, layer2)+b2)\n",
    "    mask2 = np.random.binomial(1, 1-rate, layer2.shape)\n",
    "    layer2 = layer2 * mask2\n",
    "    layer2 = layer2 / (1 - rate)\n",
    "    \n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d37dfcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def another_test(rate, x, w1, b1, w2, b2):\n",
    "    layer1 = np.maximum(0, np.dot(w1, x) + b1)\n",
    "    \n",
    "    layer2 = np.maximum(0, np.dot(w2, layer1) + b2)\n",
    "    \n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40006ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
