{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f35f24d",
   "metadata": {},
   "source": [
    "## torch.utils.data.Dataset\n",
    "\n",
    "&emsp;&emsp;`Dataset`用来处理单个样本，如何从磁盘中读取我们的训练数据集。包括如何处理特征，标签，以及一些变形。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759aad1",
   "metadata": {},
   "source": [
    "### 创建自定义的DataSet类\n",
    "\n",
    "&emsp;&emsp;自定义的`Dataset`类需要实现三个函数：\n",
    "\n",
    "1. __init__: 初始化数据。\n",
    "\n",
    "\n",
    "2. __len__: 返回所有训练样本的长度。\n",
    "\n",
    "\n",
    "3. __getitem__: 依据索引来返回训练样本。\n",
    "\n",
    "&emsp;&emsp;官方给定的`FashionMNIST images`存储在`img_dir`中，其标签存储在`CSV`文件`annotations_file`中。官方给出的示意源码为：\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file) # 获取图像lable\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform  # 是否进行归一化等操作。\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])  # 获取到图像的路径。\n",
    "        image = read_image(img_path)  # 获取到图像数据。\n",
    "        label = self.img_labels.iloc[idx, 1]  # 获取到标签数据。\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabea8d",
   "metadata": {},
   "source": [
    "## torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e748e",
   "metadata": {},
   "source": [
    "`DataSet`一次只返回一个样本对应的特征和标签。当我们训练一个模型的时候，我们通常希望处理的是一个`minibatches`，每个`epoch`下打乱样本顺序，防止过拟合，并且采用`Python`的`multiprocessing`来加速数据读取，使其减少`GPU`训练时的等待时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a485e6e",
   "metadata": {},
   "source": [
    "从torchvision库中下载数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee6a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinyzqh/miniforge3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-wdh4qigd/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "training_data = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b278fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)  # test一般不用shuffle。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e0f95f",
   "metadata": {},
   "source": [
    "将dataset加载到DataLoader之后，我们就可以对其进行遍历，每次返回一个batch的训练特征和训练标签，每次包含一个batch=64的样本。\n",
    "\n",
    "因为设置了shuffle=True，所以当我们加载完了所有数据之后，数据将会被再次打乱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4c4ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(\"Feature batch shape {}\".format(train_features.size()))\n",
    "print(\"Labels batch shape {}\".format(train_labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc6af0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d37fd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Label {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f900b",
   "metadata": {},
   "source": [
    "## 自定义Sampler\n",
    "\n",
    "自定义`Sampler`即是自定义数据采样方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c78c45a",
   "metadata": {},
   "source": [
    "首先，我们虚构一个`Dataset`类，用于测试。这个类中的`label`标签是混乱的，无法通过控制`index`范围来实现单类别训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792250e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f45d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self):\n",
    "        self.img = torch.cat([torch.ones(2, 2) for i in range(100)], dim=0)\n",
    "        self.num_classes = 2\n",
    "        self.label = torch.tensor([random.randint(0, self.num_classes-1) for i in range(100)])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.img[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988973a",
   "metadata": {},
   "source": [
    "然后，自定义一个`Sampler`类，这个类的作用是生成一个`index`列表，可以理解为重排`data`中的`index`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "845452f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSampler(Sampler):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __iter__(self):\n",
    "        indices = []\n",
    "        for n in range(self.data.num_classes):\n",
    "            index = torch.where(self.data.label == n)[0]\n",
    "            indices.append(index)\n",
    "        indices = torch.cat(indices, dim=0)\n",
    "        return iter(indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308670a",
   "metadata": {},
   "source": [
    "定义好了之后，可以封装成`DataLoader`并查看运行结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d10036b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "d = Data()\n",
    "s = CustomSampler(d)\n",
    "dl = DataLoader(d, 8, sampler=s)\n",
    "for img, label in dl:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c1be75",
   "metadata": {},
   "source": [
    "上述结果中，有一个`batch`中还是包含了两种不同类型的标签，为了达到目的，我们还需要定义一个`BatchSampler`类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7cef5d",
   "metadata": {},
   "source": [
    "## 自定义BatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3052a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBatchSampler:\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        i = 0\n",
    "        sampler_list = list(self.sampler)\n",
    "        for idx in sampler_list:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "            if(\n",
    "                i < len(sampler_list) - 1\n",
    "                and self.sampler.data.label[idx]\n",
    "                != self.sampler.data.label[sampler_list[i + 1]]\n",
    "            ):\n",
    "                if len(batch) > 0 and not self.drop_last:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "                else:\n",
    "                    batch = []\n",
    "            i += 1\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler)   # self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bd0de9",
   "metadata": {},
   "source": [
    "再次封装成`DataLoader`并查看运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f94b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "d = Data()\n",
    "s = CustomSampler(d)\n",
    "bs = CustomBatchSampler(s, 8, False)\n",
    "dl = DataLoader(d, batch_sampler=bs)\n",
    "for img, label in dl:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f1d600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "d = Data()\n",
    "s = CustomSampler(d)\n",
    "bs = CustomBatchSampler(s, 8, True)\n",
    "dl = DataLoader(d, batch_sampler=bs)\n",
    "for img, label in dl:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d7615",
   "metadata": {},
   "source": [
    "## collate_fn\n",
    "\n",
    "`collate_fn`是对一个`batch`的样本进行处理。比如如下方法将一个`batch`的样本进行批处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c80180cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    target = [torch.reshape(item[1], (-1,)) for item in batch]\n",
    "    data = torch.stack(data)\n",
    "    target = torch.stack(target)\n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6ed2429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "d = Data()\n",
    "s = CustomSampler(d)\n",
    "bs = CustomBatchSampler(s, 8, True)\n",
    "dl = DataLoader(d, batch_sampler=bs, collate_fn=collate_fn)\n",
    "for img, label in dl:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d692d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class HousePriceTrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super(HousePriceTrainDataset, self).__init__()\n",
    "        train_data = pd.read_csv('train.csv')\n",
    "        test_data = pd.read_csv('test.csv')\n",
    "\n",
    "        print(\"The Train Data Shape is : {}\".format(train_data.shape))\n",
    "        print(\"The Test Daata Shape is : {}\\n\".format(test_data.shape))\n",
    "\n",
    "        print(\"The front four row, [0, 1, 2, 3, -3, -2, -1] columns data: \")\n",
    "        print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # remove id and concat data\n",
    "        all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n",
    "\n",
    "        numeric_features = all_features.dtypes[all_features.dtypes != \"object\"].index\n",
    "\n",
    "        all_features[numeric_features] = all_features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "        all_features[numeric_features] = all_features[numeric_features].fillna(0)\n",
    "\n",
    "        all_features = pd.get_dummies(all_features, dummy_na=True)\n",
    "\n",
    "        n_train = train_data.shape[0]\n",
    "        train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\n",
    "        train_labels = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n",
    "    \n",
    "\n",
    "def get_net(in_features):\n",
    "    net = nn.Sequential(nn.Linear(in_features, 1))\n",
    "    return net\n",
    "\n",
    "def log_rmse(net, features, labels, loss):\n",
    "    clipped_pre = torch.clamp(net(features), 1, float('inf'))\n",
    "    rmse = torch.sqrt(loss(torch.log(clipped_pre), torch.log(labels)))\n",
    "    return rmse.item()\n",
    "\n",
    "def train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size, loss):\n",
    "    train_ls, test_ls = [], []\n",
    "    dataset = torch.utils.data.TensorData(train_features, train_labels)\n",
    "    train_loader = torch.utils.dataDataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    print(f\"num of each batch {len(next(iter(train_loader))[0])} all {len(train_loader)} batch\")\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "        train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
    "    return train_ls, test_ls\n",
    "\n",
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate, weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "    return train_l_sum / k, valid_l_sum / k\n",
    "            \n",
    "\n",
    "def main():\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "\n",
    "    print(\"The Train Data Shape is : {}\".format(train_data.shape))\n",
    "    print(\"The Test Daata Shape is : {}\\n\".format(test_data.shape))\n",
    "\n",
    "    print(\"The front four row, [0, 1, 2, 3, -3, -2, -1] columns data: \")\n",
    "    print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # remove id and concat data\n",
    "    all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n",
    "\n",
    "    numeric_features = all_features.dtypes[all_features.dtypes != \"object\"].index\n",
    "\n",
    "    all_features[numeric_features] = all_features[numeric_features].apply(lambda x : (x - x.mean()) / (x.std()))\n",
    "    all_features[numeric_features] = all_features[numeric_features].fillna(0)\n",
    "\n",
    "    all_features = pd.get_dummies(all_features, dummy_na=True)\n",
    "\n",
    "    n_train = train_data.shape[0]\n",
    "    train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\n",
    "    train_labels = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    in_features = train_features.shape[1]\n",
    "    \n",
    "    net = get_net(in_features=in_features)\n",
    "    test_features = None\n",
    "    test_labels = None\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = \n",
    "    train(net, train_features, train_labels, test_features, test_labels, num_epochs, learning_rate, weight_decay, batch_size, loss)\n",
    "\n",
    "    print(\"....\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
